{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhYjHDP9wUiw"
      },
      "source": [
        "# **Assignment 1: Translation with a Sequence to Sequence Network** ü§ñ\n",
        "\n",
        "This assignment is split into two sections: **Neural Machine Translation** with (1) **RNNs** and (2) **Transformer**. To be more specifically, in Machine Translation, our goal is to convert a sentence from the source language (e.g. Vietnamese) to the target language (e.g. English). In this assignment, we will implement a sequence-to-sequence (**Seq2Seq**) network based on two architectures: **RNNs with Attention** and **Transformer**, to build a **Neural Machine Translation (NMT) system**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObihRu3tGB93"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1ltoz1Yr_6QTyR8bHd4wzOh3J03abwRg4\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzUewzPQyv7j"
      },
      "source": [
        "That's a lot to digest, the goal of this assignment is to break it down into easy to understand parts. In this assignment you will:\n",
        "\n",
        "- [Data Preparation](#data-preparation): A machine cannot directly understand \"T√¥i y√™u b·∫°n\" the way humans do. We must first prepare and preprocess the data into a format the machine can comprehend.\n",
        "- **Seq2Seq Model Build:** After preparing the necessary data, we proceed to build our Seq2Seq model for the machine translation task.\n",
        "    - [RNNs with Attention Architecture]():\n",
        "        - **Embedding Layer:** initialize layer to convert input words to their embeddings.\n",
        "        - **Initialize Layers:** declare basic layers of our model.\n",
        "        - **The Encoder & Decoder:** implement the `encode` function to obtain encoder hidden states from source sentences and `decode` function to compute combined output vectors for a batch inputs.\n",
        "\n",
        "    - [Transformer Architecture]():\n",
        "        - **Positional embeddings:** initialize layer to combines token embeddings with positional embeddings. Unlike RNNs, the Transformer architecture processes all input tokens in parallel. Therefore, positional embeddings are added to provide the information of order and position of elements in an input sequence, which is crucial for our machine translation tasks.\n",
        "        - **Transformer Layer:** implement the `forward` pass function with the help of transformer library.\n",
        "\n",
        "- **Train & Test our two models:** The training and testing code are provided for you.\n",
        "- [Generate translations](): The model uses either the `beam_search` or `greedy` function to generate translations in the target language. While `beam_search` is provided as the standard for testing, you are required to implement the `greedy` method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfRYHN7dxXfn"
      },
      "source": [
        "**Requirements**\n",
        "\n",
        "Firstly, apart from standards libraries, we need to install some package:\n",
        "\n",
        "1. `sentencepiece`: To build your own vocabulary \\\\\n",
        "2. `sacrebleu`: To evaluate our model using BLUE score metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hbap3qmadgJ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install sentencepiece==0.1.97\n",
        "!pip install tqdm==4.29.1\n",
        "!pip install sacrebleu\n",
        "!pip install nltk\n",
        "!pip install 'portalocker>=2.0.0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMNtv0jhyZuJ"
      },
      "source": [
        "Below, we import our standard libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivUAAQs2XuIw"
      },
      "outputs": [],
      "source": [
        "#@title Standard libraries\n",
        "# Standard libraries\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Dict, Set, Union\n",
        "from collections import Counter, namedtuple\n",
        "from itertools import chain\n",
        "from dataclasses import dataclass\n",
        "\n",
        "# to compute BLUE score\n",
        "import sacrebleu\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.utils\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
        "\n",
        "# To train vocabulary\n",
        "import sentencepiece as spm\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhgJuptiW9Pa"
      },
      "outputs": [],
      "source": [
        "#@title Default hyperparameters\n",
        "@dataclass\n",
        "class Args:\n",
        "    cuda: str = \"cuda:0\"\n",
        "    train_src: str = \"data/train.vi\"\n",
        "    train_tgt: str = \"data/train.en\"\n",
        "    dev_src: str = \"data/dev.vi\"\n",
        "    dev_tgt: str = 'data/dev.en'\n",
        "    vocab_file: str = 'vocab.json'\n",
        "    src_vocab_size: int = 15000\n",
        "    tgt_vocab_size: int = 21000\n",
        "    seed: int = 0\n",
        "    batch_size: int = 32\n",
        "    max_len: int = 320\n",
        "    embed_size: int = 1024\n",
        "    hidden_size: int = 768\n",
        "    clip_grad: float = 5.0                  # gradient clipping\n",
        "    log_every: int = 10                     # log every\n",
        "    max_epoch: int = 100                    # max epoch\n",
        "    patience: int = 5                       # wait for how many iterations to decay learning rate\n",
        "    max_num_trial: int = 5                  # terminate training after how many trials\n",
        "    lr_decay: float = 0.5                   # learning rate decay\n",
        "    beam_size: int = 5                      # beam size\n",
        "    lr: float = 0.001                       # learning rate\n",
        "    uniform_init: float = 0.1               # uniformly initialize all parameters\n",
        "    model_save_path: str = 'lstm_model.bin' # model save path\n",
        "    valid_niter: int = 2000                 # perform validation after how many iterations\n",
        "    dropout: float = 0.3\n",
        "    max_decoding_time_step: int = 70        # maximum number of decoding time steps\n",
        "\n",
        "args = Args()\n",
        "device = torch.device(args.cuda) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "seed = int(args.seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "np.random.seed(seed * 13 // 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX-l-b2jIcLY"
      },
      "source": [
        "# Let's start your assignment implementation üí™\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFw7oNvnzRjf"
      },
      "source": [
        "# Data Preparation\n",
        "\n",
        "A machine cannot directly understand \"T√¥i y√™u b·∫°n\" the way humans do. We must first prepare and preprocess the data into a format the machine can comprehend.\n",
        "\n",
        "Below is a visual representation of the data preparation process using two sample data points. The steps are as follows:\n",
        "\n",
        "1. Download and load the data.\n",
        "2. Construct our `Vocab` from a list of subwords generated by `SentencePiece` ([github](https://github.com/google/sentencepiece)) for both the source and target languages.\n",
        "3. Apply `SentencePiece` to encode raw text into subwords and add special tokens for target data.\n",
        "3. Apply padding to ensure all elements in the same batch have the same length.\n",
        "4. Convert the padded batch into a tensor using word ids from our `Vocab`.\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1z9E_s55APC_pAGzpuuvENzvJqnHRz8j6\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiTtm4ednPXG"
      },
      "source": [
        "If you're not yet familiar with tensors in PyTorch, you can visit this link for more information: [Tensor tutorial](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HvcA-jc2CmC"
      },
      "source": [
        "\n",
        "## Loading data files\n",
        "The data for this project is a set of thousands of Vietnamese to\n",
        "English translation pairs. We will download them first then save to 'data' folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQCDCurrVPwh"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!mkdir data\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "def download_file(google_drive_link, to_save_path):\n",
        "    match = re.search(r\"/file/d/(.*?)/\", google_drive_link)\n",
        "    file_id = match.group(1) if match else None\n",
        "    new_path = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "    os.system(f\"wget --no-check-certificate {new_path} -O {to_save_path}\")\n",
        "\n",
        "\n",
        "\n",
        "data_path = 'https://drive.google.com/file/d/1eq68XlKxWBFCj4YgMRl2N5YdrZvB9FDs/view?usp=sharing'\n",
        "download_file(data_path, args.train_src)\n",
        "\n",
        "data_path = 'https://drive.google.com/file/d/1679j2kIvdl8Oe_WRSX0vi62JtOrhr1GD/view?usp=sharing'\n",
        "download_file(data_path, args.train_tgt)\n",
        "\n",
        "\n",
        "data_path = 'https://drive.google.com/file/d/1p0tBxnD-MVXyve772omfq1nFDraeI_sO/view?usp=sharing'\n",
        "download_file(data_path, args.dev_src)\n",
        "\n",
        "data_path = 'https://drive.google.com/file/d/1ZvBBTUwzYJuN4J8WCZ9-kZiBEm4iPpiL/view?usp=sharing'\n",
        "download_file(data_path, args.dev_tgt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0yO-MHgGKeX"
      },
      "source": [
        "## Logging data files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UQxrse_JeVw"
      },
      "source": [
        "Understanding the appearance of our data serves as a solid starting point when addressing the issue. The data will be recorded in the source-target (src-tgt) format, featuring raw, unprocessed text. In the subsequent section, you will observe how the data undergoes preprocessing into a more suitable format, specifically using a sub-word tokenizer, to enhance the machine's comprehension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_U7PVxJIVcv"
      },
      "outputs": [],
      "source": [
        "def read_file(file_path):\n",
        "  \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n",
        "  @param file_path (str): path to file containing corpus\n",
        "  \"\"\"\n",
        "  data = []\n",
        "  with open(file_path, 'r', encoding='utf8') as f:\n",
        "    for line in f:\n",
        "      data.append(line.strip())\n",
        "\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgJsKOk-FosY"
      },
      "outputs": [],
      "source": [
        "def logging_data(file_path_src, file_path_tgt, num_data):\n",
        "    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n",
        "    @param file_path (str): path to file containing corpus\n",
        "    \"\"\"\n",
        "\n",
        "    data_src = read_file(file_path_src)\n",
        "    data_tgt = read_file(file_path_tgt)\n",
        "\n",
        "    for i in range(num_data):\n",
        "      print(\"Src sent: \", data_src[i])\n",
        "      print(\"Tgt sent: \", data_tgt[i])\n",
        "      print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pqsnAr6Gw0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "505b16e6-5a55-4532-9cb4-98c540d116da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Src sent:  Khoa h·ªçc ƒë·∫±ng sau m·ªôt ti√™u ƒë·ªÅ v·ªÅ kh√≠ h·∫≠u\n",
            "Tgt sent:  Rachel Pike : The science behind a climate headline\n",
            "\n",
            "\n",
            "Src sent:  Trong 4 ph√∫t , chuy√™n gia ho√° h·ªçc kh√≠ quy·ªÉn Rachel Pike gi·ªõi thi·ªáu s∆° l∆∞·ª£c v·ªÅ nh·ªØng n·ªó l·ª±c khoa h·ªçc mi·ªát m√†i ƒë·∫±ng sau nh·ªØng ti√™u ƒë·ªÅ t√°o b·∫°o v·ªÅ bi·∫øn ƒë·ªïi kh√≠ h·∫≠u , c√πng v·ªõi ƒëo√†n nghi√™n c·ª©u c·ªßa m√¨nh -- h√†ng ng√†n ng∆∞·ªùi ƒë√£ c·ªëng hi·∫øn cho d·ª± √°n n√†y -- m·ªôt chuy·∫øn bay m·∫°o hi·ªÉm qua r·ª´ng gi√† ƒë·ªÉ t√¨m ki·∫øm th√¥ng tin v·ªÅ m·ªôt ph√¢n t·ª≠ then ch·ªët .\n",
            "Tgt sent:  In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .\n",
            "\n",
            "\n",
            "Src sent:  T√¥i mu·ªën cho c√°c b·∫°n bi·∫øt v·ªÅ s·ª± to l·ªõn c·ªßa nh·ªØng n·ªó l·ª±c khoa h·ªçc ƒë√£ g√≥p ph·∫ßn l√†m n√™n c√°c d√≤ng t√≠t b·∫°n th∆∞·ªùng th·∫•y tr√™n b√°o .\n",
            "Tgt sent:  I &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .\n",
            "\n",
            "\n",
            "Src sent:  C√≥ nh·ªØng d√≤ng tr√¥ng nh∆∞ th·∫ø n√†y khi b√†n v·ªÅ bi·∫øn ƒë·ªïi kh√≠ h·∫≠u , v√† nh∆∞ th·∫ø n√†y khi n√≥i v·ªÅ ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠ hay kh√≥i b·ª•i .\n",
            "Tgt sent:  Headlines that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .\n",
            "\n",
            "\n",
            "Src sent:  C·∫£ hai ƒë·ªÅu l√† m·ªôt nh√°nh c·ªßa c√πng m·ªôt lƒ©nh v·ª±c trong ng√†nh khoa h·ªçc kh√≠ quy·ªÉn .\n",
            "Tgt sent:  They are both two branches of the same field of atmospheric science .\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "logging_data(args.train_src, args.train_tgt, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEmMVX8J2hHw"
      },
      "source": [
        "## Q1: Padding function (5 points)\n",
        "In order to apply tensor operations, we must ensure that the sentences in a given batch are of the same length. Thus, we must identify the longest sentence in a batch and pad others to be the same length. Implement the `pad_sents` function, which shall produce these padded sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1iQUZHWNWULr"
      },
      "outputs": [],
      "source": [
        "def pad_sents(sents, pad_token):\n",
        "    \"\"\" Pad list of sentences according to the longest sentence in the batch.\n",
        "        The paddings should be at the end of each sentence.\n",
        "    @param sents (list[list[str]]): list of sentences, where each sentence\n",
        "                                    is represented as a list of words\n",
        "    @param pad_token (str): padding token\n",
        "    @returns sents_padded (list[list[str]]): list of sentences where sentences shorter\n",
        "        than the max length sentence are padded out with the pad_token, such that\n",
        "        each sentences in the batch now has equal length.\n",
        "    \"\"\"\n",
        "    sents_padded = []\n",
        "\n",
        "    ### YOUR CODE HERE (~6 Lines)\n",
        "    max_len = max([len(sent) for sent in sents])\n",
        "    for sent in sents:\n",
        "        if len(sent) < max_len:\n",
        "            sent.extend([pad_token] * (max_len - len(sent)))\n",
        "        sents_padded.append(sent)\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return sents_padded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNwJBxvO2u75"
      },
      "outputs": [],
      "source": [
        "sents = [\n",
        "    \"VietAI l√† t·ªï ch·ª©c phi l·ª£i nhu·∫≠n.\",\n",
        "    \"Theo b√°o c√°o m·ªõi nh·∫•t c·ªßa Linkedin v·ªÅ danh s√°ch vi·ªác l√†m tri·ªÉn v·ªçng v·ªõi m·ª©c l∆∞∆°ng h·∫•p d·∫´n nƒÉm 2020, c√°c ch·ª©c danh c√¥ng vi·ªác li√™n quan ƒë·∫øn AI ƒë·ªÅu x·∫øp th·ª© h·∫°ng cao.\",\n",
        "    \"S·ª© m·ªánh c·ªßa VietAI l√† ∆∞∆°m m·∫ßm t√†i nƒÉng v·ªÅ tr√≠ tu·ªá nh√¢n t·∫°o v√† x√¢y d·ª±ng m·ªôt c·ªông ƒë·ªìng c√°c chuy√™n gia trong lƒ©nh v·ª±c tr√≠ tu·ªá nh√¢n t·∫°o ƒë·∫≥ng c·∫•p qu·ªëc t·∫ø.\"\n",
        "    \"Ch√∫ng ta ƒëang tr√™n h√†nh tr√¨nh ti·∫øn b·ªô v√† d√¢n ch·ªß ho√° tr√≠ tu·ªá nh√¢n t·∫°o th√¥ng qua m√£ ngu·ªìn m·ªü v√† khoa h·ªçc m·ªü\"]\n",
        "\n",
        "sents = [s.split() for s in sents]\n",
        "padded_sents = pad_sents(sents, pad_token=\"<PAD>\")\n",
        "\n",
        "for sent in padded_sents:\n",
        "    assert len(sent) == len(padded_sents[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGywMiTH5MCa"
      },
      "source": [
        "Below, we define the `VocabEntry` class. The `VocabEntry` class is a vocabulary entry that contains a dictionary that maps words to indices and provides methods to convert words to indices, indices to words, and sentences to tensors. The purpose of this class is to facilitate the management of the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "euaeDPZSaISN"
      },
      "outputs": [],
      "source": [
        "class VocabEntry(object):\n",
        "    \"\"\" Vocabulary Entry, i.e. structure containing either\n",
        "    src or tgt language terms.\n",
        "    \"\"\"\n",
        "    def __init__(self, word2id=None):\n",
        "        \"\"\" Init VocabEntry Instance.\n",
        "        @param word2id (dict): dictionary mapping words 2 indices\n",
        "        \"\"\"\n",
        "        if word2id:\n",
        "            self.word2id = word2id\n",
        "        else:\n",
        "            self.word2id = dict()\n",
        "            self.word2id['<pad>'] = 0   # Pad Token\n",
        "            self.word2id['<s>'] = 1 # Start Token\n",
        "            self.word2id['</s>'] = 2    # End Token\n",
        "            self.word2id['<unk>'] = 3   # Unknown Token\n",
        "        self.unk_id = self.word2id['<unk>']\n",
        "        self.id2word = {v: k for k, v in self.word2id.items()}\n",
        "\n",
        "    def __getitem__(self, word):\n",
        "        \"\"\" Retrieve word's index. Return the index for the unk\n",
        "        token if the word is out of vocabulary.\n",
        "        @param word (str): word to look up.\n",
        "        @returns index (int): index of word\n",
        "        \"\"\"\n",
        "        return self.word2id.get(word, self.unk_id)\n",
        "\n",
        "    def __contains__(self, word):\n",
        "        \"\"\" Check if word is captured by VocabEntry.\n",
        "        @param word (str): word to look up\n",
        "        @returns contains (bool): whether word is contained\n",
        "        \"\"\"\n",
        "        return word in self.word2id\n",
        "\n",
        "    def __setitem__(self, key, value):\n",
        "        \"\"\" Raise error, if one tries to edit the VocabEntry.\n",
        "        \"\"\"\n",
        "        raise ValueError('vocabulary is readonly')\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" Compute number of words in VocabEntry.\n",
        "        @returns len (int): number of words in VocabEntry\n",
        "        \"\"\"\n",
        "        return len(self.word2id)\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\" Representation of VocabEntry to be used\n",
        "        when printing the object.\n",
        "        \"\"\"\n",
        "        return 'Vocabulary[size=%d]' % len(self)\n",
        "\n",
        "    def id2word(self, wid):\n",
        "        \"\"\" Return mapping of index to word.\n",
        "        @param wid (int): word index\n",
        "        @returns word (str): word corresponding to index\n",
        "        \"\"\"\n",
        "        return self.id2word[wid]\n",
        "\n",
        "    def add(self, word):\n",
        "        \"\"\" Add word to VocabEntry, if it is previously unseen.\n",
        "        @param word (str): word to add to VocabEntry\n",
        "        @return index (int): index that the word has been assigned\n",
        "        \"\"\"\n",
        "        if word not in self:\n",
        "            wid = self.word2id[word] = len(self)\n",
        "            self.id2word[wid] = word\n",
        "            return wid\n",
        "        else:\n",
        "            return self[word]\n",
        "\n",
        "    def words2indices(self, sents):\n",
        "        \"\"\" Convert list of words or list of sentences of words\n",
        "        into list or list of list of indices.\n",
        "        @param sents (list[str] or list[list[str]]): sentence(s) in words\n",
        "        @return word_ids (list[int] or list[list[int]]): sentence(s) in indices\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if type(sents[0]) == list:\n",
        "                for i in range(len(sents)):\n",
        "                    # set max length\n",
        "                    sents[i] = sents[i][:args.max_len]\n",
        "                return [[self[w] for w in s] for s in sents]\n",
        "            else:\n",
        "                # set max length\n",
        "                sents = sents[:args.max_len]\n",
        "                return [[self[w] for w in sents]]\n",
        "        except Exception as e:\n",
        "            print(e, sents)\n",
        "            return []\n",
        "\n",
        "    def indices2words(self, word_ids):\n",
        "        \"\"\" Convert list of indices into words.\n",
        "        @param word_ids (list[int]): list of word ids\n",
        "        @return sents (list[str]): list of words\n",
        "        \"\"\"\n",
        "        return [self.id2word[w_id] for w_id in word_ids]\n",
        "\n",
        "    def to_input_tensor(self, sents: List[List[str]], device: torch.device = device) -> torch.Tensor:\n",
        "        \"\"\" Convert list of sentences (words) into tensor with necessary padding for\n",
        "        shorter sentences.\n",
        "\n",
        "        @param sents (List[List[str]]): list of sentences (words)\n",
        "        @param device: device on which to load the tesnor, i.e. CPU or GPU\n",
        "\n",
        "        @returns sents_var: tensor of (max_sentence_length, batch_size)\n",
        "        \"\"\"\n",
        "        word_ids = self.words2indices(sents)\n",
        "        sents_t = pad_sents(word_ids, self['<pad>'])\n",
        "        sents_var = torch.tensor(sents_t, dtype=torch.long, device=device)\n",
        "        return torch.t(sents_var)\n",
        "\n",
        "    @staticmethod\n",
        "    def from_corpus(corpus, size, freq_cutoff=2):\n",
        "        \"\"\" Given a corpus construct a Vocab Entry.\n",
        "        @param corpus (list[str]): corpus of text produced by read_corpus function\n",
        "        @param size (int): # of words in vocabulary\n",
        "        @param freq_cutoff (int): if word occurs n < freq_cutoff times, drop the word\n",
        "        @returns vocab_entry (VocabEntry): VocabEntry instance produced from provided corpus\n",
        "        \"\"\"\n",
        "        vocab_entry = VocabEntry()\n",
        "        word_freq = Counter(chain(*corpus))\n",
        "        valid_words = [w for w, v in word_freq.items() if v >= freq_cutoff]\n",
        "        print('number of word types: {}, number of word types w/ frequency >= {}: {}'\n",
        "              .format(len(word_freq), freq_cutoff, len(valid_words)))\n",
        "        top_k_words = sorted(valid_words, key=lambda w: word_freq[w], reverse=True)[:size]\n",
        "        for word in top_k_words:\n",
        "            vocab_entry.add(word)\n",
        "        return vocab_entry\n",
        "\n",
        "    @staticmethod\n",
        "    def from_subword_list(subword_list):\n",
        "        vocab_entry = VocabEntry()\n",
        "        for subword in subword_list:\n",
        "            vocab_entry.add(subword)\n",
        "        return vocab_entry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9afEr0m6ZkF"
      },
      "source": [
        "Afterwards, we use a `Vocab` class to wrap vocabulary used for both the source and target languages in a machine translation task. It is composed of two `VocabEntry` objects, one for the source language and one for the target language.\n",
        "\n",
        "The build method is used to construct a `Vocab` object from a list of subwords generated by `SentencePiece` for both the source and target languages. Then, we save them to a JSON file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoIvy1Fw6LGK"
      },
      "outputs": [],
      "source": [
        "class Vocab(object):\n",
        "    \"\"\" Vocab encapsulating src and target langauges.\n",
        "    \"\"\"\n",
        "    def __init__(self, src_vocab: VocabEntry, tgt_vocab: VocabEntry):\n",
        "        \"\"\" Init Vocab.\n",
        "        @param src_vocab (VocabEntry): VocabEntry for source language\n",
        "        @param tgt_vocab (VocabEntry): VocabEntry for target language\n",
        "        \"\"\"\n",
        "        self.src = src_vocab\n",
        "        self.tgt = tgt_vocab\n",
        "\n",
        "    @staticmethod\n",
        "    def build(src_sents, tgt_sents) -> 'Vocab':\n",
        "        \"\"\" Build Vocabulary.\n",
        "        @param src_sents (list[str]): Source subwords provided by SentencePiece\n",
        "        @param tgt_sents (list[str]): Target subwords provided by SentencePiece\n",
        "        \"\"\"\n",
        "\n",
        "        print('initialize source vocabulary ..')\n",
        "        src = VocabEntry.from_subword_list(src_sents)\n",
        "\n",
        "        print('initialize target vocabulary ..')\n",
        "        tgt = VocabEntry.from_subword_list(tgt_sents)\n",
        "\n",
        "        return Vocab(src, tgt)\n",
        "\n",
        "    def save(self, file_path):\n",
        "        \"\"\" Save Vocab to file as JSON dump.\n",
        "        @param file_path (str): file path to vocab file\n",
        "        \"\"\"\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(dict(src_word2id=self.src.word2id, tgt_word2id=self.tgt.word2id), f, indent=2)\n",
        "\n",
        "    @staticmethod\n",
        "    def load(file_path):\n",
        "        \"\"\" Load vocabulary from JSON dump.\n",
        "        @param file_path (str): file path to vocab file\n",
        "        @returns Vocab object loaded from JSON dump\n",
        "        \"\"\"\n",
        "        entry = json.load(open(file_path, 'r'))\n",
        "        src_word2id = entry['src_word2id']\n",
        "        tgt_word2id = entry['tgt_word2id']\n",
        "\n",
        "        return Vocab(VocabEntry(src_word2id), VocabEntry(tgt_word2id))\n",
        "\n",
        "    def __repr__(self):\n",
        "        \"\"\" Representation of Vocab to be used\n",
        "        when printing the object.\n",
        "        \"\"\"\n",
        "        return 'Vocab(source %d words, target %d words)' % (len(self.src), len(self.tgt))\n",
        "\n",
        "\n",
        "def get_vocab_list(file_path, source, vocab_size):\n",
        "    \"\"\" Use SentencePiece to tokenize and acquire list of unique subwords.\n",
        "    @param file_path (str): file path to corpus\n",
        "    @param source (str): tgt or src\n",
        "    @param vocab_size: desired vocabulary size\n",
        "    \"\"\"\n",
        "    spm.SentencePieceTrainer.Train(input=file_path, model_prefix=source, vocab_size=vocab_size)     # train the spm model\n",
        "    sp = spm.SentencePieceProcessor()   # create an instance; this saves .model and .vocab files\n",
        "    sp.Load('{}.model'.format(source))  # loads tgt.model or src.model\n",
        "    sp_list = [sp.IdToPiece(piece_id) for piece_id in range(sp.GetPieceSize())] # this is the list of subwords\n",
        "    return sp_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01HlWwyR6x0v"
      },
      "source": [
        "## Train and save our vocabulary to a json file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh-tl8jWat2S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "976d7c91-bdc5-423f-dd2c-e5dc2e9ad4ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "read in source sentences: data/train.vi\n",
            "read in target sentences: data/train.en\n",
            "initialize source vocabulary ..\n",
            "initialize target vocabulary ..\n",
            "generated vocabulary, source 15000 words, target 21000 words\n",
            "vocabulary saved to vocab.json\n"
          ]
        }
      ],
      "source": [
        "print('read in source sentences: %s' % args.train_src)\n",
        "print('read in target sentences: %s' % args.train_tgt)\n",
        "\n",
        "src_sents = get_vocab_list(args.train_src, source='src', vocab_size=args.src_vocab_size)\n",
        "tgt_sents = get_vocab_list(args.train_tgt, source='tgt', vocab_size=args.tgt_vocab_size)\n",
        "vocab = Vocab.build(src_sents, tgt_sents)\n",
        "print('generated vocabulary, source %d words, target %d words' % (len(src_sents), len(tgt_sents)))\n",
        "\n",
        "vocab.save(args.vocab_file)\n",
        "print('vocabulary saved to %s' % args.vocab_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiC1obxY763V"
      },
      "source": [
        "## Read sentence pairs for training\n",
        "The full process for preparing the data is:\n",
        "\n",
        "- Read text file  into pairs\n",
        "- Encode raw text into subwords\n",
        "- Add word lists into our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIwV0sOye2JJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a903b011-7e14-4133-c6ad-176b9967c522"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "133317\n",
            "1553\n"
          ]
        }
      ],
      "source": [
        "def read_corpus(file_path, source):\n",
        "    \"\"\" Read file, where each sentence is dilineated by a `\\n`.\n",
        "    @param file_path (str): path to file containing corpus\n",
        "    @param source (str): \"tgt\" or \"src\" indicating whether text\n",
        "        is of the source language or target language\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    sp = spm.SentencePieceProcessor()\n",
        "    sp.load('{}.model'.format(source))\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            subword_tokens = sp.encode_as_pieces(line)\n",
        "            # only append <s> and </s> to the target sentence\n",
        "            if source == 'tgt':\n",
        "                subword_tokens = ['<s>'] + subword_tokens + ['</s>']\n",
        "            data.append(subword_tokens)\n",
        "\n",
        "    return data\n",
        "\n",
        "train_data_src = read_corpus(args.train_src, source='src')\n",
        "train_data_tgt = read_corpus(args.train_tgt, source='tgt')\n",
        "\n",
        "dev_data_src = read_corpus(args.dev_src, source='src')\n",
        "dev_data_tgt = read_corpus(args.dev_tgt, source='tgt')\n",
        "\n",
        "train_data = list(zip(train_data_src, train_data_tgt))\n",
        "dev_data = list(zip(dev_data_src, dev_data_tgt))\n",
        "\n",
        "print(len(train_data)) # Not required code\n",
        "print(len(dev_data)) # Not required code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1GXVgR08Nw3"
      },
      "source": [
        "We will visualize certain pairs of training data after encoding into subwords to gain insights into the data. It becomes evident that when raw text is encoded into subwords, a single word can be represented as the concatenation of other subwords. For instance, `\"Trong\"` transforms into `\"_Tro\"` and `\"ng\"` while `\"d√≤ng\"` transforms into `\"_d√≤\"` and `\"ng\"`. Both examples will share the `\"ng\"` in common, reduce the number of item needed in the vocab size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VWCT-JzV6BeW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54f0536e-5d2e-4144-8c56-68776bfb31c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Src sent: ‚ñÅKhoa|‚ñÅh·ªçc|‚ñÅƒë|·∫±ng|‚ñÅsau|‚ñÅm·ªôt|‚ñÅti√™u|‚ñÅƒë·ªÅ|‚ñÅv·ªÅ|‚ñÅkh√≠|‚ñÅh·∫≠u\n",
            "Tgt sent: <s>|‚ñÅRachel|‚ñÅP|ike|‚ñÅ|:|‚ñÅThe|‚ñÅscience|‚ñÅ|behind|‚ñÅa|‚ñÅclimate|‚ñÅheadline|</s>\n",
            "\n",
            "\n",
            "Src sent: ‚ñÅTro|ng|‚ñÅ4|‚ñÅph√∫t|‚ñÅ,|‚ñÅchuy√™n|‚ñÅgia|‚ñÅho√°|‚ñÅh·ªçc|‚ñÅkh√≠|‚ñÅquy·ªÉn|‚ñÅRachel|‚ñÅP|ike|‚ñÅgi·ªõi|‚ñÅt|hi·ªáu|‚ñÅs∆°|‚ñÅl∆∞·ª£c|‚ñÅv·ªÅ|‚ñÅnh·ªØng|‚ñÅn·ªó|‚ñÅl·ª±c|‚ñÅkhoa|‚ñÅh·ªçc|‚ñÅm|i·ªát|‚ñÅm√†|i|‚ñÅƒë|·∫±ng|‚ñÅsau|‚ñÅnh·ªØng|‚ñÅti√™u|‚ñÅƒë·ªÅ|‚ñÅt√°o|‚ñÅb·∫°o|‚ñÅv·ªÅ|‚ñÅbi·∫øn|‚ñÅƒë·ªïi|‚ñÅkh√≠|‚ñÅh·∫≠u|‚ñÅ,|‚ñÅc√πng|‚ñÅv·ªõi|‚ñÅƒëo√†n|‚ñÅnghi√™n|‚ñÅc·ª©u|‚ñÅc·ªßa|‚ñÅm√¨nh|‚ñÅ--|‚ñÅh√†ng|‚ñÅng√†n|‚ñÅng∆∞·ªùi|‚ñÅƒë√£|‚ñÅc·ªë|ng|‚ñÅ|hi·∫øn|‚ñÅcho|‚ñÅd·ª±|‚ñÅ√°n|‚ñÅn√†y|‚ñÅ--|‚ñÅm·ªôt|‚ñÅchuy·∫øn|‚ñÅbay|‚ñÅm·∫°o|‚ñÅhi·ªÉm|‚ñÅqua|‚ñÅr·ª´ng|‚ñÅgi√†|‚ñÅƒë·ªÉ|‚ñÅt√¨m|‚ñÅki·∫øm|‚ñÅth√¥ng|‚ñÅtin|‚ñÅv·ªÅ|‚ñÅm·ªôt|‚ñÅph√¢n|‚ñÅt·ª≠|‚ñÅthe|n|‚ñÅch·ªë|t|‚ñÅ.\n",
            "Tgt sent: <s>|‚ñÅIn|‚ñÅ4|‚ñÅminutes|‚ñÅ|,|‚ñÅatmospher|ic|‚ñÅchemist|‚ñÅRachel|‚ñÅP|ike|‚ñÅprovide|s|‚ñÅa|‚ñÅglimpse|‚ñÅof|‚ñÅthe|‚ñÅmassive|‚ñÅscientific|‚ñÅeffort|‚ñÅ|behind|‚ñÅthe|‚ñÅbold|‚ñÅheadline|s|‚ñÅon|‚ñÅclimate|‚ñÅchange|‚ñÅ|,|‚ñÅwith|‚ñÅher|‚ñÅteam|‚ñÅ--|‚ñÅone|‚ñÅof|‚ñÅ|thousands|‚ñÅwho|‚ñÅcontribute|d|‚ñÅ--|‚ñÅ|taking|‚ñÅa|‚ñÅrisk|y|‚ñÅflight|‚ñÅover|‚ñÅthe|‚ñÅrainforest|‚ñÅin|‚ñÅpursuit|‚ñÅof|‚ñÅdata|‚ñÅon|‚ñÅa|‚ñÅkey|‚ñÅmolecule|‚ñÅ.|</s>\n",
            "\n",
            "\n",
            "Src sent: ‚ñÅT√¥i|‚ñÅmu·ªën|‚ñÅcho|‚ñÅc√°c|‚ñÅb·∫°n|‚ñÅbi·∫øt|‚ñÅv·ªÅ|‚ñÅs·ª±|‚ñÅto|‚ñÅl·ªõn|‚ñÅc·ªßa|‚ñÅnh·ªØng|‚ñÅn·ªó|‚ñÅl·ª±c|‚ñÅkhoa|‚ñÅh·ªçc|‚ñÅƒë√£|‚ñÅg√≥p|‚ñÅph·∫ßn|‚ñÅl√†m|‚ñÅn√™n|‚ñÅc√°c|‚ñÅd√≤|ng|‚ñÅt√≠|t|‚ñÅb·∫°n|‚ñÅth∆∞·ªùng|‚ñÅth·∫•y|‚ñÅtr√™n|‚ñÅb√°o|‚ñÅ.\n",
            "Tgt sent: <s>|‚ñÅI|‚ñÅ&|apos|;|d|‚ñÅlike|‚ñÅto|‚ñÅtalk|‚ñÅto|‚ñÅyou|‚ñÅto|day|‚ñÅabout|‚ñÅthe|‚ñÅscale|‚ñÅof|‚ñÅthe|‚ñÅscientific|‚ñÅeffort|‚ñÅthat|‚ñÅgo|es|‚ñÅinto|‚ñÅ|making|‚ñÅthe|‚ñÅheadline|s|‚ñÅyou|‚ñÅsee|‚ñÅin|‚ñÅthe|‚ñÅpaper|‚ñÅ.|</s>\n",
            "\n",
            "\n",
            "Src sent: ‚ñÅC√≥|‚ñÅnh·ªØng|‚ñÅd√≤|ng|‚ñÅtr|√¥ng|‚ñÅnh∆∞|‚ñÅth·∫ø|‚ñÅn√†y|‚ñÅkhi|‚ñÅb√†n|‚ñÅv·ªÅ|‚ñÅbi·∫øn|‚ñÅƒë·ªïi|‚ñÅkh√≠|‚ñÅh·∫≠u|‚ñÅ,|‚ñÅv√†|‚ñÅnh∆∞|‚ñÅth·∫ø|‚ñÅn√†y|‚ñÅkhi|‚ñÅn√≥i|‚ñÅv·ªÅ|‚ñÅch·∫•t|‚ñÅl∆∞·ª£ng|‚ñÅkh√¥ng|‚ñÅkh√≠|‚ñÅhay|‚ñÅkh√≥i|‚ñÅb·ª•|i|‚ñÅ.\n",
            "Tgt sent: <s>|‚ñÅHead|lines|‚ñÅthat|‚ñÅlook|‚ñÅlike|‚ñÅ|this|‚ñÅwhen|‚ñÅthe|y|‚ñÅhave|‚ñÅto|‚ñÅdo|‚ñÅwith|‚ñÅclimate|‚ñÅchange|‚ñÅ|,|‚ñÅand|‚ñÅheadline|s|‚ñÅthat|‚ñÅlook|‚ñÅlike|‚ñÅ|this|‚ñÅwhen|‚ñÅthe|y|‚ñÅhave|‚ñÅto|‚ñÅdo|‚ñÅwith|‚ñÅair|‚ñÅquality|‚ñÅor|‚ñÅsmog|‚ñÅ.|</s>\n",
            "\n",
            "\n",
            "Src sent: ‚ñÅC·∫£|‚ñÅhai|‚ñÅƒë·ªÅu|‚ñÅl√†|‚ñÅm·ªôt|‚ñÅnh|√°nh|‚ñÅc·ªßa|‚ñÅc√πng|‚ñÅm·ªôt|‚ñÅlƒ©nh|‚ñÅv·ª±c|‚ñÅtrong|‚ñÅng√†nh|‚ñÅkhoa|‚ñÅh·ªçc|‚ñÅkh√≠|‚ñÅquy·ªÉn|‚ñÅ.\n",
            "Tgt sent: <s>|‚ñÅThe|y|‚ñÅare|‚ñÅboth|‚ñÅtwo|‚ñÅbranche|s|‚ñÅof|‚ñÅthe|‚ñÅsame|‚ñÅfield|‚ñÅof|‚ñÅatmospher|ic|‚ñÅscience|‚ñÅ.|</s>\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "n = 5\n",
        "for i in range(n):\n",
        "  print(\"Src sent: \" + \"|\".join(train_data_src[i]))\n",
        "  print(\"Tgt sent: \" + \"|\".join(train_data_tgt[i]))\n",
        "  print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzUtPU1A32gt"
      },
      "source": [
        "We define the `batch_iter` function to iterate through the given data in batches of a specified size, where each batch contains source and target sentences.\n",
        "\n",
        "The sentences are sorted in reverse order by their length, so that longer sentences come first.\n",
        "\n",
        "The function takes three arguments: the data to iterate through, the batch size, and a flag indicating whether to shuffle the data randomly or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n4Lpa-Kck-QJ"
      },
      "outputs": [],
      "source": [
        "def batch_iter(data, batch_size, shuffle=False):\n",
        "    \"\"\" Yield batches of source and target sentences reverse sorted by length (largest to smallest).\n",
        "    @param data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
        "    @param batch_size (int): batch size\n",
        "    @param shuffle (boolean): whether to randomly shuffle the dataset\n",
        "    \"\"\"\n",
        "    batch_num = math.ceil(len(data) / batch_size)\n",
        "    index_array = list(range(len(data)))\n",
        "\n",
        "    if shuffle:\n",
        "        np.random.shuffle(index_array)\n",
        "\n",
        "    for i in range(batch_num):\n",
        "        indices = index_array[i * batch_size: (i + 1) * batch_size]\n",
        "        examples = [data[idx] for idx in indices]\n",
        "\n",
        "        examples = sorted(examples, key=lambda e: len(e[0]), reverse=True)\n",
        "        src_sents, tgt_sents = list(), list()\n",
        "        for src_sent, tgt_sent in examples:\n",
        "            if len(src_sent) > 0 and len(tgt_sent) > 0:\n",
        "                src_sents.append(src_sent)\n",
        "                tgt_sents.append(tgt_sent)\n",
        "        yield src_sents, tgt_sents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5x0BzxmmC8R"
      },
      "source": [
        "We will print the tensor output of sample sentences (or words) to visualize how they are represented, with necessary padding applied to shorter sentences for a batch size of 2. You will notice that the output contains numbers representing the word IDs of their corresponding words (e.g., 0 for `<PAD>`, 1 for `<s>`, and 2 for `</s>`). This tensor will be used during both training and evaluation, and you can find the `batch_iter` and `to_input_tensor` functions in the respective sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtqqPSFFE92j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d6511e8-d566-44b9-e8b5-3d959f65484e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SOURCE TENSOR:  tensor([[ 700,  196],\n",
            "        [  68,  841],\n",
            "        [ 161,  163],\n",
            "        [   7,   22],\n",
            "        [  10,  324],\n",
            "        [  21,  195],\n",
            "        [ 192,  636],\n",
            "        [ 940,    5],\n",
            "        [  20,    0],\n",
            "        [ 529,    0],\n",
            "        [ 662,    0],\n",
            "        [  41,    0],\n",
            "        [ 541,    0],\n",
            "        [  27,    0],\n",
            "        [ 936,    0],\n",
            "        [ 327,    0],\n",
            "        [ 195,    0],\n",
            "        [  11,    0],\n",
            "        [  18,    0],\n",
            "        [ 304,    0],\n",
            "        [ 317,    0],\n",
            "        [ 159,    0],\n",
            "        [ 456,    0],\n",
            "        [  46,    0],\n",
            "        [ 187,    0],\n",
            "        [   4,    0],\n",
            "        [  41,    0],\n",
            "        [  61,    0],\n",
            "        [  36,    0],\n",
            "        [  46,    0],\n",
            "        [  79,    0],\n",
            "        [  11,    0],\n",
            "        [ 541,    0],\n",
            "        [  27,    0],\n",
            "        [ 936,    0],\n",
            "        [ 401,    0],\n",
            "        [1109,    0],\n",
            "        [ 377,    0],\n",
            "        [ 212,    0],\n",
            "        [ 697,    0],\n",
            "        [ 134,    0],\n",
            "        [  23,    0],\n",
            "        [8757,    0],\n",
            "        [ 100,    0],\n",
            "        [ 167,    0],\n",
            "        [1151,    0],\n",
            "        [ 368,    0],\n",
            "        [ 126,    0],\n",
            "        [ 120,    0],\n",
            "        [  13,    0],\n",
            "        [   9,    0],\n",
            "        [ 221,    0],\n",
            "        [ 403,    0],\n",
            "        [ 990,    0],\n",
            "        [ 341,    0],\n",
            "        [1937,    0],\n",
            "        [ 843,    0],\n",
            "        [ 542,    0],\n",
            "        [  29,    0],\n",
            "        [ 115,    0],\n",
            "        [ 646,    0],\n",
            "        [ 339,    0],\n",
            "        [ 661,    0],\n",
            "        [ 265,    0],\n",
            "        [  13,    0],\n",
            "        [ 221,    0],\n",
            "        [ 403,    0],\n",
            "        [   4,    0],\n",
            "        [  29,    0],\n",
            "        [ 480,    0],\n",
            "        [   8,    0],\n",
            "        [ 449,    0],\n",
            "        [  20,    0],\n",
            "        [ 676,    0],\n",
            "        [  84,    0],\n",
            "        [ 165,    0],\n",
            "        [ 952,    0],\n",
            "        [  75,    0],\n",
            "        [  73,    0],\n",
            "        [1537,    0],\n",
            "        [ 265,    0],\n",
            "        [ 153,    0],\n",
            "        [ 227,    0],\n",
            "        [3976,    0],\n",
            "        [ 267,    0],\n",
            "        [  75,    0],\n",
            "        [ 245,    0],\n",
            "        [2517,    0],\n",
            "        [1184,    0],\n",
            "        [  65,    0],\n",
            "        [ 833,    0],\n",
            "        [  84,    0],\n",
            "        [ 172,    0],\n",
            "        [   4,    0],\n",
            "        [  11,    0],\n",
            "        [ 541,    0],\n",
            "        [  27,    0],\n",
            "        [ 936,    0],\n",
            "        [ 327,    0],\n",
            "        [ 195,    0],\n",
            "        [  23,    0],\n",
            "        [8757,    0],\n",
            "        [ 100,    0],\n",
            "        [ 368,    0],\n",
            "        [ 126,    0],\n",
            "        [ 386,    0],\n",
            "        [ 221,    0],\n",
            "        [ 403,    0],\n",
            "        [ 134,    0],\n",
            "        [   8,    0],\n",
            "        [  15,    0],\n",
            "        [ 256,    0],\n",
            "        [ 118,    0],\n",
            "        [  14,    0],\n",
            "        [1078,    0],\n",
            "        [  86,    0],\n",
            "        [  91,    0],\n",
            "        [ 221,    0],\n",
            "        [ 403,    0],\n",
            "        [ 134,    0],\n",
            "        [ 118,    0],\n",
            "        [ 120,    0],\n",
            "        [  38,    0],\n",
            "        [1774,    0],\n",
            "        [2099,    0],\n",
            "        [2078,    0],\n",
            "        [   8,    0],\n",
            "        [  58,    0],\n",
            "        [  51,    0],\n",
            "        [ 163,    0],\n",
            "        [  29,    0],\n",
            "        [  24,    0],\n",
            "        [  19,    0],\n",
            "        [  14,    0],\n",
            "        [ 548,    0],\n",
            "        [ 247,    0],\n",
            "        [  54,    0]], device='cuda:0')\n",
            "TARGET TENSOR:  tensor([[    1,     1],\n",
            "        [  242,   108],\n",
            "        [   39,     7],\n",
            "        [   17,    20],\n",
            "        [  696,    48],\n",
            "        [   18,    13],\n",
            "        [  303,  1215],\n",
            "        [  558,    24],\n",
            "        [   45,   248],\n",
            "        [    8,     6],\n",
            "        [ 3029,     2],\n",
            "        [    7,     0],\n",
            "        [   14,     0],\n",
            "        [  283,     0],\n",
            "        [  148,     0],\n",
            "        [  193,     0],\n",
            "        [  737,     0],\n",
            "        [    7,     0],\n",
            "        [    4,     0],\n",
            "        [    5,     0],\n",
            "        [   45,     0],\n",
            "        [   95,     0],\n",
            "        [    8,     0],\n",
            "        [ 2962,     0],\n",
            "        [ 1861,     0],\n",
            "        [   14,     0],\n",
            "        [  214,     0],\n",
            "        [  172,     0],\n",
            "        [ 3029,     0],\n",
            "        [    7,     0],\n",
            "        [   11,     0],\n",
            "        [  318,     0],\n",
            "        [  638,     0],\n",
            "        [   18,     0],\n",
            "        [    8,     0],\n",
            "        [  147,     0],\n",
            "        [   14,     0],\n",
            "        [   13,     0],\n",
            "        [  293,     0],\n",
            "        [   44,     0],\n",
            "        [  672,     0],\n",
            "        [   44,     0],\n",
            "        [  599,     0],\n",
            "        [  798,     0],\n",
            "        [12208,     0],\n",
            "        [   31,     0],\n",
            "        [   35,     0],\n",
            "        [  174,     0],\n",
            "        [  449,     0],\n",
            "        [ 1593,     0],\n",
            "        [    4,     0],\n",
            "        [    5,     0],\n",
            "        [  174,     0],\n",
            "        [  595,     0],\n",
            "        [   15,     0],\n",
            "        [  174,     0],\n",
            "        [ 2067,     0],\n",
            "        [   34,     0],\n",
            "        [  321,     0],\n",
            "        [  518,     0],\n",
            "        [   15,     0],\n",
            "        [  561,     0],\n",
            "        [    7,     0],\n",
            "        [   18,     0],\n",
            "        [   13,     0],\n",
            "        [  330,     0],\n",
            "        [ 2097,     0],\n",
            "        [   18,     0],\n",
            "        [    8,     0],\n",
            "        [  570,     0],\n",
            "        [ 2572,     0],\n",
            "        [   85,     0],\n",
            "        [  137,     0],\n",
            "        [  546,     0],\n",
            "        [  119,     0],\n",
            "        [  270,     0],\n",
            "        [    4,     0],\n",
            "        [    5,     0],\n",
            "        [ 3029,     0],\n",
            "        [    7,     0],\n",
            "        [   16,     0],\n",
            "        [   11,     0],\n",
            "        [  318,     0],\n",
            "        [  638,     0],\n",
            "        [   18,     0],\n",
            "        [    8,     0],\n",
            "        [  147,     0],\n",
            "        [   14,     0],\n",
            "        [   16,     0],\n",
            "        [  330,     0],\n",
            "        [  798,     0],\n",
            "        [   15,     0],\n",
            "        [  241,     0],\n",
            "        [  226,     0],\n",
            "        [  100,     0],\n",
            "        [   40,     0],\n",
            "        [   16,     0],\n",
            "        [  330,     0],\n",
            "        [  798,     0],\n",
            "        [  122,     0],\n",
            "        [  413,     0],\n",
            "        [   18,     0],\n",
            "        [ 1768,     0],\n",
            "        [ 2783,     0],\n",
            "        [   15,     0],\n",
            "        [ 1546,     0],\n",
            "        [   11,     0],\n",
            "        [   21,     0],\n",
            "        [   11,     0],\n",
            "        [  182,     0],\n",
            "        [   42,     0],\n",
            "        [    2,     0]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "for src_sents, tgt_sents in batch_iter(dev_data, batch_size=2):\n",
        "  src_tensor = vocab.src.to_input_tensor(src_sents)\n",
        "  tgt_tensor = vocab.tgt.to_input_tensor(tgt_sents)\n",
        "\n",
        "  print(\"SOURCE TENSOR: \", src_tensor)\n",
        "  print(\"TARGET TENSOR: \", tgt_tensor)\n",
        "  break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TVBV17b9N3Z"
      },
      "source": [
        "# The Seq2Seq Model 1: RNNs with global attention\n",
        "\n",
        "In this section, we describe the training procedure for the proposed NMT system, which uses a Bidirectional LSTM Encoder and a Unidirectional LSTM Decoder.\n",
        "\n",
        "<img src=\"https://i.ibb.co/pjRW6tC/arc.png\" alt=\"arc\" border=\"0\" width=70%>\n",
        "\n",
        "# Model description (training procedure)\n",
        "\n",
        "Given a sentence in the source language, we look up the character or word embeddings from an **embeddings matrix**, yielding $x_1,...,x_m (x_i \\in \\mathbb{R}^e)$, where $m$ is the length of the source sentence and e is the embedding size. We feed the embeddings to the bidirectional encoder, yielding hidden states and cell states for both the forwards (‚Üí) and backwards (‚Üê) LSTMs. The forwards and backwards versions are concatenated to give hidden states $h^{enc}_i$ and cell states $c^{enc}_i$ :\n",
        "\n",
        "$$ h^{enc}_i = [\\overleftarrow{h^{enc}_i}; \\overrightarrow{h^{enc}_i}] \\:\\: \\text{where} \\:\\: h^{enc}_i \\in \\mathbb{R}^{2h \\times 1} $$\n",
        "$$ c^{enc}_i = [\\overleftarrow{c^{enc}_i}; \\overrightarrow{c^{enc}_i}] \\:\\: \\text{where} \\:\\: c^{enc}_i \\in \\mathbb{R}^{2h \\times 1} $$ \\\\\n",
        "\n",
        "We then initialize the **decoder**‚Äôs first hidden state $h^{enc}_0$ and cell state $c^{enc}_0$  with a linear projection of the encoder‚Äôs final hidden state and final cell state.\n",
        "\n",
        "$$ h^{dec}_0 = W_h[\\overleftarrow{h^{enc}_1}; \\overrightarrow{h^{enc}_m}] \\:\\: \\text{where} \\:\\: h^{dec}_0 \\in \\mathbb{R}^{h \\times 1} $$\n",
        "$$ c^{dec}_0 = W_c[\\overleftarrow{c^{enc}_1}; \\overrightarrow{c^{enc}_m}] \\:\\: \\text{where} \\:\\: c^{dec}_0 \\in \\mathbb{R}^{h \\times 1} $$ \\\\\n",
        "\n",
        "With the decoder initialized, we must now feed it a target sentence. On the $t^{th}$ step, we look up the embedding for the $t^{th}$ subword, $y_t \\in \\mathbb{R}^{e \\times 1}$ . We then concatenate $y_t$ with the combined-output vector $o_{t-1} \\in \\mathbb{R}^{h \\times 1}$ from the previous timestep (we will explain what this is later!) to produce $\\bar{y_t} \\in \\mathbb{R}^{(e+h) \\times 1}$. Note that for the first target subword (i.e. the start token) $o_0$ is a zero-vector. We then feed $\\bar{y_t}$ as input to the decoder.\n",
        "\n",
        "\n",
        "$$ h^{dec}_t , c^{dec}_t = \\text{Decoder}(\\bar{y_t},  h^{dec}_{t-1} , c^{dec}_{t-1} ) \\:\\:\\: \\text{where} \\:\\:\\: h^{dec}_t \\in \\mathbb{R}^{h \\times 1} , c^{dec}_t \\in \\mathbb{R}^{h \\times 1} $$ \\\\\n",
        "\n",
        "We then use $h^{dec}_t$ to compute multiplicative attention over $h^{enc}_1,...,, h^{enc}_m$ :\n",
        "\n",
        "$$ e_{t,i} = (h_t^{dec})^TW_{attProj}h^{enc}_i \\:\\:\\: \\text{where} \\:\\:\\: e_t \\in \\mathbb{R}^{m \\times 1}, W_{attProj} \\in \\mathbb{R}^{h \\times 2h} $$\n",
        "\n",
        "$$ \\alpha_t = softmax(e_t) \\:\\:\\: \\text{where} \\:\\:\\: \\alpha_t \\in \\mathbb{R}^{m \\times 1}$$\n",
        "\n",
        "$$ a_t = ‚àë_{i=1}^m \\alpha_{t, i} h^{enc}_i \\:\\:\\: \\text{where} \\:\\:\\: a_t \\in \\mathbb{R}^{2h \\times 1}$$ \\\\\n",
        "\n",
        "We now concatenate the attention output $a_t$ with the decoder hidden state $h^{dec}_t$ and pass this through a linear layer, tanh, and dropout to attain the *combined-output* vector $o_t$.\n",
        "\n",
        "$$ u_t = [a_t;h^{dec}_t] \\:\\:\\: \\text{where} \\:\\:\\: u_t \\in \\mathbb{R}^{3h \\times 1} $$\n",
        "\n",
        "$$ v_t = W_uu_t \\:\\:\\: where \\:\\:\\: v_t \\in \\mathbb{R}^{h \\times 1},W_u \\in \\mathbb{R}^{h \\times 3h}$$\n",
        "\n",
        "$$ o_t = dropout(tanh(v_t)) \\:\\:\\: where \\:\\:\\: o_t \\in \\mathbb{R}^{h \\times 1}$$ \\\\\n",
        "\n",
        "Then, we produce a probability distribution $P_t$ over target subwords at the $t^{th}$ timestep:\n",
        "\n",
        "$$ P_t = softmax(W_{vocab}o_t) \\:\\:\\: where \\:\\:\\: P_t \\in \\mathbb{R}^{V_t \\times 1}, W_{vocab}\\in \\mathbb{R}^{V_t \\times h} $$\n",
        "\n",
        "Here, $V_t$ is the size of the target vocabulary. Finally, to train the network we then compute the cross entropy loss between $P_t$ and $g_t$, where $g_t$ is the one-hot vector of the target subword at timestep $t$:\n",
        "\n",
        "$$ J_t(Œ∏) = CrossEntropy(P_t, g_t)$$\n",
        "Here, $Œ∏$ represents all the parameters of the model and $J_t(Œ∏)$ is the loss on step t of the decoder.\n",
        "\n",
        "Now that we have described the model, let‚Äôs try implementing it Mandarin Vietnamese to English translation!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLAaS2fZ4aBS"
      },
      "source": [
        "## Q2 (5 points) Embedding Layer Initilization\n",
        "\n",
        "Implement the `__init__` function to initialize the necessary source and target embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9zylrls4d6A"
      },
      "outputs": [],
      "source": [
        "class ModelEmbeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Class that converts input words to their embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_size, vocab):\n",
        "        \"\"\"\n",
        "        Init the Embedding layers.\n",
        "\n",
        "        @param embed_size (int): Embedding size (dimensionality)\n",
        "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
        "                              See vocab.py for documentation.\n",
        "        \"\"\"\n",
        "        super(ModelEmbeddings, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        # default values\n",
        "        # self.source = None\n",
        "        # self.target = None\n",
        "\n",
        "        src_pad_token_idx = vocab.src['<pad>']\n",
        "        tgt_pad_token_idx = vocab.tgt['<pad>']\n",
        "\n",
        "        ### YOUR CODE HERE (~2 Lines)\n",
        "        ### TODO - Initialize the following variables:\n",
        "        ###     self.source (Embedding Layer for source language)\n",
        "        ###     self.target (Embedding Layer for target langauge)\n",
        "        ###\n",
        "        ### Note:\n",
        "        ###     1. `vocab` object contains two vocabularies:\n",
        "        ###            `vocab.src` for source\n",
        "        ###            `vocab.tgt` for target\n",
        "        ###     2. You can get the length of a specific vocabulary by running:\n",
        "        ###             `len(vocab.<specific_vocabulary>)`\n",
        "        ###     3. Remember to include the padding token for the specific vocabulary\n",
        "        ###        when creating your Embedding.\n",
        "        ###\n",
        "        ### Use the following docs to properly initialize these variables:\n",
        "        ###     Embedding Layer:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding\n",
        "        self.source = nn.Embedding(num_embeddings=len(vocab.src), embedding_dim=embed_size, padding_idx=src_pad_token_idx)\n",
        "        self.target = nn.Embedding(num_embeddings=len(vocab.tgt), embedding_dim=embed_size, padding_idx=tgt_pad_token_idx)\n",
        "        ### END YOUR CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtcWFUr4NKoF"
      },
      "source": [
        "## Q3-6 RNN with Global attention NMT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQedhWy5JE6I"
      },
      "source": [
        "\n",
        "### Q3 (10 points) Initialize layers in NMT model\n",
        "Implement the `init` function to initialize the necessary model layers (LSTM, projection, and dropout) for the NMT system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZukPek7NPbn"
      },
      "source": [
        "###  Q4 (15 points) Encoder\n",
        "Implement the `encode` function. This function converts the padded source sentences into the tensor $X$, generates $h^{enc}_1 , . . . , h^{enc}_m $, and computes the initial state $h^{dec}_0$ and initial cell  $h^{dec}_0$ for the $\\text{Decoder}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOFoVBl-NSBf"
      },
      "source": [
        "\n",
        "### Q5 (15 points) Decoder\n",
        "Implement the `decode` function. This function constructs $\\bar{y}$ and runs the step function over every timestep for the input.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKOojY7ENUf0"
      },
      "source": [
        "### Q6: (20 points) Decoder step\n",
        "Implement the `step` function. This function applies the Decoder‚Äôs LSTM cell for a single timestep, computing the encoding of the target subword $h^{dec}_t$ , the attention scores $e_t$, attention distribution $\\alpha_t$, the attention output $a_t$, and finally the combined output $o_t$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfa3jewziImC"
      },
      "outputs": [],
      "source": [
        "Hypothesis = namedtuple('Hypothesis', ['value', 'score'])\n",
        "\n",
        "class NMT(nn.Module):\n",
        "    \"\"\" Simple Neural Machine Translation Model:\n",
        "        - Bidrectional LSTM Encoder\n",
        "        - Unidirection LSTM Decoder\n",
        "        - Global Attention Model (Luong, et al. 2015)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_size, hidden_size, vocab, dropout_rate=0.2):\n",
        "        \"\"\" Init NMT Model.\n",
        "\n",
        "        @param embed_size (int): Embedding size (dimensionality)\n",
        "        @param hidden_size (int): Hidden Size, the size of hidden states (dimensionality)\n",
        "        @param vocab (Vocab): Vocabulary object containing src and tgt languages\n",
        "                              See vocab.py for documentation.\n",
        "        @param dropout_rate (float): Dropout probability, for attention\n",
        "        \"\"\"\n",
        "        print(\"hidden_size\", hidden_size)\n",
        "        super(NMT, self).__init__()\n",
        "        self.model_embeddings = ModelEmbeddings(embed_size, vocab)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.vocab = vocab\n",
        "\n",
        "        # default values\n",
        "        # self.encoder = None\n",
        "        # self.decoder = None\n",
        "        # self.h_projection = None\n",
        "        # self.c_projection = None\n",
        "        # self.att_projection = None\n",
        "        # self.combined_output_projection = None\n",
        "        # self.target_vocab_projection = None\n",
        "        # self.dropout = None\n",
        "        # For sanity check only, not relevant to implementation\n",
        "        self.gen_sanity_check = False\n",
        "        self.counter = 0\n",
        "\n",
        "        ### YOUR CODE HERE (~9 Lines)\n",
        "        ### TODO - Initialize the following variables IN THIS ORDER:\n",
        "        ###     self.post_embed_cnn (Conv1d layer with kernel size 2, input and output channels = embed_size,\n",
        "        ###         padding = same to preserve output shape )\n",
        "        ###     self.encoder (Bidirectional LSTM with bias)\n",
        "        ###     self.decoder (LSTM Cell with bias)\n",
        "        ###     self.h_projection (Linear Layer with no bias), called W_{h} .\n",
        "        ###     self.c_projection (Linear Layer with no bias), called W_{c} .\n",
        "        ###     self.att_projection (Linear Layer with no bias), called W_{attProj}.\n",
        "        ###     self.combined_output_projection (Linear Layer with no bias), called W_{u}.\n",
        "        ###     self.target_vocab_projection (Linear Layer with no bias), called W_{vocab}.\n",
        "        ###     self.dropout (Dropout Layer)\n",
        "        ###\n",
        "        ### Use the following docs to properly initialize these variables:\n",
        "        ###     LSTM:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM\n",
        "        ###     LSTM Cell:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell\n",
        "        ###     Linear Layer:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Linear\n",
        "        ###     Dropout Layer:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
        "        ###     Conv1D Layer:\n",
        "        ###         https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html\n",
        "        self.post_embed_cnn = nn.Conv1d(in_channels=embed_size, out_channels=embed_size, kernel_size=2, padding=\"same\")\n",
        "        self.encoder = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, bidirectional=True) # has bias by default\n",
        "        self.decoder = nn.LSTMCell(input_size=embed_size+hidden_size, hidden_size=hidden_size) # has bias by default\n",
        "        self.h_projection = nn.Linear(in_features=hidden_size*2, out_features=hidden_size, bias=False)\n",
        "        self.c_projection = nn.Linear(in_features=hidden_size*2, out_features=hidden_size, bias=False)\n",
        "        self.att_projection = nn.Linear(in_features=hidden_size*2, out_features=hidden_size, bias=False)\n",
        "        self.combined_output_projection = nn.Linear(in_features=hidden_size*3, out_features=hidden_size, bias=False)\n",
        "        self.target_vocab_projection = nn.Linear(in_features=hidden_size, out_features=len(vocab.tgt), bias=False)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def forward(self, source: List[List[str]], target: List[List[str]]) -> torch.Tensor:\n",
        "        \"\"\" Take a mini-batch of source and target sentences, compute the log-likelihood of\n",
        "        target sentences under the language models learned by the NMT system.\n",
        "\n",
        "        @param source (List[List[str]]): list of source sentence tokens\n",
        "        @param target (List[List[str]]): list of target sentence tokens, wrapped by `<s>` and `</s>`\n",
        "\n",
        "        @returns scores (Tensor): a variable/tensor of shape (b, ) representing the\n",
        "                                    log-likelihood of generating the gold-standard target sentence for\n",
        "                                    each example in the input batch. Here b = batch size.\n",
        "        \"\"\"\n",
        "        # Compute sentence lengths\n",
        "        # source_lengths = [len(s) for s in source]\n",
        "        source_lengths = [len(s) if len(s) <= args.max_len else args.max_len for s in source]\n",
        "\n",
        "        # Convert list of lists into tensors\n",
        "        source_padded = self.vocab.src.to_input_tensor(source, device=self.device)  # Tensor: (src_len, b)\n",
        "        target_padded = self.vocab.tgt.to_input_tensor(target, device=self.device)  # Tensor: (tgt_len, b)\n",
        "\n",
        "        ###     Run the network forward:\n",
        "        ###     1. Apply the encoder to `source_padded` by calling `self.encode()`\n",
        "        ###     2. Generate sentence masks for `source_padded` by calling `self.generate_sent_masks()`\n",
        "        ###     3. Apply the decoder to compute combined-output by calling `self.decode()`\n",
        "        ###     4. Compute log probability distribution over the target vocabulary using the\n",
        "        ###        combined_outputs returned by the `self.decode()` function.\n",
        "\n",
        "        enc_hiddens, dec_init_state = self.encode(source_padded, source_lengths)\n",
        "        enc_masks = self.generate_sent_masks(enc_hiddens, source_lengths)\n",
        "        combined_outputs = self.decode(enc_hiddens, enc_masks, dec_init_state, target_padded)\n",
        "        P = F.log_softmax(self.target_vocab_projection(combined_outputs), dim=-1)\n",
        "\n",
        "        # Zero out, probabilities for which we have nothing in the target text\n",
        "        target_masks = (target_padded != self.vocab.tgt['<pad>']).float()\n",
        "\n",
        "        # Compute log probability of generating true target words\n",
        "        target_gold_words_log_prob = torch.gather(P, index=target_padded[1:].unsqueeze(-1), dim=-1).squeeze(\n",
        "            -1) * target_masks[1:]\n",
        "        scores = target_gold_words_log_prob.sum(dim=0)\n",
        "        return scores\n",
        "\n",
        "    def encode(self, source_padded: torch.Tensor, source_lengths: List[int]) -> Tuple[\n",
        "        torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        \"\"\" Apply the encoder to source sentences to obtain encoder hidden states.\n",
        "            Additionally, take the final states of the encoder and project them to obtain initial states for decoder.\n",
        "\n",
        "        @param source_padded (Tensor): Tensor of padded source sentences with shape (src_len, b), where\n",
        "                                        b = batch_size, src_len = maximum source sentence length. Note that\n",
        "                                       these have already been sorted in order of longest to shortest sentence.\n",
        "        @param source_lengths (List[int]): List of actual lengths for each of the source sentences in the batch\n",
        "        @returns enc_hiddens (Tensor): Tensor of hidden units with shape (b, src_len, h*2), where\n",
        "                                        b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
        "        @returns dec_init_state (tuple(Tensor, Tensor)): Tuple of tensors representing the decoder's initial\n",
        "                                                hidden state and cell. Both tensors should have shape (2, b, h).\n",
        "        \"\"\"\n",
        "        enc_hiddens, dec_init_state = None, None\n",
        "\n",
        "        ### YOUR CODE HERE (~ 11 Lines)\n",
        "        ### TODO:\n",
        "        ###     1. Construct Tensor `X` of source sentences with shape (src_len, b, e) using the source model embeddings.\n",
        "        ###         src_len = maximum source sentence length, b = batch size, e = embedding size. Note\n",
        "        ###         that there is no initial hidden state or cell for the encoder.\n",
        "        ###     2. Apply the post_embed_cnn layer. Before feeding X into the CNN, first use torch.permute to change the\n",
        "        ###         shape of X to (b, e, src_len). After getting the output from the CNN, still stored in the X variable,\n",
        "        ###         remember to use torch.permute again to revert X back to its original shape.\n",
        "        ###     3. Compute `enc_hiddens`, `last_hidden`, `last_cell` by applying the encoder to `X`.\n",
        "        ###         - Before you can apply the encoder, you need to apply the `pack_padded_sequence` function to X.\n",
        "        ###         - After you apply the encoder, you need to apply the `pad_packed_sequence` function to enc_hiddens.\n",
        "        ###         - Note that the shape of the tensor output returned by the encoder RNN is (src_len, b, h*2) and we want to\n",
        "        ###           return a tensor of shape (b, src_len, h*2) as `enc_hiddens`, so you may need to do more permuting.\n",
        "        ###         - Note on using pad_packed_sequence -> For batched inputs, you need to make sure that each of the\n",
        "        ###           individual input examples has the same shape.\n",
        "        ###     4. Compute `dec_init_state` = (init_decoder_hidden, init_decoder_cell):\n",
        "        ###         - `init_decoder_hidden`:\n",
        "        ###             `last_hidden` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.\n",
        "        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).\n",
        "        ###             Apply the h_projection layer to this in order to compute init_decoder_hidden.\n",
        "        ###             This is h_0^{dec} in the PDF. Here b = batch size, h = hidden size\n",
        "        ###         - `init_decoder_cell`:\n",
        "        ###             `last_cell` is a tensor shape (2, b, h). The first dimension corresponds to forwards and backwards.\n",
        "        ###             Concatenate the forwards and backwards tensors to obtain a tensor shape (b, 2*h).\n",
        "        ###             Apply the c_projection layer to this in order to compute init_decoder_cell.\n",
        "        ###             This is c_0^{dec} in the PDF. Here b = batch size, h = hidden size\n",
        "        ###\n",
        "        ### See the following docs, as you may need to use some of the following functions in your implementation:\n",
        "        ###     Pack the padded sequence X before passing to the encoder:\n",
        "        ###         https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
        "        ###     Pad the packed sequence, enc_hiddens, returned by the encoder:\n",
        "        ###         https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html\n",
        "        ###     Tensor Concatenation:\n",
        "        ###         https://pytorch.org/docs/stable/generated/torch.cat.html\n",
        "        ###     Tensor Permute:\n",
        "        ###         https://pytorch.org/docs/stable/generated/torch.permute.html\n",
        "        ###     Tensor Reshape (a possible alternative to permute):\n",
        "        ###         https://pytorch.org/docs/stable/generated/torch.Tensor.reshape.html\n",
        "        # src_len = max(source_lengths)\n",
        "        # print(\"src_len\", src_len)\n",
        "        # print(\"source_padded shape\", source_padded.shape) # shape (src_len, b)\n",
        "        X = self.model_embeddings.source(source_padded) # shape(src_len, b, e)\n",
        "        # print(\"X 1st shape\", X.shape)\n",
        "        X = X.permute(1, 2, 0) # shape(b, e, src_len)\n",
        "        # print(\"X 2nd shape\", X.shape)\n",
        "        X = self.post_embed_cnn(X) # shape(b, e, src_len)\n",
        "        # print(\"X 3rd shape\", X.shape)\n",
        "        X = X.permute(2, 0, 1) # shape(src_len, b, e)\n",
        "        # print(\"X 4th shape\", X.shape)\n",
        "        # print(\"source_lengths size\", len(source_lengths))\n",
        "        enc_hiddens = self.encoder(pack_padded_sequence(input=X, lengths=source_lengths))[0]\n",
        "        enc_hiddens = pad_packed_sequence(sequence=enc_hiddens)[0] # shape(src_len, b, h*2)\n",
        "        enc_hiddens = enc_hiddens.permute(1, 0, 2) # shape(b, src_len, h*2)\n",
        "        # last_hidden, last_cell = self.encoder(X)[1] # shape(2, b, h), shape(2, b, h)\n",
        "        last_hidden, last_cell = self.encoder(pack_padded_sequence(input=X, lengths=source_lengths))[1] # shape(2, b, h), shape(2, b, h)\n",
        "        # print(\"concatenate hidden\", torch.cat((last_hidden[0], last_hidden[1]), dim=-1).shape) # torch.Size([32, 1536]) -> shape(b, 2*h)\n",
        "        init_decoder_hidden = self.h_projection(torch.cat((last_hidden[0], last_hidden[1]), dim=-1)) # shape(b, 2*h)\n",
        "        # print(\"init_decoder_hidden\", init_decoder_hidden.shape)\n",
        "        # print(\"concatenate cell\", torch.cat((last_cell[0], last_cell[1]), dim=-1).shape) # torch.Size([32, 1536]) -> shape(b, 2*h)\n",
        "        init_decoder_cell = self.c_projection(torch.cat((last_cell[0], last_cell[1]), dim=-1)) # shape(b, 2*h)\n",
        "        dec_init_state = (init_decoder_hidden, init_decoder_cell)\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        return enc_hiddens, dec_init_state\n",
        "\n",
        "    def decode(self, enc_hiddens: torch.Tensor, enc_masks: torch.Tensor,\n",
        "               dec_init_state: Tuple[torch.Tensor, torch.Tensor], target_padded: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Compute combined output vectors for a batch.\n",
        "\n",
        "        @param enc_hiddens (Tensor): Hidden states (b, src_len, h*2), where\n",
        "                                     b = batch size, src_len = maximum source sentence length, h = hidden size.\n",
        "        @param enc_masks (Tensor): Tensor of sentence masks (b, src_len), where\n",
        "                                     b = batch size, src_len = maximum source sentence length.\n",
        "        @param dec_init_state (tuple(Tensor, Tensor)): Initial state and cell for decoder\n",
        "        @param target_padded (Tensor): Gold-standard padded target sentences (tgt_len, b), where\n",
        "                                       tgt_len = maximum target sentence length, b = batch size.\n",
        "\n",
        "        @returns combined_outputs (Tensor): combined output tensor  (tgt_len, b,  h), where\n",
        "                                        tgt_len = maximum target sentence length, b = batch_size,  h = hidden size\n",
        "        \"\"\"\n",
        "        # Chop off the <END> token for max length sentences.\n",
        "        target_padded = target_padded[:-1]\n",
        "\n",
        "        # Initialize the decoder state (hidden and cell)\n",
        "        dec_state = dec_init_state\n",
        "\n",
        "        # Initialize previous combined output vector o_{t-1} as zero\n",
        "        batch_size = enc_hiddens.size(0)\n",
        "        o_prev = torch.zeros(batch_size, self.hidden_size, device=self.device)\n",
        "\n",
        "        # Initialize a list we will use to collect the combined output o_t on each step\n",
        "        combined_outputs = []\n",
        "\n",
        "        ### YOUR CODE HERE (~9 Lines)\n",
        "        ### TODO:\n",
        "        ###     1. Apply the attention projection layer to `enc_hiddens` to obtain `enc_hiddens_proj`,\n",
        "        ###         which should be shape (b, src_len, h),\n",
        "        ###         where b = batch size, src_len = maximum source length, h = hidden size.\n",
        "        ###         This is applying W_{attProj} to h^enc, as described in the PDF.\n",
        "        ###     2. Construct tensor `Y` of target sentences with shape (tgt_len, b, e) using the target model embeddings.\n",
        "        ###         where tgt_len = maximum target sentence length, b = batch size, e = embedding size.\n",
        "        ###     3. Use the torch.split function to iterate over the time dimension of Y.\n",
        "        ###         Within the loop, this will give you Y_t of shape (1, b, e) where b = batch size, e = embedding size.\n",
        "        ###             - Squeeze Y_t into a tensor of dimension (b, e).\n",
        "        ###             - Construct Ybar_t by concatenating Y_t with o_prev on their last dimension\n",
        "        ###             - Use the step function to compute the the Decoder's next (cell, state) values\n",
        "        ###               as well as the new combined output o_t.\n",
        "        ###             - Append o_t to combined_outputs\n",
        "        ###             - Update o_prev to the new o_t.\n",
        "        ###     4. Use torch.stack to convert combined_outputs from a list length tgt_len of\n",
        "        ###         tensors shape (b, h), to a single tensor shape (tgt_len, b, h)\n",
        "        ###         where tgt_len = maximum target sentence length, b = batch size, h = hidden size.\n",
        "        ###\n",
        "        ### Note:\n",
        "        ###    - When using the squeeze() function make sure to specify the dimension you want to squeeze\n",
        "        ###      over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n",
        "        ###\n",
        "        ### You may find some of these functions useful:\n",
        "        ###     Zeros Tensor:\n",
        "        ###         https://pytorch.org/docs/stable/torch.html#torch.zeros\n",
        "        ###     Tensor Splitting (iteration):\n",
        "        ###         https://pytorch.org/docs/stable/torch.html#torch.split\n",
        "        ###     Tensor Dimension Squeezing:\n",
        "        ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze\n",
        "        ###     Tensor Concatenation:\n",
        "        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n",
        "        ###     Tensor Stacking:\n",
        "        ###         https://pytorch.org/docs/stable/torch.html#torch.stack\n",
        "        enc_hiddens_proj = self.att_projection(enc_hiddens) # shape(b, src_len, h)\n",
        "        Y = self.model_embeddings.target(target_padded) #shape(tgt_len, b, e)\n",
        "        for Y_t in torch.split(Y, split_size_or_sections=1):\n",
        "            Y_t = Y_t.squeeze() # defaut dim=0 # shape(b, e)\n",
        "            Ybar_t = torch.cat(tensors=(Y_t, o_prev), dim=-1)\n",
        "            (dec_state, o_t, e_t) = self.step(Ybar_t, dec_state, enc_hiddens, enc_hiddens_proj, enc_masks) # dec_state, combined_output, e_t\n",
        "            combined_outputs.append(o_t)\n",
        "            o_prev = o_t\n",
        "        combined_outputs = torch.stack(tensors=combined_outputs) # shape(tgt_len, b, h)\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        return combined_outputs\n",
        "\n",
        "    def step(self, Ybar_t: torch.Tensor,\n",
        "             dec_state: Tuple[torch.Tensor, torch.Tensor],\n",
        "             enc_hiddens: torch.Tensor,\n",
        "             enc_hiddens_proj: torch.Tensor,\n",
        "             enc_masks: torch.Tensor) -> Tuple[Tuple, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\" Compute one forward step of the LSTM decoder, including the attention computation.\n",
        "\n",
        "        @param Ybar_t (Tensor): Concatenated Tensor of [Y_t o_prev], with shape (b, e + h). The input for the decoder,\n",
        "                                where b = batch size, e = embedding size, h = hidden size.\n",
        "        @param dec_state (tuple(Tensor, Tensor)): Tuple of tensors both with shape (b, h), where b = batch size, h = hidden size.\n",
        "                First tensor is decoder's prev hidden state, second tensor is decoder's prev cell.\n",
        "        @param enc_hiddens (Tensor): Encoder hidden states Tensor, with shape (b, src_len, h * 2), where b = batch size,\n",
        "                                    src_len = maximum source length, h = hidden size.\n",
        "        @param enc_hiddens_proj (Tensor): Encoder hidden states Tensor, projected from (h * 2) to h. Tensor is with shape (b, src_len, h),\n",
        "                                    where b = batch size, src_len = maximum source length, h = hidden size.\n",
        "        @param enc_masks (Tensor): Tensor of sentence masks shape (b, src_len),\n",
        "                                    where b = batch size, src_len is maximum source length.\n",
        "\n",
        "        @returns dec_state (tuple (Tensor, Tensor)): Tuple of tensors both shape (b, h), where b = batch size, h = hidden size.\n",
        "                First tensor is decoder's new hidden state, second tensor is decoder's new cell.\n",
        "        @returns combined_output (Tensor): Combined output Tensor at timestep t, shape (b, h), where b = batch size, h = hidden size.\n",
        "        @returns e_t (Tensor): Tensor of shape (b, src_len). It is attention scores distribution.\n",
        "                                Note: You will not use this outside of this function.\n",
        "                                      We are simply returning this value so that we can sanity check\n",
        "                                      your implementation.\n",
        "        \"\"\"\n",
        "\n",
        "        combined_output = None\n",
        "\n",
        "        ### YOUR CODE HERE (~3 Lines)\n",
        "        ### TODO:\n",
        "        ###     1. Apply the decoder to `Ybar_t` and `dec_state`to obtain the new dec_state.\n",
        "        ###     2. Split dec_state into its two parts (dec_hidden, dec_cell)\n",
        "        ###     3. Compute the attention scores e_t, a Tensor shape (b, src_len).\n",
        "        ###        Note: b = batch_size, src_len = maximum source length, h = hidden size.\n",
        "        ###\n",
        "        ###       Hints:\n",
        "        ###         - dec_hidden is shape (b, h) and corresponds to h^dec_t in the PDF (batched)\n",
        "        ###         - enc_hiddens_proj is shape (b, src_len, h) and corresponds to W_{attProj} h^enc (batched).\n",
        "        ###         - Use batched matrix multiplication (torch.bmm) to compute e_t (be careful about the input/ output shapes!)\n",
        "        ###         - To get the tensors into the right shapes for bmm, you will need to do some squeezing and unsqueezing.\n",
        "        ###         - When using the squeeze() function make sure to specify the dimension you want to squeeze\n",
        "        ###             over. Otherwise, you will remove the batch dimension accidentally, if batch_size = 1.\n",
        "        ###\n",
        "        ### Use the following docs to implement this functionality:\n",
        "        ###     Batch Multiplication:\n",
        "        ###        https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
        "        ###     Tensor Unsqueeze:\n",
        "        ###         https://pytorch.org/docs/stable/torch.html#torch.unsqueeze\n",
        "        ###     Tensor Squeeze:\n",
        "        ###         https://pytorch.org/docs/stable/torch.html#torch.squeeze\n",
        "        # print(\"Ybar_t\", Ybar_t.shape) # torch.Size([32, 1792]) -> shape(b, e + h)\n",
        "        dec_state = self.decoder(Ybar_t, dec_state)\n",
        "        (dec_hidden, dec_cell) = dec_state\n",
        "        # print(\"dec_hidden\", dec_hidden.unsqueeze(1).transpose(1, 2).shape) # torch.Size([32, 768, 1]) -> shape(b, h, 1)\n",
        "        # print(\"enc_hiddens_proj\", enc_hiddens_proj.shape) # torch.Size([32, 64, 768]) -> shape(b, src_len, h)\n",
        "        # print(\"dec_cell\", dec_cell.shape) # torch.Size([32, 768]) -> shape(b, h)\n",
        "        # print(\"src_len\", enc_hiddens_proj.shape[1])\n",
        "        # e_t = torch.bmm(input=dec_hidden.unsqueeze(1).transpose(1, 2), mat2=enc_hiddens_proj) #shape(b, src_len)\n",
        "        e_t = torch.bmm(input=enc_hiddens_proj, mat2=dec_hidden.unsqueeze(1).transpose(1, 2)).squeeze(-1) #shape(b, src_len)\n",
        "        # print(\"e_t\", e_t.shape) # torch.Size([32, 64]) -> shape(b, src_len)\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        # Set e_t to -inf where enc_masks has 1\n",
        "        if enc_masks is not None:\n",
        "            e_t.data.masked_fill_(enc_masks.bool(), -float('inf'))\n",
        "\n",
        "        ### YOUR CODE HERE (~6 Lines)\n",
        "        ### TODO:\n",
        "        ###     1. Apply softmax to e_t to yield alpha_t\n",
        "        ###     2. Use batched matrix multiplication between alpha_t and enc_hiddens to obtain the\n",
        "        ###         attention output vector, a_t.\n",
        "        # $$     Hints:\n",
        "        ###           - alpha_t is shape (b, src_len)\n",
        "        ###           - enc_hiddens is shape (b, src_len, 2h)\n",
        "        ###           - a_t should be shape (b, 2h)\n",
        "        ###           - You will need to do some squeezing and unsqueezing.\n",
        "        ###     Note: b = batch size, src_len = maximum source length, h = hidden size.\n",
        "        ###\n",
        "        ###     3. Concatenate dec_hidden with a_t to compute tensor U_t\n",
        "        ###     4. Apply the combined output projection layer to U_t to compute tensor V_t\n",
        "        ###     5. Compute tensor O_t by first applying the Tanh function and then the dropout layer.\n",
        "        ###\n",
        "        ### Use the following docs to implement this functionality:\n",
        "        ###     Softmax:\n",
        "        ###         https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.softmax\n",
        "        ###     Batch Multiplication:\n",
        "        ###        https://pytorch.org/docs/stable/torch.html#torch.bmm\n",
        "        ###     Tensor View:\n",
        "        ###         https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
        "        ###     Tensor Concatenation:\n",
        "        ###         https://pytorch.org/docs/stable/torch.html#torch.cat\n",
        "        ###     Tanh:\n",
        "        ###         https://pytorch.org/docs/stable/torch.html#torch.tanh\n",
        "        alpha_t = nn.functional.softmax(input=e_t, dim=-1) # shape(b, src_len)\n",
        "        # print(\"alpha_t\", alpha_t.shape)\n",
        "        a_t = torch.bmm(alpha_t.unsqueeze(1), enc_hiddens).squeeze(1)  # shape(b, 2h)\n",
        "        # print(\"a_t\", a_t.shape)\n",
        "        # dec_hidden -> shape(b, h)\n",
        "        # a_t -> shape(b, 2h)\n",
        "        U_t = torch.cat(tensors=(dec_hidden, a_t), dim=-1)\n",
        "        # print(\"U_t\", U_t.shape) # shape(b, 3h)\n",
        "        # combined_output_projection shape(2h, h)\n",
        "        V_t = self.combined_output_projection(U_t)\n",
        "        O_t = torch.tanh(input=V_t)\n",
        "        O_t = self.dropout(O_t)\n",
        "        ### END YOUR CODE\n",
        "\n",
        "        combined_output = O_t\n",
        "        return dec_state, combined_output, e_t\n",
        "\n",
        "    def generate_sent_masks(self, enc_hiddens: torch.Tensor, source_lengths: List[int]) -> torch.Tensor:\n",
        "        \"\"\" Generate sentence masks for encoder hidden states.\n",
        "\n",
        "        @param enc_hiddens (Tensor): encodings of shape (b, src_len, 2*h), where b = batch size,\n",
        "                                     src_len = max source length, h = hidden size.\n",
        "        @param source_lengths (List[int]): List of actual lengths for each of the sentences in the batch.\n",
        "\n",
        "        @returns enc_masks (Tensor): Tensor of sentence masks of shape (b, src_len),\n",
        "                                    where src_len = max source length, h = hidden size.\n",
        "        \"\"\"\n",
        "        enc_masks = torch.zeros(enc_hiddens.size(0), enc_hiddens.size(1), dtype=torch.float)\n",
        "        for e_id, src_len in enumerate(source_lengths):\n",
        "            enc_masks[e_id, src_len:] = 1\n",
        "        return enc_masks.to(self.device)\n",
        "\n",
        "    def beam_search(self, src_sent: List[str], beam_size: int = 5, max_decoding_time_step: int = 70) -> List[\n",
        "        Hypothesis]:\n",
        "        \"\"\" Given a single source sentence, perform beam search, yielding translations in the target language.\n",
        "        @param src_sent (List[str]): a single source sentence (words)\n",
        "        @param beam_size (int): beam size\n",
        "        @param max_decoding_time_step (int): maximum number of time steps to unroll the decoding RNN\n",
        "        @returns hypotheses (List[Hypothesis]): a list of hypothesis, each hypothesis has two fields:\n",
        "                value: List[str]: the decoded target sentence, represented as a list of words\n",
        "                score: float: the log-likelihood of the target sentence\n",
        "        \"\"\"\n",
        "        src_sents_var = self.vocab.src.to_input_tensor([src_sent], self.device)\n",
        "\n",
        "        src_encodings, dec_init_vec = self.encode(src_sents_var, [len(src_sent)])\n",
        "        src_encodings_att_linear = self.att_projection(src_encodings)\n",
        "\n",
        "        h_tm1 = dec_init_vec\n",
        "        att_tm1 = torch.zeros(1, self.hidden_size, device=self.device)\n",
        "\n",
        "        eos_id = self.vocab.tgt['</s>']\n",
        "\n",
        "        hypotheses = [['<s>']]\n",
        "        hyp_scores = torch.zeros(len(hypotheses), dtype=torch.float, device=self.device)\n",
        "        completed_hypotheses = []\n",
        "\n",
        "        t = 0\n",
        "        while len(completed_hypotheses) < beam_size and t < max_decoding_time_step:\n",
        "            t += 1\n",
        "            hyp_num = len(hypotheses)\n",
        "\n",
        "            exp_src_encodings = src_encodings.expand(hyp_num,\n",
        "                                                     src_encodings.size(1),\n",
        "                                                     src_encodings.size(2))\n",
        "\n",
        "            exp_src_encodings_att_linear = src_encodings_att_linear.expand(hyp_num,\n",
        "                                                                           src_encodings_att_linear.size(1),\n",
        "                                                                           src_encodings_att_linear.size(2))\n",
        "\n",
        "            y_tm1 = torch.tensor([self.vocab.tgt[hyp[-1]] for hyp in hypotheses], dtype=torch.long, device=self.device)\n",
        "            y_t_embed = self.model_embeddings.target(y_tm1)\n",
        "\n",
        "            x = torch.cat([y_t_embed, att_tm1], dim=-1)\n",
        "\n",
        "            (h_t, cell_t), att_t, _ = self.step(x, h_tm1,\n",
        "                                                exp_src_encodings, exp_src_encodings_att_linear, enc_masks=None)\n",
        "\n",
        "            # log probabilities over target words\n",
        "            log_p_t = F.log_softmax(self.target_vocab_projection(att_t), dim=-1)\n",
        "\n",
        "            live_hyp_num = beam_size - len(completed_hypotheses)\n",
        "            contiuating_hyp_scores = (hyp_scores.unsqueeze(1).expand_as(log_p_t) + log_p_t).view(-1)\n",
        "            top_cand_hyp_scores, top_cand_hyp_pos = torch.topk(contiuating_hyp_scores, k=live_hyp_num)\n",
        "\n",
        "            prev_hyp_ids = torch.div(top_cand_hyp_pos, len(self.vocab.tgt), rounding_mode='floor')\n",
        "            hyp_word_ids = top_cand_hyp_pos % len(self.vocab.tgt)\n",
        "\n",
        "            new_hypotheses = []\n",
        "            live_hyp_ids = []\n",
        "            new_hyp_scores = []\n",
        "\n",
        "            for prev_hyp_id, hyp_word_id, cand_new_hyp_score in zip(prev_hyp_ids, hyp_word_ids, top_cand_hyp_scores):\n",
        "                prev_hyp_id = prev_hyp_id.item()\n",
        "                hyp_word_id = hyp_word_id.item()\n",
        "                cand_new_hyp_score = cand_new_hyp_score.item()\n",
        "\n",
        "                hyp_word = self.vocab.tgt.id2word[hyp_word_id]\n",
        "                new_hyp_sent = hypotheses[prev_hyp_id] + [hyp_word]\n",
        "                if hyp_word == '</s>':\n",
        "                    completed_hypotheses.append(Hypothesis(value=new_hyp_sent[1:-1],\n",
        "                                                           score=cand_new_hyp_score))\n",
        "                else:\n",
        "                    new_hypotheses.append(new_hyp_sent)\n",
        "                    live_hyp_ids.append(prev_hyp_id)\n",
        "                    new_hyp_scores.append(cand_new_hyp_score)\n",
        "\n",
        "            if len(completed_hypotheses) == beam_size:\n",
        "                break\n",
        "\n",
        "            live_hyp_ids = torch.tensor(live_hyp_ids, dtype=torch.long, device=self.device)\n",
        "            h_tm1 = (h_t[live_hyp_ids], cell_t[live_hyp_ids])\n",
        "            att_tm1 = att_t[live_hyp_ids]\n",
        "\n",
        "            hypotheses = new_hypotheses\n",
        "            hyp_scores = torch.tensor(new_hyp_scores, dtype=torch.float, device=self.device)\n",
        "\n",
        "        if len(completed_hypotheses) == 0:\n",
        "            completed_hypotheses.append(Hypothesis(value=hypotheses[0][1:],\n",
        "                                                   score=hyp_scores[0].item()))\n",
        "\n",
        "        completed_hypotheses.sort(key=lambda hyp: hyp.score, reverse=True)\n",
        "\n",
        "        return completed_hypotheses\n",
        "\n",
        "    @property\n",
        "    def device(self) -> torch.device:\n",
        "        \"\"\" Determine which device to place the Tensors upon, CPU or GPU.\n",
        "        \"\"\"\n",
        "        return self.model_embeddings.source.weight.device\n",
        "\n",
        "    @staticmethod\n",
        "    def load(model_path: str):\n",
        "        \"\"\" Load the model from a file.\n",
        "        @param model_path (str): path to model\n",
        "        \"\"\"\n",
        "        params = torch.load(model_path, map_location=lambda storage, loc: storage)\n",
        "        args = params['args']\n",
        "        model = NMT(vocab=params['vocab'], **args)\n",
        "        model.load_state_dict(params['state_dict'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def save(self, path: str):\n",
        "        \"\"\" Save the odel to a file.\n",
        "        @param path (str): path to the model\n",
        "        \"\"\"\n",
        "        print('save model parameters to [%s]' % path, file=sys.stderr)\n",
        "\n",
        "        params = {\n",
        "            'args': dict(embed_size=self.model_embeddings.embed_size, hidden_size=self.hidden_size,\n",
        "                         dropout_rate=self.dropout_rate),\n",
        "            'vocab': self.vocab,\n",
        "            'state_dict': self.state_dict()\n",
        "        }\n",
        "\n",
        "        torch.save(params, path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ViJAE00fMKw7"
      },
      "source": [
        "Now it‚Äôs time to get things running!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJjqKeVjNdJt"
      },
      "source": [
        "## Evaluating function\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80dkhcV5lT91"
      },
      "outputs": [],
      "source": [
        "def evaluate_ppl(model, dev_data, batch_size=32):\n",
        "    \"\"\" Evaluate perplexity on dev sentences\n",
        "    @param model (NMT): NMT Model\n",
        "    @param dev_data (list of (src_sent, tgt_sent)): list of tuples containing source and target sentence\n",
        "    @param batch_size (batch size)\n",
        "    @returns ppl (perplixty on dev sentences)\n",
        "    \"\"\"\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    cum_loss = 0.\n",
        "    cum_tgt_words = 0.\n",
        "\n",
        "    # no_grad() signals backend to throw away all gradients\n",
        "    with torch.no_grad():\n",
        "        for src_sents, tgt_sents in batch_iter(dev_data, batch_size):\n",
        "            loss = -model(src_sents, tgt_sents).sum()\n",
        "\n",
        "            cum_loss += loss.item()\n",
        "            tgt_word_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
        "            cum_tgt_words += tgt_word_num_to_predict\n",
        "\n",
        "        ppl = np.exp(cum_loss / cum_tgt_words)\n",
        "\n",
        "    if was_training:\n",
        "        model.train()\n",
        "\n",
        "    return ppl\n",
        "\n",
        "def compute_corpus_level_bleu_score(references: List[List[str]], hypotheses: List[Hypothesis]) -> float:\n",
        "    \"\"\" Given decoding results and reference sentences, compute corpus-level BLEU score.\n",
        "    @param references (List[List[str]]): a list of gold-standard reference target sentences\n",
        "    @param hypotheses (List[Hypothesis]): a list of hypotheses, one for each reference\n",
        "    @returns bleu_score: corpus-level BLEU score\n",
        "    \"\"\"\n",
        "    # remove the start and end tokens\n",
        "    if references[0][0] == '<s>':\n",
        "        references = [ref[1:-1] for ref in references]\n",
        "\n",
        "    # detokenize the subword pieces to get full sentences\n",
        "    detokened_refs = [''.join(pieces).replace('‚ñÅ', ' ') for pieces in references]\n",
        "    detokened_hyps = [''.join(hyp.value).replace('‚ñÅ', ' ') for hyp in hypotheses]\n",
        "\n",
        "    # sacreBLEU can take multiple references (golden example per sentence) but we only feed it one\n",
        "    bleu = sacrebleu.corpus_bleu(detokened_hyps, [detokened_refs])\n",
        "\n",
        "    return bleu.score, detokened_refs, detokened_hyps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM4LMEmT62FL"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etmZ3f8Ei1vG",
        "outputId": "01328a20-a396-4aa7-f247-29e7959ed6e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hidden_size 768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "uniformly initialize parameters [-0.100000, +0.100000]\n"
          ]
        }
      ],
      "source": [
        "# Initialize our model and optimizer\n",
        "model = NMT(embed_size=args.embed_size,\n",
        "            hidden_size=args.hidden_size,\n",
        "            dropout_rate=float(args.dropout),\n",
        "            vocab=vocab)\n",
        "model.train()\n",
        "\n",
        "uniform_init = float(args.uniform_init)\n",
        "if np.abs(uniform_init) > 0.:\n",
        "    print('uniformly initialize parameters [-%f, +%f]' % (uniform_init, uniform_init), file=sys.stderr)\n",
        "    for p in model.parameters():\n",
        "        p.data.uniform_(-uniform_init, uniform_init)\n",
        "\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=float(args.lr))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHL9PVKB7iQa"
      },
      "source": [
        "We wil first train our model on a small training set of 50 samples and evaluate it on a small dev set of 50 samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBinbqo068vb"
      },
      "outputs": [],
      "source": [
        "max_train = 50\n",
        "\n",
        "train_data_small = [val for val in train_data if len(val[0]) > 3][:max_train]\n",
        "dev_data_small = [val for val in dev_data if len(val[0]) > 3][:max_train]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFp33trgjLpP",
        "outputId": "f1f9c2da-8c97-48fc-eb91-910c5460d981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "begin Maximum Likelihood training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py:370: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at ../aten/src/ATen/native/Convolution.cpp:1036.)\n",
            "  return F.conv1d(\n",
            "epoch 4, iter 10, avg. loss 195.20, avg. ppl 1529.50 cum. examples 250, speed 2004.38 words/sec, time elapsed 3.32 sec\n",
            "epoch 9, iter 20, avg. loss 124.45, avg. ppl 107.25 cum. examples 500, speed 2693.65 words/sec, time elapsed 5.79 sec\n",
            "epoch 14, iter 30, avg. loss 89.33, avg. ppl 28.66 cum. examples 750, speed 2704.17 words/sec, time elapsed 8.25 sec\n",
            "epoch 19, iter 40, avg. loss 67.60, avg. ppl 12.67 cum. examples 1000, speed 2691.00 words/sec, time elapsed 10.73 sec\n",
            "epoch 24, iter 50, avg. loss 54.60, avg. ppl 7.78 cum. examples 1250, speed 2861.99 words/sec, time elapsed 13.05 sec\n",
            "epoch 29, iter 60, avg. loss 40.96, avg. ppl 4.66 cum. examples 1500, speed 2684.63 words/sec, time elapsed 15.53 sec\n",
            "epoch 34, iter 70, avg. loss 29.14, avg. ppl 2.99 cum. examples 1750, speed 2718.93 words/sec, time elapsed 17.98 sec\n",
            "epoch 39, iter 80, avg. loss 18.41, avg. ppl 2.00 cum. examples 2000, speed 2782.08 words/sec, time elapsed 20.37 sec\n",
            "epoch 44, iter 90, avg. loss 10.39, avg. ppl 1.48 cum. examples 2250, speed 2732.63 words/sec, time elapsed 22.81 sec\n",
            "epoch 49, iter 100, avg. loss 5.27, avg. ppl 1.22 cum. examples 2500, speed 2687.48 words/sec, time elapsed 25.28 sec\n",
            "epoch 54, iter 110, avg. loss 2.50, avg. ppl 1.10 cum. examples 2750, speed 2679.59 words/sec, time elapsed 27.77 sec\n",
            "epoch 59, iter 120, avg. loss 1.38, avg. ppl 1.05 cum. examples 3000, speed 2632.73 words/sec, time elapsed 30.29 sec\n",
            "epoch 64, iter 130, avg. loss 0.81, avg. ppl 1.03 cum. examples 3250, speed 2660.17 words/sec, time elapsed 32.79 sec\n",
            "epoch 69, iter 140, avg. loss 0.98, avg. ppl 1.04 cum. examples 3500, speed 2671.40 words/sec, time elapsed 35.29 sec\n",
            "epoch 74, iter 150, avg. loss 0.64, avg. ppl 1.02 cum. examples 3750, speed 2615.08 words/sec, time elapsed 37.83 sec\n",
            "epoch 79, iter 160, avg. loss 0.37, avg. ppl 1.01 cum. examples 4000, speed 2671.43 words/sec, time elapsed 40.32 sec\n",
            "epoch 84, iter 170, avg. loss 0.22, avg. ppl 1.01 cum. examples 4250, speed 2650.73 words/sec, time elapsed 42.83 sec\n",
            "epoch 89, iter 180, avg. loss 0.13, avg. ppl 1.00 cum. examples 4500, speed 2623.41 words/sec, time elapsed 45.37 sec\n",
            "epoch 94, iter 190, avg. loss 0.10, avg. ppl 1.00 cum. examples 4750, speed 2645.07 words/sec, time elapsed 47.89 sec\n",
            "epoch 99, iter 200, avg. loss 0.13, avg. ppl 1.00 cum. examples 5000, speed 2750.26 words/sec, time elapsed 50.31 sec\n"
          ]
        }
      ],
      "source": [
        "num_trial = 0\n",
        "train_iter = patience = cum_loss = report_loss = cum_tgt_words = report_tgt_words = 0\n",
        "cum_examples = report_examples = epoch = valid_num = 0\n",
        "hist_valid_scores = []\n",
        "train_time = begin_time = time.time()\n",
        "print('begin Maximum Likelihood training')\n",
        "\n",
        "for epoch in range(args.max_epoch):\n",
        "    for src_sents, tgt_sents in batch_iter(train_data_small, batch_size=args.batch_size, shuffle=True):\n",
        "        train_iter += 1\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        batch_size = len(src_sents)\n",
        "\n",
        "        example_losses = -model(src_sents, tgt_sents) # (batch_size,)\n",
        "        batch_loss = example_losses.sum()\n",
        "        loss = batch_loss / batch_size\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        # clip gradient\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip_grad)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_losses_val = batch_loss.item()\n",
        "        report_loss += batch_losses_val\n",
        "        cum_loss += batch_losses_val\n",
        "\n",
        "        tgt_words_num_to_predict = sum(len(s[1:]) for s in tgt_sents)  # omitting leading `<s>`\n",
        "        report_tgt_words += tgt_words_num_to_predict\n",
        "        cum_tgt_words += tgt_words_num_to_predict\n",
        "        report_examples += batch_size\n",
        "        cum_examples += batch_size\n",
        "\n",
        "        if train_iter % args.log_every == 0:\n",
        "            print('epoch %d, iter %d, avg. loss %.2f, avg. ppl %.2f ' \\\n",
        "                    'cum. examples %d, speed %.2f words/sec, time elapsed %.2f sec' % (epoch, train_iter,\n",
        "                                                                                        report_loss / report_examples,\n",
        "                                                                                        math.exp(report_loss / report_tgt_words),\n",
        "                                                                                        cum_examples,\n",
        "                                                                                        report_tgt_words / (time.time() - train_time),\n",
        "                                                                                        time.time() - begin_time), file=sys.stderr)\n",
        "\n",
        "            train_time = time.time()\n",
        "            report_loss = report_tgt_words = report_examples = 0.\n",
        "\n",
        "        # perform validation\n",
        "        if train_iter % args.valid_niter == 0:\n",
        "            print('epoch %d, iter %d, cum. loss %.2f, cum. ppl %.2f cum. examples %d' % (epoch, train_iter,\n",
        "                                                                                        cum_loss / cum_examples,\n",
        "                                                                                        np.exp(cum_loss / cum_tgt_words),\n",
        "                                                                                        cum_examples), file=sys.stderr)\n",
        "\n",
        "            cum_loss = cum_examples = cum_tgt_words = 0.\n",
        "            valid_num += 1\n",
        "\n",
        "            print('begin validation ...', file=sys.stderr)\n",
        "\n",
        "            # compute dev. ppl and bleu\n",
        "            dev_ppl = evaluate_ppl(model, dev_data, batch_size=128)   # dev batch size can be a bit larger\n",
        "            valid_metric = -dev_ppl\n",
        "\n",
        "            print('validation: iter %d, dev. ppl %f' % (train_iter, dev_ppl), file=sys.stderr)\n",
        "\n",
        "            is_better = len(hist_valid_scores) == 0 or valid_metric > max(hist_valid_scores)\n",
        "            hist_valid_scores.append(valid_metric)\n",
        "\n",
        "            if is_better:\n",
        "                patience = 0\n",
        "                print('save currently the best model to [%s]' % args.model_save_path, file=sys.stderr)\n",
        "                model.save(args.model_save_path)\n",
        "\n",
        "                # also save the optimizers' state\n",
        "                torch.save(optimizer.state_dict(), args.model_save_path + '.optim')\n",
        "            elif patience < int(args.patience):\n",
        "                patience += 1\n",
        "                print('hit patience %d' % patience, file=sys.stderr)\n",
        "\n",
        "                if patience == int(args.patience):\n",
        "                    num_trial += 1\n",
        "                    print('hit #%d trial' % num_trial, file=sys.stderr)\n",
        "                    if num_trial == int(args.max_num_trial):\n",
        "                        print('early stop!', file=sys.stderr)\n",
        "                        exit(0)\n",
        "\n",
        "                    # decay lr, and restore from previously best checkpoint\n",
        "                    lr = optimizer.param_groups[0]['lr'] * float(args.lr_decay)\n",
        "                    print('load previously best model and decay learning rate to %f' % lr, file=sys.stderr)\n",
        "\n",
        "                    # load model\n",
        "                    params = torch.load(args.model_save_path, map_location=lambda storage, loc: storage)\n",
        "                    model.load_state_dict(params['state_dict'])\n",
        "                    model = model.to(device)\n",
        "\n",
        "                    print('restore parameters of the optimizers', file=sys.stderr)\n",
        "                    optimizer.load_state_dict(torch.load(args.model_save_path + '.optim'))\n",
        "\n",
        "                    # set new lr\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        param_group['lr'] = lr\n",
        "\n",
        "                    # reset patience\n",
        "                    patience = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1kxo8fYNC_n"
      },
      "source": [
        "## Testing the model\n",
        "\n",
        "Beam search is an algorithm used in many NLP and speech recognition models as a final decision making layer to choose the best output given target variables like maximum probability or next output character. The figure is referenced from the blog [Sequence to Sequence (seq2seq) and Attention](https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html).\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1rjVuANPjwTQi33beysq_LRNCxd6xhRHY\" width=\"800\" height=\"400\"/>\n",
        "\n",
        "The model uses `beam_search` function to yield the translations in the target language. `beam_search` will return a list of hypothesis for the translated sentences, each will have the value and the score for it, sorted in the descending order, so the first element in the Hypothesis is the best option for translated output by the model. The samples on training data should be very good with bleu score 100. The samples on validation data, however, probably won't make sense (because we're overfitting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pI1zdeCMNBvp",
        "outputId": "aaa06eea-1149-48e9-c3cf-bd35ecd28382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0:\n",
            "Bleu score: 100.00000000000004\n",
            "Source: ['‚ñÅKhoa', '‚ñÅh·ªçc', '‚ñÅƒë', '·∫±ng', '‚ñÅsau', '‚ñÅm·ªôt', '‚ñÅti√™u', '‚ñÅƒë·ªÅ', '‚ñÅv·ªÅ', '‚ñÅkh√≠', '‚ñÅh·∫≠u']\n",
            "Reference: [' Rachel Pike : The science behind a climate headline']\n",
            "Hypotheses: [' Rachel Pike : The science behind a climate headline', ' Rachel Pike behind The science behind a climate headline', ' Rachel Pike this The science behind a climate headline', ' Rachel Pike : The The science behind a climate headline', ' Rachel Pike : The science a climate behind a climate headline']\n",
            "\n",
            "\n",
            "Sample 1:\n",
            "Bleu score: 100.00000000000004\n",
            "Source: ['‚ñÅTro', 'ng', '‚ñÅ4', '‚ñÅph√∫t', '‚ñÅ,', '‚ñÅchuy√™n', '‚ñÅgia', '‚ñÅho√°', '‚ñÅh·ªçc', '‚ñÅkh√≠', '‚ñÅquy·ªÉn', '‚ñÅRachel', '‚ñÅP', 'ike', '‚ñÅgi·ªõi', '‚ñÅt', 'hi·ªáu', '‚ñÅs∆°', '‚ñÅl∆∞·ª£c', '‚ñÅv·ªÅ', '‚ñÅnh·ªØng', '‚ñÅn·ªó', '‚ñÅl·ª±c', '‚ñÅkhoa', '‚ñÅh·ªçc', '‚ñÅm', 'i·ªát', '‚ñÅm√†', 'i', '‚ñÅƒë', '·∫±ng', '‚ñÅsau', '‚ñÅnh·ªØng', '‚ñÅti√™u', '‚ñÅƒë·ªÅ', '‚ñÅt√°o', '‚ñÅb·∫°o', '‚ñÅv·ªÅ', '‚ñÅbi·∫øn', '‚ñÅƒë·ªïi', '‚ñÅkh√≠', '‚ñÅh·∫≠u', '‚ñÅ,', '‚ñÅc√πng', '‚ñÅv·ªõi', '‚ñÅƒëo√†n', '‚ñÅnghi√™n', '‚ñÅc·ª©u', '‚ñÅc·ªßa', '‚ñÅm√¨nh', '‚ñÅ--', '‚ñÅh√†ng', '‚ñÅng√†n', '‚ñÅng∆∞·ªùi', '‚ñÅƒë√£', '‚ñÅc·ªë', 'ng', '‚ñÅ', 'hi·∫øn', '‚ñÅcho', '‚ñÅd·ª±', '‚ñÅ√°n', '‚ñÅn√†y', '‚ñÅ--', '‚ñÅm·ªôt', '‚ñÅchuy·∫øn', '‚ñÅbay', '‚ñÅm·∫°o', '‚ñÅhi·ªÉm', '‚ñÅqua', '‚ñÅr·ª´ng', '‚ñÅgi√†', '‚ñÅƒë·ªÉ', '‚ñÅt√¨m', '‚ñÅki·∫øm', '‚ñÅth√¥ng', '‚ñÅtin', '‚ñÅv·ªÅ', '‚ñÅm·ªôt', '‚ñÅph√¢n', '‚ñÅt·ª≠', '‚ñÅthe', 'n', '‚ñÅch·ªë', 't', '‚ñÅ.']\n",
            "Reference: [' In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .']\n",
            "Hypotheses: [' In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .', ' In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on a key molecule .', ' In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a key molecule .', ' In 4 minutes , atmospheric chemist Rachel Pike provides a glimpse of the massive scientific effort behind the bold headlines on climate change , with her team -- one of thousands who contributed -- taking a risky flight over the rainforest in pursuit of data on the key molecule .']\n",
            "\n",
            "\n",
            "Sample 2:\n",
            "Bleu score: 100.00000000000004\n",
            "Source: ['‚ñÅT√¥i', '‚ñÅmu·ªën', '‚ñÅcho', '‚ñÅc√°c', '‚ñÅb·∫°n', '‚ñÅbi·∫øt', '‚ñÅv·ªÅ', '‚ñÅs·ª±', '‚ñÅto', '‚ñÅl·ªõn', '‚ñÅc·ªßa', '‚ñÅnh·ªØng', '‚ñÅn·ªó', '‚ñÅl·ª±c', '‚ñÅkhoa', '‚ñÅh·ªçc', '‚ñÅƒë√£', '‚ñÅg√≥p', '‚ñÅph·∫ßn', '‚ñÅl√†m', '‚ñÅn√™n', '‚ñÅc√°c', '‚ñÅd√≤', 'ng', '‚ñÅt√≠', 't', '‚ñÅb·∫°n', '‚ñÅth∆∞·ªùng', '‚ñÅth·∫•y', '‚ñÅtr√™n', '‚ñÅb√°o', '‚ñÅ.']\n",
            "Reference: [' I &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .']\n",
            "Hypotheses: [' I &apos;d like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .', ' I &apos; like like to talk to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .', ' I &apos;d like today to you today about the scale of the scientific effort that goes into making the headlines you see in the paper .', ' I &apos;d like to talk to you today about the scale of the scientific effort thateses into making the headlines you see in the paper .', ' I &apos;d like to talk to you today about the scientific effort that goes into making the headlines you see in the paper .']\n",
            "\n",
            "\n",
            "Sample 3:\n",
            "Bleu score: 100.00000000000004\n",
            "Source: ['‚ñÅC√≥', '‚ñÅnh·ªØng', '‚ñÅd√≤', 'ng', '‚ñÅtr', '√¥ng', '‚ñÅnh∆∞', '‚ñÅth·∫ø', '‚ñÅn√†y', '‚ñÅkhi', '‚ñÅb√†n', '‚ñÅv·ªÅ', '‚ñÅbi·∫øn', '‚ñÅƒë·ªïi', '‚ñÅkh√≠', '‚ñÅh·∫≠u', '‚ñÅ,', '‚ñÅv√†', '‚ñÅnh∆∞', '‚ñÅth·∫ø', '‚ñÅn√†y', '‚ñÅkhi', '‚ñÅn√≥i', '‚ñÅv·ªÅ', '‚ñÅch·∫•t', '‚ñÅl∆∞·ª£ng', '‚ñÅkh√¥ng', '‚ñÅkh√≠', '‚ñÅhay', '‚ñÅkh√≥i', '‚ñÅb·ª•', 'i', '‚ñÅ.']\n",
            "Reference: [' Headlines that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .']\n",
            "Hypotheses: [' Headlines that look like this when they have to do with climate change , and headlines that look like this when they have to do with air quality or smog .', ' Headlines that look like this when they have to do with climate change , and headlines that look look like this when they have to do with air quality or smog .', ' Headlines that look like this when they have to do with climate change , and headlines that look like this when they have to do withs or smog .', ' Headlines that look like this when they have to do with air change , and headlines that look like this when they have to do with air quality or smog .', ' Headlines that look like this when they have to do with climate change , and headlines that look like this when they have to do withs .']\n",
            "\n",
            "\n",
            "Sample 4:\n",
            "Bleu score: 100.00000000000004\n",
            "Source: ['‚ñÅC·∫£', '‚ñÅhai', '‚ñÅƒë·ªÅu', '‚ñÅl√†', '‚ñÅm·ªôt', '‚ñÅnh', '√°nh', '‚ñÅc·ªßa', '‚ñÅc√πng', '‚ñÅm·ªôt', '‚ñÅlƒ©nh', '‚ñÅv·ª±c', '‚ñÅtrong', '‚ñÅng√†nh', '‚ñÅkhoa', '‚ñÅh·ªçc', '‚ñÅkh√≠', '‚ñÅquy·ªÉn', '‚ñÅ.']\n",
            "Reference: [' They are both two branches of the same field of atmospheric science .']\n",
            "Hypotheses: [' They are both two branches of the same field of atmospheric science .', ' The are are both two branches of the same field of atmospheric science .', ' They are two two branches of the same field of atmospheric science .', ' They are both branches of the same field of atmospheric science .', ' They are both two branches of the same field field of atmospheric science .']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Testing on training set\n",
        "references = []\n",
        "hypotheses = []\n",
        "\n",
        "num_logs = 5\n",
        "if (num_logs > len(train_data_small)):\n",
        "  num_logs = len(train_data_small)\n",
        "\n",
        "for i in range(num_logs):\n",
        "  src, tgt = train_data_small[i]\n",
        "  references = [tgt]\n",
        "  hypotheses = model.beam_search(src)\n",
        "  bleu_score, detokened_refs, detokened_hyps = compute_corpus_level_bleu_score(references, hypotheses)\n",
        "  print(f\"Sample {i}:\")\n",
        "  print(\"Bleu score: \" + str(bleu_score))\n",
        "  print(\"Source: \" + str(src))\n",
        "  print(\"Reference: \" + str(detokened_refs))\n",
        "  print(\"Hypotheses: \" + str(detokened_hyps))\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxyncmAebxPR",
        "outputId": "073696c1-8e0a-4956-e692-bfd9a9c75c5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 0:\n",
            "Bleu score: 0.00560761438726327\n",
            "Source: ['‚ñÅL√†', 'm', '‚ñÅsao', '‚ñÅt√¥i', '‚ñÅc√≥', '‚ñÅth·ªÉ', '‚ñÅtr√¨nh', '‚ñÅb√†y', '‚ñÅtrong', '‚ñÅ10', '‚ñÅph√∫t', '‚ñÅv·ªÅ', '‚ñÅs·ª£', 'i', '‚ñÅd√¢y', '‚ñÅli√™n', '‚ñÅk·∫øt', '‚ñÅnh·ªØng', '‚ñÅng∆∞·ªùi', '‚ñÅph·ª•', '‚ñÅn·ªØ', '‚ñÅqua', '‚ñÅba', '‚ñÅth·∫ø', '‚ñÅh·ªá', '‚ñÅ,', '‚ñÅv·ªÅ', '‚ñÅvi·ªác', '‚ñÅl√†m', '‚ñÅth·∫ø', '‚ñÅn√†o', '‚ñÅnh·ªØng', '‚ñÅs·ª£', 'i', '‚ñÅd√¢y', '‚ñÅm·∫°nh', '‚ñÅm·∫Ω', '‚ñÅƒë√°ng', '‚ñÅkinh', '‚ñÅng·∫°c', '‚ñÅ·∫•y', '‚ñÅƒë√£', '‚ñÅn√≠', 'u', '‚ñÅch', '·∫∑t', '‚ñÅl·∫•y', '‚ñÅcu·ªôc', '‚ñÅs·ªëng', '‚ñÅc·ªßa', '‚ñÅm·ªôt', '‚ñÅc√¥', '‚ñÅb√©', '‚ñÅb·ªën', '‚ñÅtu·ªïi', '‚ñÅco', '‚ñÅqu', '·∫Øp', '‚ñÅv·ªõi', '‚ñÅƒë', '·ª©a', '‚ñÅem', '‚ñÅg√°i', '‚ñÅnh·ªè', '‚ñÅc·ªßa', '‚ñÅc√¥', '‚ñÅb√©', '‚ñÅ,', '‚ñÅv·ªõi', '‚ñÅm·∫π', '‚ñÅv√†', '‚ñÅb√†', '‚ñÅtrong', '‚ñÅsu·ªët', '‚ñÅnƒÉm', '‚ñÅng√†y', '‚ñÅƒë√™m', '‚ñÅtr√™n', '‚ñÅcon', '‚ñÅthuy·ªÅn', '‚ñÅnh·ªè', '‚ñÅl√™n', 'h', '‚ñÅƒë√™', 'nh', '‚ñÅtr√™n', '‚ñÅB', 'i·ªÉn', '‚ñÅƒê√¥ng', '‚ñÅh∆°n', '‚ñÅ30', '‚ñÅnƒÉm', '‚ñÅtr∆∞·ªõc', '‚ñÅ,', '‚ñÅnh·ªØng', '‚ñÅs·ª£', 'i', '‚ñÅd√¢y', '‚ñÅli√™n', '‚ñÅk·∫øt', '‚ñÅƒë√£', '‚ñÅn√≠', 'u', '‚ñÅl·∫•y', '‚ñÅcu·ªôc', '‚ñÅƒë·ªùi', '‚ñÅc√¥', '‚ñÅb√©', '‚ñÅ·∫•y', '‚ñÅv√†', '‚ñÅkh√¥ng', '‚ñÅbao', '‚ñÅgi·ªù', '‚ñÅ', 'r·ªùi', '‚ñÅƒëi', '‚ñÅ--', '‚ñÅc√¥', '‚ñÅb√©', '‚ñÅ·∫•y', '‚ñÅgi·ªù', '‚ñÅs·ªëng', '‚ñÅ·ªü', '‚ñÅSan', '‚ñÅFrancis', 'co', '‚ñÅv√†', '‚ñÅƒëang', '‚ñÅn√≥i', '‚ñÅchuy·ªán', '‚ñÅv·ªõi', '‚ñÅc√°c', '‚ñÅb·∫°n', '‚ñÅ', 'h√¥m', '‚ñÅnay', '‚ñÅ?']\n",
            "Reference: [' How can I speak in 10 minutes about the bonds of women over three generations , about how the astonishing strength of those bonds took hold in the life of a four-year-old girl huddled with her young sister , her mother and her grandmother for five days and nights in a small boat in the China Sea more than 30 years ago , bonds that took hold in the life of that small girl and never let go -- that small girl now living in San Francisco and speaking to you today ?']\n",
            "Hypotheses: [' And all of this to understand the chemistry of one molecule .', ' We weight of this to understand the chemistry of one molecule .', ' And all of this to understand the weight of this molecule .', ' Andy one of this to understand the chemistry of one molecule .', ' Andy one of this to understand the chemistry of the rainforest , to they of one of this molecule .']\n",
            "\n",
            "\n",
            "Sample 1:\n",
            "Bleu score: 6.567274736060395\n",
            "Source: ['‚ñÅC', '√¢u', '‚ñÅchuy·ªán', '‚ñÅn√†y', '‚ñÅch∆∞a', '‚ñÅk·∫øt', '‚ñÅth√∫c', '‚ñÅ.']\n",
            "Reference: [' This is not a finished story .']\n",
            "Hypotheses: [' And this plane , the world .', ' And this plane , the models , which is here .', ' And this plane this is the middle of the rainforest , and hungs .', ' And this plane this is the middle of the rainforest , and headlines .', ' And this plane this is they of the rainforest , and headlines .']\n",
            "\n",
            "\n",
            "Sample 2:\n",
            "Bleu score: 5.61480827173619\n",
            "Source: ['‚ñÅN√≥', '‚ñÅl√†', '‚ñÅm·ªôt', '‚ñÅtr√≤', '‚ñÅch∆°i', '‚ñÅgh√©p', '‚ñÅh√¨nh', '‚ñÅv·∫´', 'n', '‚ñÅƒëang', '‚ñÅƒë∆∞·ª£c', '‚ñÅx·∫øp', '‚ñÅ.']\n",
            "Reference: [' It is a jigsaw puzzle still being put together .']\n",
            "Hypotheses: [' It &apos;s a big community .', ' It &apos;s a big community , in they have to do withs of the rainforest .', ' It &apos;s a biged in the middle of the rainforest , and hung hundreds of thousands of dollars worth .', ' It &apos;s a big community , in they have to do withs of the rainforest in pursuit of data on a key molecule .', ' It &apos;s a big community , in they have to do withs of the rainforest in pursuit of data on a few molecules .']\n",
            "\n",
            "\n",
            "Sample 3:\n",
            "Bleu score: 5.816635421147513\n",
            "Source: ['‚ñÅH√£', 'y', '‚ñÅƒë·ªÉ', '‚ñÅt√¥i', '‚ñÅk·ªÉ', '‚ñÅcho', '‚ñÅc√°c', '‚ñÅb·∫°n', '‚ñÅv·ªÅ', '‚ñÅv√†', 'i', '‚ñÅm', '·∫£nh', '‚ñÅgh√©p', '‚ñÅnh√©', '‚ñÅ.']\n",
            "Reference: [' Let me tell you about some of the pieces .']\n",
            "Hypotheses: [' We blow it of , in the middle of the rainforest , and headlines .', ' We blow it of , or the middle of the rainforest , and headlines .', ' We blow it of , in the middle of the rainforest , and headlines on a few molecule .', ' We blow it of , in the middle of the rainforest , and headlines on climate change , or is what .', ' We blow it of , in the middle of the rainforest , and headlines on climate change , or is what I happen to study .']\n",
            "\n",
            "\n",
            "Sample 4:\n",
            "Bleu score: 16.94357181593088\n",
            "Source: ['‚ñÅH√£', 'y', '‚ñÅt∆∞·ªüng', '‚ñÅt∆∞·ª£ng', '‚ñÅm', '·∫£nh', '‚ñÅƒë·∫ßu', '‚ñÅti√™n', '‚ñÅ:', '‚ñÅm·ªôt', '‚ñÅng∆∞·ªùi', '‚ñÅƒë√†n', '‚ñÅ√¥ng', '‚ñÅƒë·ªë', 't', '‚ñÅch√°', 'y', '‚ñÅs·ª±', '‚ñÅnghi·ªáp', '‚ñÅc·∫£', '‚ñÅƒë·ªùi', '‚ñÅm√¨nh', '‚ñÅ.']\n",
            "Reference: [' Imagine the first piece : a man burning his life &apos;s work .']\n",
            "Hypotheses: [' We &apos;s on the middle of the rainforest , and hung hundreds .', ' We &apos;s on the middle of the rainforest , and hung hundreds that produce biofuels .', ' We &apos;s a glimpse of the middle of the rainforest , and hung hundreds of thousands of dollars worth .', ' We &apos;s on the middle of the rainforest , and hung hundreds of thousands of dollars worth of scientific equip', ' We &apos;s on the middle of the rainforest , and hung hundreds of thousands of dollars worth of scientific equipment off this is they of that .']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Testing on evaluate set\n",
        "references = []\n",
        "hypotheses = []\n",
        "\n",
        "num_logs = 5\n",
        "if (num_logs > len(dev_data_small)):\n",
        "  num_logs = len(dev_data_small)\n",
        "\n",
        "for i in range(num_logs):\n",
        "  src, tgt = dev_data_small[i]\n",
        "  references = [tgt]\n",
        "  hypotheses = model.beam_search(src)\n",
        "  bleu_score, detokened_refs, detokened_hyps = compute_corpus_level_bleu_score(references, hypotheses)\n",
        "  print(f\"Sample {i}:\")\n",
        "  print(\"Bleu score: \" + str(bleu_score))\n",
        "  print(\"Source: \" + str(src))\n",
        "  print(\"Reference: \" + str(detokened_refs))\n",
        "  print(\"Hypotheses: \" + str(detokened_hyps))\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsCdZ2dcjS-r"
      },
      "source": [
        "# The Seq2Seq Model 2: Transformer\n",
        "In this part, you will train a sequence-to-sequence Transformer model to translate Portuguese into English. The Transformer was originally proposed in \"Attention is all you need\" by Vaswani et al. (2017).\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/tutorials/transformer/apply_the_transformer_to_machine_translation.gif\" alt=\"Applying the Transformer to machine translation\">\n",
        "\n",
        "Figure 2: Applying the Transformer to machine translation. Source: [Google AI Blog](https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBdNGGPSj4V3"
      },
      "source": [
        "A Transformer is a sequence-to-sequence encoder-decoder model similar to the model in the [NMT with attention tutorial](https://www.tensorflow.org/text/tutorials/nmt_with_attention).\n",
        "A single-layer Transformer takes a little more code to write, but is almost identical to that encoder-decoder RNN model. The only difference is that the RNN layers are replaced with self attention layers.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th>The <a href=https://www.tensorflow.org/text/tutorials/nmt_with_attention>RNN+Attention model</a></th>\n",
        "  <th>A 1-layer transformer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=411 src=\"https://www.tensorflow.org/images/tutorials/transformer/RNN+attention-words.png\"/>\n",
        "  </td>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-1layer-words.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YagB7OYkE0i"
      },
      "source": [
        "### The Embedding and Positional Encoding Layer\n",
        "\n",
        "The inputs to both the encoder and decoder use the same embedding and positional encoding logic.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The embedding and positional encoding layer</th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://www.tensorflow.org/images/tutorials/transformer/PositionalEmbedding.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhPl3LVekMWD"
      },
      "source": [
        "The formula for calculating the positional encoding (implemented in Python below) is as follows:\n",
        "\n",
        "$$\\Large{PE_{(pos, 2i)} = \\sin(pos / 10000^{2i / d_{model}})} $$\n",
        "$$\\Large{PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i / d_{model}})} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ik3jYEAilHtB"
      },
      "source": [
        "## Q7: Transformer Embedding Layer (10 points)\n",
        "Implement `TransformerEmbedding` (consists of lookup embedding & positional encoding) for transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljJYuvtlj6uc"
      },
      "outputs": [],
      "source": [
        "class TransformerEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Class that combines token embeddings with positional embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_size, max_len, dropout_rate):\n",
        "        \"\"\"\n",
        "        Init the Transformer Embedding layer.\n",
        "\n",
        "        @param vocab_size (int): Vocabulary size (number of unique tokens)\n",
        "        @param embedding_size (int): Embedding size (dimensionality)\n",
        "        @param max_len (int): Maximum sequence length\n",
        "        @param dropout_rate (float): Dropout probability\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # default values\n",
        "        self.embedding_size = embedding_size\n",
        "        # self.token_embedding = None\n",
        "        # self.dropout = None\n",
        "        # pos_embedding = None\n",
        "\n",
        "        ### YOUR CODE HERE\n",
        "        ### TODO - Implement the positional embedding and Initialize the following variables :\n",
        "        ###     self.token_embedding (Embedding Layer)\n",
        "        ###     self.pos_embedding (Positional Embedding Layer), notes that pos_embedding is not learnable parameters,\n",
        "        ###         so we should use the self.register_buffer function to initialize it.\n",
        "        ###     self.dropout (Dropout Layer)\n",
        "        ###\n",
        "        ### Note:\n",
        "        ###     1. `vocab_size` represents the size of the vocabulary (number of unique tokens)\n",
        "        ###     2. `embedding_size` represents the size of each embedding vector\n",
        "        ###     3. `max_len` represents the maximum sequence length\n",
        "        ###     4. `dropout_rate` represents the dropout probability\n",
        "        ###\n",
        "        ### Use the following docs to properly initialize these variables:\n",
        "        ###     Embedding Layer:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Embeddingl\n",
        "        ###     Dropout Layer:\n",
        "        ###         https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout\n",
        "        ###\n",
        "        self.token_embedding = nn.Embedding(num_embeddings=max_len, embedding_dim=embedding_size)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        # pos_embedding = nn.Embedding(num_embeddings=max_len, embedding_dim=embedding_size)\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1)  # Positions 0 to max_len-1, shape (max_len, 1) //\n",
        "        div_term = torch.exp(torch.arange(0, embedding_size, 2) * -(math.log(10000.0) / embedding_size))\n",
        "        pos_embedding = torch.zeros(max_len, embedding_size)\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * div_term)  # Apply the sine function for even indices\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * div_term)  # Apply the cosine function for odd indices\n",
        "\n",
        "        pos_embedding = pos_embedding.unsqueeze(1).requires_grad_(False)\n",
        "        # pos_embedding = PositionalEncoding(embedding_size, dropout_rate, max_len)\n",
        "        ### END YOUR CODE\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Maps input sequences of tokens to their embeddings.\n",
        "\n",
        "        @param x (Tensor): Input tensor of tokens with shape (seq_len, batch_size)\n",
        "\n",
        "        @returns embedded (Tensor): Tensor of token embeddings with shape (seq_len, batch_size, embedding_size)\n",
        "        \"\"\"\n",
        "        # Retrieve token embeddings\n",
        "        ### YOUR CODE HERE (~1 Line) ###\n",
        "        embedded_tokens = self.token_embedding(x) # # [seq_len, batch_size, embedding_size]\n",
        "        # print(embedded_tokens.size())\n",
        "\n",
        "        # Retrieve positional embeddings for the appropriate segment of the input sequence\n",
        "        ### YOUR CODE HERE (~1 Line) ###\n",
        "        # embedded_positions = self.pos_embedding(x)\n",
        "        embedded_positions = self.pos_embedding[:x.size(0), :] # [seq_len, 1, embedding_size]\n",
        "        # Add token and positional embeddings together, apply dropout, and return\n",
        "        ### YOUR CODE HERE (~1 Line) ###\n",
        "        embedded = self.dropout(embedded_tokens + embedded_positions)\n",
        "        return embedded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6GgNEpvXK58"
      },
      "source": [
        "### Visualize the positional encoding\n",
        "\n",
        "Positional encoding can be tricky to implement correctly, even if the code runs. Use the test visualization below to confirm it produces the expected sinusoidal shape.\n",
        "The `PositionalEncoding` class is identical to the positional encoding logic within the `TransformerEmbedding`. We extracted it separately to facilitate debugging of the positional encoding values before their addition to the `embedded_tokens`. Copy the necessary code of `__init__` and `forward` of the positional encoding part for testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0BP3BQBXKYJ"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Class that combines token embeddings with positional embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_size, dropout_rate, max_len=5000):\n",
        "        \"\"\"\n",
        "        Init the Transformer Embedding layer.\n",
        "\n",
        "        @param vocab_size (int): Vocabulary size (number of unique tokens)\n",
        "        @param embedding_size (int): Embedding size (dimensionality)\n",
        "        @param max_len (int): Maximum sequence length\n",
        "        @param dropout_rate (float): Dropout probability\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # default values\n",
        "        self.embedding_size = embedding_size\n",
        "        # self.dropout = None\n",
        "        # pos_embedding = None\n",
        "\n",
        "        ### YOUR CODE HERE: COPY FROM YOUR PREVIOUS IMPLEMENTATION OF THE POSITIONAL ENCODING\n",
        "        ### TODO - Implement the positional embedding and Initialize the following variables :\n",
        "        ###     self.pos_embedding (Positional Embedding Layer), notes that pos_embedding is not learnable parameters,\n",
        "        ###         so we should use the self.register_buffer function to initialize it.\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        pos = torch.arange(0, max_len).unsqueeze(1)  # Positions 0 to max_len-1, shape (max_len, 1)\n",
        "        div_term = torch.exp(torch.arange(0, embedding_size, 2) * -(math.log(10000.0) / embedding_size))\n",
        "        pos_embedding = torch.zeros(max_len, embedding_size)\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * div_term)  # Apply the sine function for even indices\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * div_term)  # Apply the cosine function for odd indices\n",
        "        ### END YOUR CODE\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Maps input sequences of tokens to their embeddings.\n",
        "\n",
        "        @param x (Tensor): Input tensor of tokens with shape (seq_len, batch_size, embedding_size)\n",
        "\n",
        "        @returns embedded (Tensor): Tensor of token embeddings with shape (seq_len, batch_size, embedding_size)\n",
        "        \"\"\"\n",
        "\n",
        "        # Retrieve positional embeddings for the appropriate segment of the input sequence\n",
        "        ### YOUR CODE HERE (~1 Line) ###\n",
        "        # print(\"x\", x.shape) # shape (seq_len, batch_size, embedding_size)\n",
        "        embedded_positions = self.pos_embedding[:x.shape[0]].unsqueeze(1)\n",
        "        # print(\"embedded_positions\", embedded_positions.shape)\n",
        "\n",
        "        # Add token and positional embeddings together, apply dropout, and return\n",
        "        ### YOUR CODE HERE (~1 Line) ###\n",
        "        embedded = self.dropout(x + embedded_positions) # shape(seq_len, batch_size, embedding_size)\n",
        "        # print(\"embedded\", embedded.shape)\n",
        "        return embedded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JFxftbNXscz"
      },
      "outputs": [],
      "source": [
        "#@title Visualize functions\n",
        "import pandas as pd\n",
        "import altair as alt\n",
        "\n",
        "RUN_EXAMPLES = True\n",
        "\n",
        "def show_example(fn, args=[]):\n",
        "    if __name__ == \"__main__\" and RUN_EXAMPLES:\n",
        "        return fn(*args)\n",
        "\n",
        "def example_positional():\n",
        "    pe = PositionalEncoding(20, 0)\n",
        "    # We simulate data with shape (100, 1, 20) where src_len = 100, b = 1, e = 20.\n",
        "    y = pe.forward(torch.zeros(100, 1, 20))\n",
        "\n",
        "    data = pd.concat(\n",
        "        [\n",
        "            pd.DataFrame(\n",
        "                {\n",
        "                    \"embedding\": y[: , 0, dim],\n",
        "                    \"dimension\": dim,\n",
        "                    \"position\": list(range(100)),\n",
        "                }\n",
        "            )\n",
        "            for dim in [4, 5, 6, 7]\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        alt.Chart(data)\n",
        "        .mark_line()\n",
        "        .properties(width=800)\n",
        "        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n",
        "        .interactive()\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "9qQ2ymktYd4A",
        "outputId": "df69c7aa-8df8-460c-b8ce-640ab2f849f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "<div id=\"altair-viz-ceacc1d868cf42a8b7d6431e236317e1\"></div>\n",
              "<script type=\"text/javascript\">\n",
              "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
              "  (function(spec, embedOpt){\n",
              "    let outputDiv = document.currentScript.previousElementSibling;\n",
              "    if (outputDiv.id !== \"altair-viz-ceacc1d868cf42a8b7d6431e236317e1\") {\n",
              "      outputDiv = document.getElementById(\"altair-viz-ceacc1d868cf42a8b7d6431e236317e1\");\n",
              "    }\n",
              "    const paths = {\n",
              "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
              "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
              "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
              "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
              "    };\n",
              "\n",
              "    function maybeLoadScript(lib, version) {\n",
              "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
              "      return (VEGA_DEBUG[key] == version) ?\n",
              "        Promise.resolve(paths[lib]) :\n",
              "        new Promise(function(resolve, reject) {\n",
              "          var s = document.createElement('script');\n",
              "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
              "          s.async = true;\n",
              "          s.onload = () => {\n",
              "            VEGA_DEBUG[key] = version;\n",
              "            return resolve(paths[lib]);\n",
              "          };\n",
              "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
              "          s.src = paths[lib];\n",
              "        });\n",
              "    }\n",
              "\n",
              "    function showError(err) {\n",
              "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
              "      throw err;\n",
              "    }\n",
              "\n",
              "    function displayChart(vegaEmbed) {\n",
              "      vegaEmbed(outputDiv, spec, embedOpt)\n",
              "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
              "    }\n",
              "\n",
              "    if(typeof define === \"function\" && define.amd) {\n",
              "      requirejs.config({paths});\n",
              "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
              "    } else {\n",
              "      maybeLoadScript(\"vega\", \"5\")\n",
              "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
              "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
              "        .catch(showError)\n",
              "        .then(() => displayChart(vegaEmbed));\n",
              "    }\n",
              "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-eafc635ddfa311c913cbcc2d2bc4477d\"}, \"mark\": \"line\", \"encoding\": {\"color\": {\"field\": \"dimension\", \"type\": \"nominal\"}, \"x\": {\"field\": \"position\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"embedding\", \"type\": \"quantitative\"}}, \"selection\": {\"selector001\": {\"type\": \"interval\", \"bind\": \"scales\", \"encodings\": [\"x\", \"y\"]}}, \"width\": 800, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-eafc635ddfa311c913cbcc2d2bc4477d\": [{\"embedding\": 0.0, \"dimension\": 4, \"position\": 0}, {\"embedding\": 0.15782663226127625, \"dimension\": 4, \"position\": 1}, {\"embedding\": 0.3116971552371979, \"dimension\": 4, \"position\": 2}, {\"embedding\": 0.45775455236434937, \"dimension\": 4, \"position\": 3}, {\"embedding\": 0.5923377275466919, \"dimension\": 4, \"position\": 4}, {\"embedding\": 0.7120732069015503, \"dimension\": 4, \"position\": 5}, {\"embedding\": 0.813959538936615, \"dimension\": 4, \"position\": 6}, {\"embedding\": 0.8954429626464844, \"dimension\": 4, \"position\": 7}, {\"embedding\": 0.9544808864593506, \"dimension\": 4, \"position\": 8}, {\"embedding\": 0.989593505859375, \"dimension\": 4, \"position\": 9}, {\"embedding\": 0.9999006390571594, \"dimension\": 4, \"position\": 10}, {\"embedding\": 0.9851439595222473, \"dimension\": 4, \"position\": 11}, {\"embedding\": 0.94569331407547, \"dimension\": 4, \"position\": 12}, {\"embedding\": 0.8825376033782959, \"dimension\": 4, \"position\": 13}, {\"embedding\": 0.7972599267959595, \"dimension\": 4, \"position\": 14}, {\"embedding\": 0.6919978260993958, \"dimension\": 4, \"position\": 15}, {\"embedding\": 0.5693899393081665, \"dimension\": 4, \"position\": 16}, {\"embedding\": 0.4325096309185028, \"dimension\": 4, \"position\": 17}, {\"embedding\": 0.284787654876709, \"dimension\": 4, \"position\": 18}, {\"embedding\": 0.12992730736732483, \"dimension\": 4, \"position\": 19}, {\"embedding\": -0.028190065175294876, \"dimension\": 4, \"position\": 20}, {\"embedding\": -0.18560057878494263, \"dimension\": 4, \"position\": 21}, {\"embedding\": -0.3383587896823883, \"dimension\": 4, \"position\": 22}, {\"embedding\": -0.4826357960700989, \"dimension\": 4, \"position\": 23}, {\"embedding\": -0.6148146390914917, \"dimension\": 4, \"position\": 24}, {\"embedding\": -0.7315824031829834, \"dimension\": 4, \"position\": 25}, {\"embedding\": -0.8300122618675232, \"dimension\": 4, \"position\": 26}, {\"embedding\": -0.9076365828514099, \"dimension\": 4, \"position\": 27}, {\"embedding\": -0.96250981092453, \"dimension\": 4, \"position\": 28}, {\"embedding\": -0.9932565093040466, \"dimension\": 4, \"position\": 29}, {\"embedding\": -0.9991058707237244, \"dimension\": 4, \"position\": 30}, {\"embedding\": -0.9799113273620605, \"dimension\": 4, \"position\": 31}, {\"embedding\": -0.9361540079116821, \"dimension\": 4, \"position\": 32}, {\"embedding\": -0.8689308166503906, \"dimension\": 4, \"position\": 33}, {\"embedding\": -0.7799267172813416, \"dimension\": 4, \"position\": 34}, {\"embedding\": -0.6713724136352539, \"dimension\": 4, \"position\": 35}, {\"embedding\": -0.5459895133972168, \"dimension\": 4, \"position\": 36}, {\"embedding\": -0.40692076086997986, \"dimension\": 4, \"position\": 37}, {\"embedding\": -0.2576519548892975, \"dimension\": 4, \"position\": 38}, {\"embedding\": -0.10192479938268661, \"dimension\": 4, \"position\": 39}, {\"embedding\": 0.056357722729444504, \"dimension\": 4, \"position\": 40}, {\"embedding\": 0.21322709321975708, \"dimension\": 4, \"position\": 41}, {\"embedding\": 0.3647516369819641, \"dimension\": 4, \"position\": 42}, {\"embedding\": 0.5071332454681396, \"dimension\": 4, \"position\": 43}, {\"embedding\": 0.6368028521537781, \"dimension\": 4, \"position\": 44}, {\"embedding\": 0.7505101561546326, \"dimension\": 4, \"position\": 45}, {\"embedding\": 0.8454052805900574, \"dimension\": 4, \"position\": 46}, {\"embedding\": 0.9191088080406189, \"dimension\": 4, \"position\": 47}, {\"embedding\": 0.9697737693786621, \"dimension\": 4, \"position\": 48}, {\"embedding\": 0.9961300492286682, \"dimension\": 4, \"position\": 49}, {\"embedding\": 0.9975170493125916, \"dimension\": 4, \"position\": 50}, {\"embedding\": 0.9738998413085938, \"dimension\": 4, \"position\": 51}, {\"embedding\": 0.9258706569671631, \"dimension\": 4, \"position\": 52}, {\"embedding\": 0.8546332716941833, \"dimension\": 4, \"position\": 53}, {\"embedding\": 0.7619734406471252, \"dimension\": 4, \"position\": 54}, {\"embedding\": 0.6502137184143066, \"dimension\": 4, \"position\": 55}, {\"embedding\": 0.5221555233001709, \"dimension\": 4, \"position\": 56}, {\"embedding\": 0.38100889325141907, \"dimension\": 4, \"position\": 57}, {\"embedding\": 0.23031172156333923, \"dimension\": 4, \"position\": 58}, {\"embedding\": 0.07384055852890015, \"dimension\": 4, \"position\": 59}, {\"embedding\": -0.08448058366775513, \"dimension\": 4, \"position\": 60}, {\"embedding\": -0.2406841218471527, \"dimension\": 4, \"position\": 61}, {\"embedding\": -0.3908545970916748, \"dimension\": 4, \"position\": 62}, {\"embedding\": -0.5312277674674988, \"dimension\": 4, \"position\": 63}, {\"embedding\": -0.6582850813865662, \"dimension\": 4, \"position\": 64}, {\"embedding\": -0.768841564655304, \"dimension\": 4, \"position\": 65}, {\"embedding\": -0.8601260781288147, \"dimension\": 4, \"position\": 66}, {\"embedding\": -0.9298503994941711, \"dimension\": 4, \"position\": 67}, {\"embedding\": -0.9762668013572693, \"dimension\": 4, \"position\": 68}, {\"embedding\": -0.9982118010520935, \"dimension\": 4, \"position\": 69}, {\"embedding\": -0.9951352477073669, \"dimension\": 4, \"position\": 70}, {\"embedding\": -0.967114269733429, \"dimension\": 4, \"position\": 71}, {\"embedding\": -0.9148513078689575, \"dimension\": 4, \"position\": 72}, {\"embedding\": -0.839656412601471, \"dimension\": 4, \"position\": 73}, {\"embedding\": -0.7434144616127014, \"dimension\": 4, \"position\": 74}, {\"embedding\": -0.6285378336906433, \"dimension\": 4, \"position\": 75}, {\"embedding\": -0.4979061186313629, \"dimension\": 4, \"position\": 76}, {\"embedding\": -0.3547937273979187, \"dimension\": 4, \"position\": 77}, {\"embedding\": -0.20278796553611755, \"dimension\": 4, \"position\": 78}, {\"embedding\": -0.04569905996322632, \"dimension\": 4, \"position\": 79}, {\"embedding\": 0.11253630369901657, \"dimension\": 4, \"position\": 80}, {\"embedding\": 0.2679498493671417, \"dimension\": 4, \"position\": 81}, {\"embedding\": 0.4166468679904938, \"dimension\": 4, \"position\": 82}, {\"embedding\": 0.5549001097679138, \"dimension\": 4, \"position\": 83}, {\"embedding\": 0.6792440414428711, \"dimension\": 4, \"position\": 84}, {\"embedding\": 0.7865618467330933, \"dimension\": 4, \"position\": 85}, {\"embedding\": 0.8741634488105774, \"dimension\": 4, \"position\": 86}, {\"embedding\": 0.9398530125617981, \"dimension\": 4, \"position\": 87}, {\"embedding\": 0.9819839596748352, \"dimension\": 4, \"position\": 88}, {\"embedding\": 0.9995002150535583, \"dimension\": 4, \"position\": 89}, {\"embedding\": 0.9919626712799072, \"dimension\": 4, \"position\": 90}, {\"embedding\": 0.959559977054596, \"dimension\": 4, \"position\": 91}, {\"embedding\": 0.903104841709137, \"dimension\": 4, \"position\": 92}, {\"embedding\": 0.8240122199058533, \"dimension\": 4, \"position\": 93}, {\"embedding\": 0.7242646217346191, \"dimension\": 4, \"position\": 94}, {\"embedding\": 0.6063624024391174, \"dimension\": 4, \"position\": 95}, {\"embedding\": 0.4732609689235687, \"dimension\": 4, \"position\": 96}, {\"embedding\": 0.32829657196998596, \"dimension\": 4, \"position\": 97}, {\"embedding\": 0.17510302364826202, \"dimension\": 4, \"position\": 98}, {\"embedding\": 0.01752028614282608, \"dimension\": 4, \"position\": 99}, {\"embedding\": 1.0, \"dimension\": 5, \"position\": 0}, {\"embedding\": 0.9874668121337891, \"dimension\": 5, \"position\": 1}, {\"embedding\": 0.9501814842224121, \"dimension\": 5, \"position\": 2}, {\"embedding\": 0.8890786170959473, \"dimension\": 5, \"position\": 3}, {\"embedding\": 0.8056897521018982, \"dimension\": 5, \"position\": 4}, {\"embedding\": 0.7021052241325378, \"dimension\": 5, \"position\": 5}, {\"embedding\": 0.5809215903282166, \"dimension\": 5, \"position\": 6}, {\"embedding\": 0.44517630338668823, \"dimension\": 5, \"position\": 7}, {\"embedding\": 0.2982720732688904, \"dimension\": 5, \"position\": 8}, {\"embedding\": 0.14389123022556305, \"dimension\": 5, \"position\": 9}, {\"embedding\": -0.01409643329679966, \"dimension\": 5, \"position\": 10}, {\"embedding\": -0.1717306226491928, \"dimension\": 5, \"position\": 11}, {\"embedding\": -0.32506027817726135, \"dimension\": 5, \"position\": 12}, {\"embedding\": -0.47024187445640564, \"dimension\": 5, \"position\": 13}, {\"embedding\": -0.6036361455917358, \"dimension\": 5, \"position\": 14}, {\"embedding\": -0.7218996286392212, \"dimension\": 5, \"position\": 15}, {\"embedding\": -0.8220675587654114, \"dimension\": 5, \"position\": 16}, {\"embedding\": -0.9016293287277222, \"dimension\": 5, \"position\": 17}, {\"embedding\": -0.9585906267166138, \"dimension\": 5, \"position\": 18}, {\"embedding\": -0.9915235042572021, \"dimension\": 5, \"position\": 19}, {\"embedding\": -0.9996025562286377, \"dimension\": 5, \"position\": 20}, {\"embedding\": -0.9826252460479736, \"dimension\": 5, \"position\": 21}, {\"embedding\": -0.941017210483551, \"dimension\": 5, \"position\": 22}, {\"embedding\": -0.8758211731910706, \"dimension\": 5, \"position\": 23}, {\"embedding\": -0.788671612739563, \"dimension\": 5, \"position\": 24}, {\"embedding\": -0.6817529797554016, \"dimension\": 5, \"position\": 25}, {\"embedding\": -0.5577451586723328, \"dimension\": 5, \"position\": 26}, {\"embedding\": -0.4197568893432617, \"dimension\": 5, \"position\": 27}, {\"embedding\": -0.27124688029289246, \"dimension\": 5, \"position\": 28}, {\"embedding\": -0.11593768745660782, \"dimension\": 5, \"position\": 29}, {\"embedding\": 0.042278096079826355, \"dimension\": 5, \"position\": 30}, {\"embedding\": 0.19943365454673767, \"dimension\": 5, \"position\": 31}, {\"embedding\": 0.3515901565551758, \"dimension\": 5, \"position\": 32}, {\"embedding\": 0.4949335753917694, \"dimension\": 5, \"position\": 33}, {\"embedding\": 0.6258708238601685, \"dimension\": 5, \"position\": 34}, {\"embedding\": 0.7411201596260071, \"dimension\": 5, \"position\": 35}, {\"embedding\": 0.8377919793128967, \"dimension\": 5, \"position\": 36}, {\"embedding\": 0.9134634733200073, \"dimension\": 5, \"position\": 37}, {\"embedding\": 0.9662377834320068, \"dimension\": 5, \"position\": 38}, {\"embedding\": 0.994792103767395, \"dimension\": 5, \"position\": 39}, {\"embedding\": 0.9984106421470642, \"dimension\": 5, \"position\": 40}, {\"embedding\": 0.9770026803016663, \"dimension\": 5, \"position\": 41}, {\"embedding\": 0.931104838848114, \"dimension\": 5, \"position\": 42}, {\"embedding\": 0.8618676662445068, \"dimension\": 5, \"position\": 43}, {\"embedding\": 0.7710266709327698, \"dimension\": 5, \"position\": 44}, {\"embedding\": 0.6608588695526123, \"dimension\": 5, \"position\": 45}, {\"embedding\": 0.5341253876686096, \"dimension\": 5, \"position\": 46}, {\"embedding\": 0.3940037488937378, \"dimension\": 5, \"position\": 47}, {\"embedding\": 0.24400585889816284, \"dimension\": 5, \"position\": 48}, {\"embedding\": 0.08789165318012238, \"dimension\": 5, \"position\": 49}, {\"embedding\": -0.07042567431926727, \"dimension\": 5, \"position\": 50}, {\"embedding\": -0.2269781529903412, \"dimension\": 5, \"position\": 51}, {\"embedding\": -0.37784066796302795, \"dimension\": 5, \"position\": 52}, {\"embedding\": -0.5192320942878723, \"dimension\": 5, \"position\": 53}, {\"embedding\": -0.6476082801818848, \"dimension\": 5, \"position\": 54}, {\"embedding\": -0.7597513794898987, \"dimension\": 5, \"position\": 55}, {\"embedding\": -0.8528502583503723, \"dimension\": 5, \"position\": 56}, {\"embedding\": -0.9245713949203491, \"dimension\": 5, \"position\": 57}, {\"embedding\": -0.973116934299469, \"dimension\": 5, \"position\": 58}, {\"embedding\": -0.9972700476646423, \"dimension\": 5, \"position\": 59}, {\"embedding\": -0.9964250922203064, \"dimension\": 5, \"position\": 60}, {\"embedding\": -0.9706035256385803, \"dimension\": 5, \"position\": 61}, {\"embedding\": -0.9204524159431458, \"dimension\": 5, \"position\": 62}, {\"embedding\": -0.8472290635108948, \"dimension\": 5, \"position\": 63}, {\"embedding\": -0.7527687549591064, \"dimension\": 5, \"position\": 64}, {\"embedding\": -0.6394393444061279, \"dimension\": 5, \"position\": 65}, {\"embedding\": -0.5100815296173096, \"dimension\": 5, \"position\": 66}, {\"embedding\": -0.3679378628730774, \"dimension\": 5, \"position\": 67}, {\"embedding\": -0.2165713608264923, \"dimension\": 5, \"position\": 68}, {\"embedding\": -0.05977622792124748, \"dimension\": 5, \"position\": 69}, {\"embedding\": 0.09851823002099991, \"dimension\": 5, \"position\": 70}, {\"embedding\": 0.254342257976532, \"dimension\": 5, \"position\": 71}, {\"embedding\": 0.4037908613681793, \"dimension\": 5, \"position\": 72}, {\"embedding\": 0.543117880821228, \"dimension\": 5, \"position\": 73}, {\"embedding\": 0.6688309907913208, \"dimension\": 5, \"position\": 74}, {\"embedding\": 0.7777789831161499, \"dimension\": 5, \"position\": 75}, {\"embedding\": 0.8672309517860413, \"dimension\": 5, \"position\": 76}, {\"embedding\": 0.9349446296691895, \"dimension\": 5, \"position\": 77}, {\"embedding\": 0.9792226552963257, \"dimension\": 5, \"position\": 78}, {\"embedding\": 0.998955249786377, \"dimension\": 5, \"position\": 79}, {\"embedding\": 0.9936476349830627, \"dimension\": 5, \"position\": 80}, {\"embedding\": 0.9634328484535217, \"dimension\": 5, \"position\": 81}, {\"embedding\": 0.9090684056282043, \"dimension\": 5, \"position\": 82}, {\"embedding\": 0.8319169878959656, \"dimension\": 5, \"position\": 83}, {\"embedding\": 0.733912467956543, \"dimension\": 5, \"position\": 84}, {\"embedding\": 0.617511510848999, \"dimension\": 5, \"position\": 85}, {\"embedding\": 0.4856317937374115, \"dimension\": 5, \"position\": 86}, {\"embedding\": 0.3415791094303131, \"dimension\": 5, \"position\": 87}, {\"embedding\": 0.18896427750587463, \"dimension\": 5, \"position\": 88}, {\"embedding\": 0.0316128134727478, \"dimension\": 5, \"position\": 89}, {\"embedding\": -0.12653106451034546, \"dimension\": 5, \"position\": 90}, {\"embedding\": -0.28150418400764465, \"dimension\": 5, \"position\": 91}, {\"embedding\": -0.4294200837612152, \"dimension\": 5, \"position\": 92}, {\"embedding\": -0.5665720105171204, \"dimension\": 5, \"position\": 93}, {\"embedding\": -0.6895220875740051, \"dimension\": 5, \"position\": 94}, {\"embedding\": -0.7951884269714355, \"dimension\": 5, \"position\": 95}, {\"embedding\": -0.880922257900238, \"dimension\": 5, \"position\": 96}, {\"embedding\": -0.9445747137069702, \"dimension\": 5, \"position\": 97}, {\"embedding\": -0.9845501184463501, \"dimension\": 5, \"position\": 98}, {\"embedding\": -0.9998465180397034, \"dimension\": 5, \"position\": 99}, {\"embedding\": 0.0, \"dimension\": 6, \"position\": 0}, {\"embedding\": 0.06305388361215591, \"dimension\": 6, \"position\": 1}, {\"embedding\": 0.12585683166980743, \"dimension\": 6, \"position\": 2}, {\"embedding\": 0.18815888464450836, \"dimension\": 6, \"position\": 3}, {\"embedding\": 0.24971213936805725, \"dimension\": 6, \"position\": 4}, {\"embedding\": 0.31027159094810486, \"dimension\": 6, \"position\": 5}, {\"embedding\": 0.3695962131023407, \"dimension\": 6, \"position\": 6}, {\"embedding\": 0.4274499714374542, \"dimension\": 6, \"position\": 7}, {\"embedding\": 0.4836025834083557, \"dimension\": 6, \"position\": 8}, {\"embedding\": 0.5378305912017822, \"dimension\": 6, \"position\": 9}, {\"embedding\": 0.5899181365966797, \"dimension\": 6, \"position\": 10}, {\"embedding\": 0.6396579146385193, \"dimension\": 6, \"position\": 11}, {\"embedding\": 0.6868520379066467, \"dimension\": 6, \"position\": 12}, {\"embedding\": 0.7313126921653748, \"dimension\": 6, \"position\": 13}, {\"embedding\": 0.7728629112243652, \"dimension\": 6, \"position\": 14}, {\"embedding\": 0.8113372921943665, \"dimension\": 6, \"position\": 15}, {\"embedding\": 0.8465827703475952, \"dimension\": 6, \"position\": 16}, {\"embedding\": 0.8784590363502502, \"dimension\": 6, \"position\": 17}, {\"embedding\": 0.9068393111228943, \"dimension\": 6, \"position\": 18}, {\"embedding\": 0.9316105246543884, \"dimension\": 6, \"position\": 19}, {\"embedding\": 0.9526742100715637, \"dimension\": 6, \"position\": 20}, {\"embedding\": 0.9699464440345764, \"dimension\": 6, \"position\": 21}, {\"embedding\": 0.9833585619926453, \"dimension\": 6, \"position\": 22}, {\"embedding\": 0.9928570985794067, \"dimension\": 6, \"position\": 23}, {\"embedding\": 0.9984043836593628, \"dimension\": 6, \"position\": 24}, {\"embedding\": 0.999978244304657, \"dimension\": 6, \"position\": 25}, {\"embedding\": 0.9975724220275879, \"dimension\": 6, \"position\": 26}, {\"embedding\": 0.9911965131759644, \"dimension\": 6, \"position\": 27}, {\"embedding\": 0.9808759093284607, \"dimension\": 6, \"position\": 28}, {\"embedding\": 0.9666516780853271, \"dimension\": 6, \"position\": 29}, {\"embedding\": 0.9485803842544556, \"dimension\": 6, \"position\": 30}, {\"embedding\": 0.9267339110374451, \"dimension\": 6, \"position\": 31}, {\"embedding\": 0.9011994004249573, \"dimension\": 6, \"position\": 32}, {\"embedding\": 0.8720782399177551, \"dimension\": 6, \"position\": 33}, {\"embedding\": 0.8394865393638611, \"dimension\": 6, \"position\": 34}, {\"embedding\": 0.8035537600517273, \"dimension\": 6, \"position\": 35}, {\"embedding\": 0.7644230127334595, \"dimension\": 6, \"position\": 36}, {\"embedding\": 0.7222501039505005, \"dimension\": 6, \"position\": 37}, {\"embedding\": 0.6772029399871826, \"dimension\": 6, \"position\": 38}, {\"embedding\": 0.6294605135917664, \"dimension\": 6, \"position\": 39}, {\"embedding\": 0.57921302318573, \"dimension\": 6, \"position\": 40}, {\"embedding\": 0.5266605615615845, \"dimension\": 6, \"position\": 41}, {\"embedding\": 0.4720119535923004, \"dimension\": 6, \"position\": 42}, {\"embedding\": 0.41548484563827515, \"dimension\": 6, \"position\": 43}, {\"embedding\": 0.3573042154312134, \"dimension\": 6, \"position\": 44}, {\"embedding\": 0.29770180583000183, \"dimension\": 6, \"position\": 45}, {\"embedding\": 0.23691439628601074, \"dimension\": 6, \"position\": 46}, {\"embedding\": 0.17518411576747894, \"dimension\": 6, \"position\": 47}, {\"embedding\": 0.1127568930387497, \"dimension\": 6, \"position\": 48}, {\"embedding\": 0.04988069087266922, \"dimension\": 6, \"position\": 49}, {\"embedding\": -0.013194027356803417, \"dimension\": 6, \"position\": 50}, {\"embedding\": -0.07621623575687408, \"dimension\": 6, \"position\": 51}, {\"embedding\": -0.1389348804950714, \"dimension\": 6, \"position\": 52}, {\"embedding\": -0.20110084116458893, \"dimension\": 6, \"position\": 53}, {\"embedding\": -0.2624664604663849, \"dimension\": 6, \"position\": 54}, {\"embedding\": -0.3227875530719757, \"dimension\": 6, \"position\": 55}, {\"embedding\": -0.3818237781524658, \"dimension\": 6, \"position\": 56}, {\"embedding\": -0.4393406808376312, \"dimension\": 6, \"position\": 57}, {\"embedding\": -0.49510911107063293, \"dimension\": 6, \"position\": 58}, {\"embedding\": -0.5489069223403931, \"dimension\": 6, \"position\": 59}, {\"embedding\": -0.6005204319953918, \"dimension\": 6, \"position\": 60}, {\"embedding\": -0.6497439742088318, \"dimension\": 6, \"position\": 61}, {\"embedding\": -0.6963817477226257, \"dimension\": 6, \"position\": 62}, {\"embedding\": -0.7402478456497192, \"dimension\": 6, \"position\": 63}, {\"embedding\": -0.7811681628227234, \"dimension\": 6, \"position\": 64}, {\"embedding\": -0.8189795017242432, \"dimension\": 6, \"position\": 65}, {\"embedding\": -0.8535317182540894, \"dimension\": 6, \"position\": 66}, {\"embedding\": -0.8846868872642517, \"dimension\": 6, \"position\": 67}, {\"embedding\": -0.9123212099075317, \"dimension\": 6, \"position\": 68}, {\"embedding\": -0.936324954032898, \"dimension\": 6, \"position\": 69}, {\"embedding\": -0.956602156162262, \"dimension\": 6, \"position\": 70}, {\"embedding\": -0.9730724096298218, \"dimension\": 6, \"position\": 71}, {\"embedding\": -0.9856699705123901, \"dimension\": 6, \"position\": 72}, {\"embedding\": -0.9943448305130005, \"dimension\": 6, \"position\": 73}, {\"embedding\": -0.9990625381469727, \"dimension\": 6, \"position\": 74}, {\"embedding\": -0.9998041391372681, \"dimension\": 6, \"position\": 75}, {\"embedding\": -0.9965668320655823, \"dimension\": 6, \"position\": 76}, {\"embedding\": -0.9893633723258972, \"dimension\": 6, \"position\": 77}, {\"embedding\": -0.9782225489616394, \"dimension\": 6, \"position\": 78}, {\"embedding\": -0.963188648223877, \"dimension\": 6, \"position\": 79}, {\"embedding\": -0.9443213939666748, \"dimension\": 6, \"position\": 80}, {\"embedding\": -0.9216960668563843, \"dimension\": 6, \"position\": 81}, {\"embedding\": -0.8954026699066162, \"dimension\": 6, \"position\": 82}, {\"embedding\": -0.8655455708503723, \"dimension\": 6, \"position\": 83}, {\"embedding\": -0.8322440981864929, \"dimension\": 6, \"position\": 84}, {\"embedding\": -0.795630156993866, \"dimension\": 6, \"position\": 85}, {\"embedding\": -0.7558501362800598, \"dimension\": 6, \"position\": 86}, {\"embedding\": -0.7130619883537292, \"dimension\": 6, \"position\": 87}, {\"embedding\": -0.6674357056617737, \"dimension\": 6, \"position\": 88}, {\"embedding\": -0.6191535592079163, \"dimension\": 6, \"position\": 89}, {\"embedding\": -0.5684073567390442, \"dimension\": 6, \"position\": 90}, {\"embedding\": -0.5153986215591431, \"dimension\": 6, \"position\": 91}, {\"embedding\": -0.46033912897109985, \"dimension\": 6, \"position\": 92}, {\"embedding\": -0.40344759821891785, \"dimension\": 6, \"position\": 93}, {\"embedding\": -0.3449500501155853, \"dimension\": 6, \"position\": 94}, {\"embedding\": -0.28508007526397705, \"dimension\": 6, \"position\": 95}, {\"embedding\": -0.22407560050487518, \"dimension\": 6, \"position\": 96}, {\"embedding\": -0.1621788740158081, \"dimension\": 6, \"position\": 97}, {\"embedding\": -0.09963719546794891, \"dimension\": 6, \"position\": 98}, {\"embedding\": -0.03669850528240204, \"dimension\": 6, \"position\": 99}, {\"embedding\": 1.0, \"dimension\": 7, \"position\": 0}, {\"embedding\": 0.9980100989341736, \"dimension\": 7, \"position\": 1}, {\"embedding\": 0.9920483827590942, \"dimension\": 7, \"position\": 2}, {\"embedding\": 0.9821385741233826, \"dimension\": 7, \"position\": 3}, {\"embedding\": 0.9683201313018799, \"dimension\": 7, \"position\": 4}, {\"embedding\": 0.9506479501724243, \"dimension\": 7, \"position\": 5}, {\"embedding\": 0.9291924834251404, \"dimension\": 7, \"position\": 6}, {\"embedding\": 0.9040390253067017, \"dimension\": 7, \"position\": 7}, {\"embedding\": 0.8752877116203308, \"dimension\": 7, \"position\": 8}, {\"embedding\": 0.8430529236793518, \"dimension\": 7, \"position\": 9}, {\"embedding\": 0.8074630498886108, \"dimension\": 7, \"position\": 10}, {\"embedding\": 0.7686597108840942, \"dimension\": 7, \"position\": 11}, {\"embedding\": 0.7267972826957703, \"dimension\": 7, \"position\": 12}, {\"embedding\": 0.6820423603057861, \"dimension\": 7, \"position\": 13}, {\"embedding\": 0.6345730423927307, \"dimension\": 7, \"position\": 14}, {\"embedding\": 0.5845783352851868, \"dimension\": 7, \"position\": 15}, {\"embedding\": 0.532257080078125, \"dimension\": 7, \"position\": 16}, {\"embedding\": 0.47781768441200256, \"dimension\": 7, \"position\": 17}, {\"embedding\": 0.4214765727519989, \"dimension\": 7, \"position\": 18}, {\"embedding\": 0.36345821619033813, \"dimension\": 7, \"position\": 19}, {\"embedding\": 0.30399325489997864, \"dimension\": 7, \"position\": 20}, {\"embedding\": 0.243318572640419, \"dimension\": 7, \"position\": 21}, {\"embedding\": 0.18167544901371002, \"dimension\": 7, \"position\": 22}, {\"embedding\": 0.11930941045284271, \"dimension\": 7, \"position\": 23}, {\"embedding\": 0.056468550115823746, \"dimension\": 7, \"position\": 24}, {\"embedding\": -0.006597157102078199, \"dimension\": 7, \"position\": 25}, {\"embedding\": -0.06963648647069931, \"dimension\": 7, \"position\": 26}, {\"embedding\": -0.1323987990617752, \"dimension\": 7, \"position\": 27}, {\"embedding\": -0.1946340948343277, \"dimension\": 7, \"position\": 28}, {\"embedding\": -0.25609490275382996, \"dimension\": 7, \"position\": 29}, {\"embedding\": -0.31653639674186707, \"dimension\": 7, \"position\": 30}, {\"embedding\": -0.37571826577186584, \"dimension\": 7, \"position\": 31}, {\"embedding\": -0.43340474367141724, \"dimension\": 7, \"position\": 32}, {\"embedding\": -0.4893665015697479, \"dimension\": 7, \"position\": 33}, {\"embedding\": -0.5433804988861084, \"dimension\": 7, \"position\": 34}, {\"embedding\": -0.5952321887016296, \"dimension\": 7, \"position\": 35}, {\"embedding\": -0.6447150111198425, \"dimension\": 7, \"position\": 36}, {\"embedding\": -0.6916319727897644, \"dimension\": 7, \"position\": 37}, {\"embedding\": -0.7357962727546692, \"dimension\": 7, \"position\": 38}, {\"embedding\": -0.7770324349403381, \"dimension\": 7, \"position\": 39}, {\"embedding\": -0.8151761889457703, \"dimension\": 7, \"position\": 40}, {\"embedding\": -0.8500756621360779, \"dimension\": 7, \"position\": 41}, {\"embedding\": -0.8815921545028687, \"dimension\": 7, \"position\": 42}, {\"embedding\": -0.9096000790596008, \"dimension\": 7, \"position\": 43}, {\"embedding\": -0.933988094329834, \"dimension\": 7, \"position\": 44}, {\"embedding\": -0.9546589255332947, \"dimension\": 7, \"position\": 45}, {\"embedding\": -0.971530556678772, \"dimension\": 7, \"position\": 46}, {\"embedding\": -0.9845356941223145, \"dimension\": 7, \"position\": 47}, {\"embedding\": -0.9936226010322571, \"dimension\": 7, \"position\": 48}, {\"embedding\": -0.998755156993866, \"dimension\": 7, \"position\": 49}, {\"embedding\": -0.9999129772186279, \"dimension\": 7, \"position\": 50}, {\"embedding\": -0.9970912933349609, \"dimension\": 7, \"position\": 51}, {\"embedding\": -0.9903014898300171, \"dimension\": 7, \"position\": 52}, {\"embedding\": -0.9795705676078796, \"dimension\": 7, \"position\": 53}, {\"embedding\": -0.964941143989563, \"dimension\": 7, \"position\": 54}, {\"embedding\": -0.9464714527130127, \"dimension\": 7, \"position\": 55}, {\"embedding\": -0.9242351651191711, \"dimension\": 7, \"position\": 56}, {\"embedding\": -0.8983205556869507, \"dimension\": 7, \"position\": 57}, {\"embedding\": -0.8688308000564575, \"dimension\": 7, \"position\": 58}, {\"embedding\": -0.8358834981918335, \"dimension\": 7, \"position\": 59}, {\"embedding\": -0.7996094226837158, \"dimension\": 7, \"position\": 60}, {\"embedding\": -0.7601531147956848, \"dimension\": 7, \"position\": 61}, {\"embedding\": -0.7176715731620789, \"dimension\": 7, \"position\": 62}, {\"embedding\": -0.6723340749740601, \"dimension\": 7, \"position\": 63}, {\"embedding\": -0.6243206262588501, \"dimension\": 7, \"position\": 64}, {\"embedding\": -0.5738227963447571, \"dimension\": 7, \"position\": 65}, {\"embedding\": -0.5210408568382263, \"dimension\": 7, \"position\": 66}, {\"embedding\": -0.46618568897247314, \"dimension\": 7, \"position\": 67}, {\"embedding\": -0.4094752371311188, \"dimension\": 7, \"position\": 68}, {\"embedding\": -0.3511347472667694, \"dimension\": 7, \"position\": 69}, {\"embedding\": -0.2913972735404968, \"dimension\": 7, \"position\": 70}, {\"embedding\": -0.23049965500831604, \"dimension\": 7, \"position\": 71}, {\"embedding\": -0.1686851680278778, \"dimension\": 7, \"position\": 72}, {\"embedding\": -0.10619935393333435, \"dimension\": 7, \"position\": 73}, {\"embedding\": -0.04329042136669159, \"dimension\": 7, \"position\": 74}, {\"embedding\": 0.019790323451161385, \"dimension\": 7, \"position\": 75}, {\"embedding\": 0.08279230445623398, \"dimension\": 7, \"position\": 76}, {\"embedding\": 0.14546526968479156, \"dimension\": 7, \"position\": 77}, {\"embedding\": 0.20755885541439056, \"dimension\": 7, \"position\": 78}, {\"embedding\": 0.2688263952732086, \"dimension\": 7, \"position\": 79}, {\"embedding\": 0.3290245532989502, \"dimension\": 7, \"position\": 80}, {\"embedding\": 0.3879128098487854, \"dimension\": 7, \"position\": 81}, {\"embedding\": 0.4452572762966156, \"dimension\": 7, \"position\": 82}, {\"embedding\": 0.5008301138877869, \"dimension\": 7, \"position\": 83}, {\"embedding\": 0.5544094443321228, \"dimension\": 7, \"position\": 84}, {\"embedding\": 0.605782687664032, \"dimension\": 7, \"position\": 85}, {\"embedding\": 0.6547446846961975, \"dimension\": 7, \"position\": 86}, {\"embedding\": 0.7011010050773621, \"dimension\": 7, \"position\": 87}, {\"embedding\": 0.7446674108505249, \"dimension\": 7, \"position\": 88}, {\"embedding\": 0.7852699160575867, \"dimension\": 7, \"position\": 89}, {\"embedding\": 0.8227472901344299, \"dimension\": 7, \"position\": 90}, {\"embedding\": 0.856950581073761, \"dimension\": 7, \"position\": 91}, {\"embedding\": 0.8877431154251099, \"dimension\": 7, \"position\": 92}, {\"embedding\": 0.9150027632713318, \"dimension\": 7, \"position\": 93}, {\"embedding\": 0.9386210441589355, \"dimension\": 7, \"position\": 94}, {\"embedding\": 0.9585037231445312, \"dimension\": 7, \"position\": 95}, {\"embedding\": 0.9745717644691467, \"dimension\": 7, \"position\": 96}, {\"embedding\": 0.9867613911628723, \"dimension\": 7, \"position\": 97}, {\"embedding\": 0.9950238466262817, \"dimension\": 7, \"position\": 98}, {\"embedding\": 0.9993264079093933, \"dimension\": 7, \"position\": 99}]}}, {\"mode\": \"vega-lite\"});\n",
              "</script>"
            ],
            "text/plain": [
              "alt.Chart(...)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "show_example(example_positional)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-8g-HtBlsRZ"
      },
      "source": [
        "## The Transformer model\n",
        "\n",
        "To be convinient, we will use `nn.Transformer` layer from PyTorch. We will build a 4-layer Transformer model.\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "  <th colspan=1>The original Transformer diagram</th>\n",
        "  <th colspan=1>A representation of a 4-layer Transformer</th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img width=400 src=\"https://www.tensorflow.org/images/tutorials/transformer/transformer.png\"/>\n",
        "  </td>\n",
        "  <td>\n",
        "   <img width=307 src=\"https://www.tensorflow.org/images/tutorials/transformer/Transformer-4layer-compact.png\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMOQpxvqODYB"
      },
      "source": [
        "## Q8-9 Transformer NMT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRncXVMqN6GS"
      },
      "source": [
        "### Q8: (5 points) Initialize layers in TransformerNMT model\n",
        "Implement the `__init__` function  to initialize the\n",
        "necessary module for our TransformerNMT model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCJQc17N8pj"
      },
      "source": [
        "### Q9: (10 points) Implement the forward function\n",
        "Complete the `forward` function in the TransformerNMT class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4FpByedD5pS"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BOS_IDX, PAD_IDX, EOS_IDX, UNK_IDX = vocab.tgt[\"<s>\"], vocab.tgt[\"<pad>\"], vocab.tgt[\"</s>\"], vocab.tgt[\"<unk>\"]\n",
        "\n",
        "class TransformerNMT(nn.Module):\n",
        "    \"\"\" Neural Machine Translation Model with Transformer:\n",
        "        - Encoder with stacked self-attention and feedforward layers\n",
        "        - Decoder with stacked self-attention, encoder-decoder attention, and feedforward layers\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 dim_feedforward: int = 512,\n",
        "                #  max_len: int = 5000,\n",
        "                 max_len: int = 50000,\n",
        "                 dropout: float = 0.1):\n",
        "        \"\"\" Init TransformerNMT NMT Model.\n",
        "        @param num_encoder_layers (int): The number of sub-layers in the Encoder Transformer\n",
        "        @param num_decoder_layers (int): The number of sub-layers in the Decoder Transformer\n",
        "        @param emb_size (int): Hidden Size, the size of hidden states (dimensionality)\n",
        "        @param nhead (int): The number of heads in the multiheadattention\n",
        "        @param src_vocab_size (int): The vocab size of src languages\n",
        "        @param tgt_vocab_size (int): The vocab size of tgt languages\n",
        "        @param dim_feedforward (int): The dimension of the feedforward network model\n",
        "        @param max_len (int) max sequence length\n",
        "        @param dropout (float): Dropout probability, for attention\n",
        "        \"\"\"\n",
        "\n",
        "        super(TransformerNMT, self).__init__()\n",
        "\n",
        "        # self.src_embedding = None\n",
        "        # self.tgt_embedding = None\n",
        "\n",
        "        # self.transformer = None\n",
        "        # self.target_vocab_projection = None\n",
        "\n",
        "        ### YOUR CODE HERE\n",
        "        ### TODO - Initialize the following variables IN THIS ORDER:\n",
        "        ###     self.src_embedding: Transformer Embedding Layer used for source language\n",
        "        ###     self.tgt_embedding: Transformer Embedding Layer used for target language\n",
        "        ###     self.transformer: Transformer layer\n",
        "        ###     self.target_vocab_projection (Linear Layer with no bias), mapping hidden representation to the vocab distribution\n",
        "        ###\n",
        "        ### Use the following docs to properly initialize these variables:\n",
        "        ###     Transformer\n",
        "        ###         https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        self.src_embedding = TransformerEmbedding(src_vocab_size, emb_size, max_len, dropout) # vocab_size, embedding_size, max_len, dropout_rate\n",
        "        self.tgt_embedding = TransformerEmbedding(tgt_vocab_size, emb_size, max_len, dropout)\n",
        "        self.transformer = nn.Transformer(d_model=emb_size, nhead=nhead, num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward, dropout=dropout)\n",
        "        self.target_vocab_projection = nn.Linear(in_features=emb_size, out_features=tgt_vocab_size, bias=False)\n",
        "        ### END YOUR CODE\n",
        "\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        \"\"\"Forward pass through the neural machine translation (NMT) model.\n",
        "        @param src (Tensor): Source sequence tensor.\n",
        "        @param trg (Tensor): Target sequence tensor.\n",
        "        @param src_mask (Tensor): Mask for the source sequence.\n",
        "        @param tgt_mask (Tensor): Mask for the target sequence.\n",
        "        @param src_padding_mask (Tensor): Padding mask for the source sequence.\n",
        "        @param tgt_padding_mask (Tensor): Padding mask for the target sequence.\n",
        "        @param memory_key_padding_mask (Tensor): Padding mask for memory keys.\n",
        "\n",
        "        @returns Tensor: Output tensor representing the NMT model's predictions for the target sequence.\n",
        "        \"\"\"\n",
        "\n",
        "        ### YOUR CODE HERE\n",
        "        ### TODO - Implement the forward function:\n",
        "        ###     1. Compute `src_embeded` and `tgt_embeded` from `src` and `tgt` using TransformerEmbedding,\n",
        "        ###     which return shape (src_len, b, e)\n",
        "        ###     src_len = maximum source sentence length, b = batch size, e = embedding size.\n",
        "        ###     2. Apply the self.transformer to compute the decoder output with shape (tgt_len, b, e)\n",
        "        ###     tgt_len = maximum target sentence length, b = batch size, e = embedding size.\n",
        "        ###     3. Mapping the decoder ouput to the vocab distribution using self.target_vocab_projection and return it.\n",
        "        ###     which return shape (tgt_len, b, tgt_vocab_size)\n",
        "\n",
        "        ### Use the following docs for how to use nn.Transformer forward function:\n",
        "        ###     Transformer\n",
        "        ###         https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html\n",
        "        src_embeded = self.src_embedding(src) # shape(src_len, b, e)\n",
        "        tgt_embeded = self.tgt_embedding(trg) # shape(src_len, b, e)\n",
        "        dec_out = self.transformer(src_embeded, tgt_embeded) # shape(tgt_len, b, e)\n",
        "        return self.target_vocab_projection(dec_out) # shape(tgt_len, b, tgt_vocab_size)\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        \"\"\"Encodes the source sequence.\n",
        "        @param src (Tensor): Source sequence tensor. (src_len, bs)\n",
        "        @param src_mask (Tensor): Mask for the source sequence. (src_len, src_len)\n",
        "\n",
        "        @returns Tensor: Encoded source sequence tensor. (src_len, bs, emb_size)\n",
        "        \"\"\"\n",
        "        return self.transformer.encoder(self.src_embedding(src), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        \"\"\"Decodes the target sequence.\n",
        "        @param tgt (Tensor): Target sequence tensor. (tgt_len, bs)\n",
        "        @param memory (Tensor): Encoded source sequence tensor. (src_len, bs, emb_size)\n",
        "        @param tgt_mask (Tensor): Mask for the target sequence. (tgt_len, tgt_len)\n",
        "\n",
        "        @returns Tensor: Decoded target sequence tensor. (tgt_len, bs, emb_size)\n",
        "        \"\"\"\n",
        "        return self.transformer.decoder(self.tgt_embedding(tgt), memory, tgt_mask)\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91bFTjSttI87"
      },
      "source": [
        "## Train our full model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUlRiRFrUJvh",
        "outputId": "dc233712-e53b-4987-8c25-25009f294ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# @title Model Initialization\n",
        "\n",
        "import torch\n",
        "# torch.manual_seed(0)\n",
        "\n",
        "args.src_vocab_size = len(vocab.src)\n",
        "args.tgt_vocab_size = len(vocab.tgt)\n",
        "args.emb_size = 776\n",
        "args.n_heads = 8\n",
        "args.ffn_hid_dim = 768\n",
        "args.batch_size = 32\n",
        "args.n_encoder_layers = 3\n",
        "args.n_decoder_layers = 3\n",
        "args.lr = 1e-3\n",
        "args.dropout = 0.01\n",
        "\n",
        "transformer = TransformerNMT(args.n_encoder_layers, args.n_decoder_layers, args.emb_size,\n",
        "                                 args.n_heads, args.src_vocab_size, args.tgt_vocab_size, args.ffn_hid_dim, dropout=args.dropout)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=args.lr, betas=(0.9, 0.98), eps=1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2ipFnjSKFVM"
      },
      "outputs": [],
      "source": [
        "# @title Helper function\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# helper function to club together sequential operations\n",
        "def sequential_transforms(*transforms):\n",
        "    def func(txt_input):\n",
        "        for transform in transforms:\n",
        "            try:\n",
        "                txt_input = transform(txt_input)\n",
        "            except Exception as e:\n",
        "                if \"device\" in str(e):\n",
        "                    print(e)\n",
        "                    txt_input = transform(txt_input, device=device)\n",
        "                else:\n",
        "                    raise ValueError(e)\n",
        "        return txt_input\n",
        "    return func\n",
        "\n",
        "# function to add BOS/EOS and create tensor for input sequence indices\n",
        "def tensor_transform(token_ids: List[int]):\n",
        "    # print(token_ids)\n",
        "    return torch.cat((torch.tensor([BOS_IDX]).to(device),\n",
        "                      token_ids,\n",
        "                      torch.tensor([EOS_IDX]).to(device))).to(device)\n",
        "\n",
        "\n",
        "def convert_to_tensor_src(txt):\n",
        "    # print(torch.tensor([BOS_IDX]).shape, vocab.src.to_input_tensor(txt).shape, torch.tensor([EOS_IDX]).shape)\n",
        "    # print(torch.tensor([BOS_IDX]), vocab.src.to_input_tensor(txt).reshape(1,), torch.tensor([EOS_IDX]))\n",
        "    return torch.cat((torch.tensor([BOS_IDX]).view(1, 1).to(device),\n",
        "                      vocab.src.to_input_tensor(txt).to(device),\n",
        "                      torch.tensor([EOS_IDX]).view(1, 1).to(device)), dim=0).view(-1)\n",
        "\n",
        "def convert_to_tensor_tgt(txt):\n",
        "    return torch.cat((torch.tensor([BOS_IDX]).view(1, 1).to(device),\n",
        "                      vocab.tgt.to_input_tensor(txt).to(device),\n",
        "                      torch.tensor([EOS_IDX]).view(1, 1).to(device)), dim=0).view(-1)\n",
        "\n",
        "# ``src`` and ``tgt`` language text transforms to convert raw strings into tensors indices\n",
        "text_transform = {}\n",
        "\n",
        "SRC_LANGUAGE, TGT_LANGUAGE = \"vi\", \"en\"\n",
        "for ln in [SRC_LANGUAGE, TGT_LANGUAGE]:\n",
        "    if ln == SRC_LANGUAGE:\n",
        "        text_transform[ln] = convert_to_tensor_src # Add BOS/EOS and create tensor\n",
        "    if ln == TGT_LANGUAGE:\n",
        "        text_transform[ln] = convert_to_tensor_tgt# Add BOS/EOS and create tensor\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tensors\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        # print(src_sample, tgt_sample)\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy_gOxZwUp2g"
      },
      "outputs": [],
      "source": [
        "# @title Training the model\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_epoch(model, optimizer, epoch):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "    # train_iter = Multi30k(split='train', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    train_dataloader = DataLoader(train_data_small, batch_size=args.batch_size, collate_fn=collate_fn)\n",
        "    # train_dataloader = batch_iter(train_data, BATCH_SIZE)\n",
        "\n",
        "    for i, (src, tgt) in enumerate(train_dataloader):\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "        if i % args.log_every == 0:\n",
        "\n",
        "            print('epoch %d, iter %d, losses %.2f, avg. loss %.2f'\n",
        "                     % (epoch, i, losses, losses/(i+1)), file=sys.stderr)\n",
        "\n",
        "\n",
        "    return losses / len(list(train_dataloader))\n",
        "\n",
        "\n",
        "def evaluate(model):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    # val_iter = Multi30k(split='valid', language_pair=(SRC_LANGUAGE, TGT_LANGUAGE))\n",
        "    val_dataloader = DataLoader(dev_data_small, batch_size=args.batch_size, collate_fn=collate_fn)\n",
        "    # val_dataloader = batch_iter(dev_data, BATCH_SIZE)\n",
        "\n",
        "    for src, tgt in val_dataloader:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(list(val_dataloader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHbTs8aweGKZ"
      },
      "source": [
        "We also pick a small subset of 1600 sampels for training and 100 samples for validating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-iE_n0gvKpxv"
      },
      "outputs": [],
      "source": [
        "train_data_small = [val for val in train_data if len(val[0]) > 3][:50]\n",
        "dev_data_small = [val for val in dev_data if len(val[0]) > 3][:50]\n",
        "# train_data_small = [val for val in train_data if len(val[0]) > 3][:]\n",
        "# dev_data_small = [val for val in dev_data if len(val[0]) > 3][:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwLcM5IOKuuH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0531f57-9aff-4a3c-e1cf-62450edfd976"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 0, iter 0, losses 10.00, avg. loss 10.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Train loss: 9.608, Val loss: 8.397, Epoch time = 0.488s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 1, iter 0, losses 7.84, avg. loss 7.84\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Train loss: 7.431, Val loss: 7.364, Epoch time = 0.390s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 2, iter 0, losses 6.01, avg. loss 6.01\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Train loss: 5.846, Val loss: 7.182, Epoch time = 0.397s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 3, iter 0, losses 5.25, avg. loss 5.25\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Train loss: 5.137, Val loss: 7.454, Epoch time = 0.392s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 4, iter 0, losses 5.11, avg. loss 5.11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Train loss: 4.957, Val loss: 7.882, Epoch time = 0.397s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 5, iter 0, losses 5.22, avg. loss 5.22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Train loss: 4.989, Val loss: 8.248, Epoch time = 0.392s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 6, iter 0, losses 5.29, avg. loss 5.29\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6, Train loss: 5.001, Val loss: 8.479, Epoch time = 0.397s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 7, iter 0, losses 5.26, avg. loss 5.26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7, Train loss: 4.966, Val loss: 8.640, Epoch time = 0.393s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 8, iter 0, losses 5.20, avg. loss 5.20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8, Train loss: 4.949, Val loss: 8.774, Epoch time = 0.390s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 9, iter 0, losses 5.14, avg. loss 5.14\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9, Train loss: 4.953, Val loss: 8.884, Epoch time = 0.393s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 10, iter 0, losses 5.08, avg. loss 5.08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10, Train loss: 4.940, Val loss: 8.990, Epoch time = 0.393s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 11, iter 0, losses 5.04, avg. loss 5.04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 11, Train loss: 4.926, Val loss: 9.104, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 12, iter 0, losses 5.02, avg. loss 5.02\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 12, Train loss: 4.920, Val loss: 9.209, Epoch time = 0.389s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 13, iter 0, losses 5.03, avg. loss 5.03\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 13, Train loss: 4.917, Val loss: 9.292, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 14, iter 0, losses 5.05, avg. loss 5.05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 14, Train loss: 4.912, Val loss: 9.356, Epoch time = 0.393s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 15, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 15, Train loss: 4.908, Val loss: 9.406, Epoch time = 0.392s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 16, iter 0, losses 5.09, avg. loss 5.09\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 16, Train loss: 4.909, Val loss: 9.443, Epoch time = 0.392s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 17, iter 0, losses 5.10, avg. loss 5.10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 17, Train loss: 4.910, Val loss: 9.470, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 18, iter 0, losses 5.10, avg. loss 5.10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 18, Train loss: 4.912, Val loss: 9.493, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 19, iter 0, losses 5.09, avg. loss 5.09\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 19, Train loss: 4.913, Val loss: 9.514, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 20, iter 0, losses 5.08, avg. loss 5.08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 20, Train loss: 4.912, Val loss: 9.531, Epoch time = 0.389s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 21, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 21, Train loss: 4.912, Val loss: 9.545, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 22, iter 0, losses 5.06, avg. loss 5.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 22, Train loss: 4.912, Val loss: 9.555, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 23, iter 0, losses 5.06, avg. loss 5.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 23, Train loss: 4.910, Val loss: 9.562, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 24, iter 0, losses 5.06, avg. loss 5.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 24, Train loss: 4.909, Val loss: 9.569, Epoch time = 0.390s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 25, iter 0, losses 5.06, avg. loss 5.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 25, Train loss: 4.907, Val loss: 9.574, Epoch time = 0.391s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 26, iter 0, losses 5.06, avg. loss 5.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 26, Train loss: 4.907, Val loss: 9.580, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 27, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 27, Train loss: 4.905, Val loss: 9.585, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 28, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 28, Train loss: 4.907, Val loss: 9.590, Epoch time = 0.389s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 29, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 29, Train loss: 4.907, Val loss: 9.593, Epoch time = 0.392s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 30, iter 0, losses 5.08, avg. loss 5.08\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 30, Train loss: 4.907, Val loss: 9.595, Epoch time = 0.394s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 31, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 31, Train loss: 4.908, Val loss: 9.596, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 32, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 32, Train loss: 4.908, Val loss: 9.595, Epoch time = 0.393s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 33, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 33, Train loss: 4.908, Val loss: 9.594, Epoch time = 0.391s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 34, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 34, Train loss: 4.908, Val loss: 9.593, Epoch time = 0.389s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 35, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 35, Train loss: 4.908, Val loss: 9.592, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 36, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 36, Train loss: 4.908, Val loss: 9.590, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 37, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 37, Train loss: 4.909, Val loss: 9.589, Epoch time = 0.383s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 38, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 38, Train loss: 4.906, Val loss: 9.587, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 39, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 39, Train loss: 4.907, Val loss: 9.586, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 40, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 40, Train loss: 4.908, Val loss: 9.584, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 41, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 41, Train loss: 4.907, Val loss: 9.583, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 42, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 42, Train loss: 4.906, Val loss: 9.582, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 43, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 43, Train loss: 4.907, Val loss: 9.581, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 44, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 44, Train loss: 4.908, Val loss: 9.580, Epoch time = 0.389s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 45, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 45, Train loss: 4.906, Val loss: 9.577, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 46, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 46, Train loss: 4.907, Val loss: 9.575, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 47, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 47, Train loss: 4.908, Val loss: 9.572, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 48, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 48, Train loss: 4.907, Val loss: 9.571, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 49, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 49, Train loss: 4.907, Val loss: 9.569, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 50, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 50, Train loss: 4.907, Val loss: 9.566, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 51, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 51, Train loss: 4.907, Val loss: 9.564, Epoch time = 0.381s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 52, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 52, Train loss: 4.908, Val loss: 9.562, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 53, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 53, Train loss: 4.908, Val loss: 9.559, Epoch time = 0.390s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 54, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 54, Train loss: 4.906, Val loss: 9.557, Epoch time = 0.390s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 55, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 55, Train loss: 4.908, Val loss: 9.554, Epoch time = 0.389s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 56, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 56, Train loss: 4.908, Val loss: 9.552, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 57, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 57, Train loss: 4.907, Val loss: 9.550, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 58, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 58, Train loss: 4.907, Val loss: 9.547, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 59, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 59, Train loss: 4.907, Val loss: 9.543, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 60, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 60, Train loss: 4.906, Val loss: 9.540, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 61, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 61, Train loss: 4.907, Val loss: 9.537, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 62, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 62, Train loss: 4.907, Val loss: 9.534, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 63, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 63, Train loss: 4.908, Val loss: 9.530, Epoch time = 0.382s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 64, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 64, Train loss: 4.906, Val loss: 9.528, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 65, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 65, Train loss: 4.906, Val loss: 9.525, Epoch time = 0.383s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 66, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 66, Train loss: 4.907, Val loss: 9.522, Epoch time = 0.381s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 67, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 67, Train loss: 4.907, Val loss: 9.520, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 68, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 68, Train loss: 4.907, Val loss: 9.517, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 69, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 69, Train loss: 4.908, Val loss: 9.514, Epoch time = 0.380s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 70, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 70, Train loss: 4.907, Val loss: 9.510, Epoch time = 0.383s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 71, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 71, Train loss: 4.907, Val loss: 9.507, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 72, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 72, Train loss: 4.908, Val loss: 9.504, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 73, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 73, Train loss: 4.908, Val loss: 9.501, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 74, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 74, Train loss: 4.907, Val loss: 9.499, Epoch time = 0.391s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 75, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 75, Train loss: 4.907, Val loss: 9.496, Epoch time = 0.382s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 76, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 76, Train loss: 4.907, Val loss: 9.493, Epoch time = 0.389s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 77, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 77, Train loss: 4.907, Val loss: 9.490, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 78, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 78, Train loss: 4.908, Val loss: 9.486, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 79, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 79, Train loss: 4.906, Val loss: 9.482, Epoch time = 0.383s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 80, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 80, Train loss: 4.907, Val loss: 9.478, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 81, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 81, Train loss: 4.906, Val loss: 9.474, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 82, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 82, Train loss: 4.907, Val loss: 9.471, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 83, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 83, Train loss: 4.907, Val loss: 9.467, Epoch time = 0.380s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 84, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 84, Train loss: 4.907, Val loss: 9.464, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 85, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 85, Train loss: 4.907, Val loss: 9.461, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 86, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 86, Train loss: 4.907, Val loss: 9.458, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 87, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 87, Train loss: 4.907, Val loss: 9.454, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 88, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 88, Train loss: 4.907, Val loss: 9.451, Epoch time = 0.383s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 89, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 89, Train loss: 4.907, Val loss: 9.445, Epoch time = 0.389s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 90, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 90, Train loss: 4.908, Val loss: 9.441, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 91, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 91, Train loss: 4.907, Val loss: 9.437, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 92, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 92, Train loss: 4.907, Val loss: 9.434, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 93, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 93, Train loss: 4.906, Val loss: 9.430, Epoch time = 0.383s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 94, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 94, Train loss: 4.907, Val loss: 9.427, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 95, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 95, Train loss: 4.907, Val loss: 9.426, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 96, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 96, Train loss: 4.906, Val loss: 9.424, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 97, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 97, Train loss: 4.907, Val loss: 9.422, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 98, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 98, Train loss: 4.908, Val loss: 9.419, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 99, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 99, Train loss: 4.907, Val loss: 9.414, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 100, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 100, Train loss: 4.907, Val loss: 9.411, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 101, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 101, Train loss: 4.907, Val loss: 9.408, Epoch time = 0.382s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 102, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 102, Train loss: 4.907, Val loss: 9.406, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 103, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 103, Train loss: 4.908, Val loss: 9.403, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 104, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 104, Train loss: 4.907, Val loss: 9.401, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 105, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 105, Train loss: 4.907, Val loss: 9.399, Epoch time = 0.383s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 106, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 106, Train loss: 4.906, Val loss: 9.396, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 107, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 107, Train loss: 4.906, Val loss: 9.392, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 108, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 108, Train loss: 4.907, Val loss: 9.388, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 109, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 109, Train loss: 4.906, Val loss: 9.384, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 110, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 110, Train loss: 4.906, Val loss: 9.383, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 111, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 111, Train loss: 4.906, Val loss: 9.380, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 112, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 112, Train loss: 4.907, Val loss: 9.375, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 113, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 113, Train loss: 4.907, Val loss: 9.371, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 114, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 114, Train loss: 4.906, Val loss: 9.368, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 115, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 115, Train loss: 4.908, Val loss: 9.364, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 116, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 116, Train loss: 4.908, Val loss: 9.360, Epoch time = 0.393s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 117, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 117, Train loss: 4.907, Val loss: 9.356, Epoch time = 0.391s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 118, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 118, Train loss: 4.906, Val loss: 9.354, Epoch time = 0.395s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 119, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 119, Train loss: 4.907, Val loss: 9.351, Epoch time = 0.383s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 120, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 120, Train loss: 4.906, Val loss: 9.350, Epoch time = 0.389s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 121, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 121, Train loss: 4.906, Val loss: 9.350, Epoch time = 0.384s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 122, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 122, Train loss: 4.906, Val loss: 9.349, Epoch time = 0.389s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 123, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 123, Train loss: 4.906, Val loss: 9.348, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 124, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 124, Train loss: 4.907, Val loss: 9.348, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 125, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 125, Train loss: 4.905, Val loss: 9.348, Epoch time = 0.382s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 126, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 126, Train loss: 4.906, Val loss: 9.348, Epoch time = 0.389s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 127, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 127, Train loss: 4.908, Val loss: 9.348, Epoch time = 0.390s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 128, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 128, Train loss: 4.908, Val loss: 9.349, Epoch time = 0.390s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 129, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 129, Train loss: 4.906, Val loss: 9.350, Epoch time = 0.392s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 130, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 130, Train loss: 4.906, Val loss: 9.350, Epoch time = 0.391s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 131, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 131, Train loss: 4.907, Val loss: 9.350, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 132, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 132, Train loss: 4.907, Val loss: 9.352, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 133, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 133, Train loss: 4.907, Val loss: 9.356, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 134, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 134, Train loss: 4.907, Val loss: 9.358, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 135, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 135, Train loss: 4.907, Val loss: 9.360, Epoch time = 0.389s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 136, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 136, Train loss: 4.906, Val loss: 9.361, Epoch time = 0.390s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 137, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 137, Train loss: 4.907, Val loss: 9.361, Epoch time = 0.383s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 138, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 138, Train loss: 4.906, Val loss: 9.362, Epoch time = 0.389s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 139, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 139, Train loss: 4.906, Val loss: 9.365, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 140, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 140, Train loss: 4.906, Val loss: 9.366, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 141, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 141, Train loss: 4.907, Val loss: 9.362, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 142, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 142, Train loss: 4.907, Val loss: 9.361, Epoch time = 0.389s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 143, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 143, Train loss: 4.908, Val loss: 9.361, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 144, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 144, Train loss: 4.907, Val loss: 9.359, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 145, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 145, Train loss: 4.907, Val loss: 9.358, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 146, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 146, Train loss: 4.908, Val loss: 9.358, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 147, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 147, Train loss: 4.907, Val loss: 9.358, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 148, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 148, Train loss: 4.907, Val loss: 9.359, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 149, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 149, Train loss: 4.906, Val loss: 9.360, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 150, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 150, Train loss: 4.907, Val loss: 9.361, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 151, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 151, Train loss: 4.906, Val loss: 9.363, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 152, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 152, Train loss: 4.907, Val loss: 9.365, Epoch time = 0.389s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 153, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 153, Train loss: 4.906, Val loss: 9.366, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 154, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 154, Train loss: 4.908, Val loss: 9.370, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 155, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 155, Train loss: 4.907, Val loss: 9.375, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 156, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 156, Train loss: 4.906, Val loss: 9.380, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 157, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 157, Train loss: 4.906, Val loss: 9.382, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 158, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 158, Train loss: 4.906, Val loss: 9.384, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 159, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 159, Train loss: 4.905, Val loss: 9.387, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 160, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 160, Train loss: 4.905, Val loss: 9.388, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 161, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 161, Train loss: 4.907, Val loss: 9.391, Epoch time = 0.392s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 162, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 162, Train loss: 4.905, Val loss: 9.394, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 163, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 163, Train loss: 4.907, Val loss: 9.395, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 164, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 164, Train loss: 4.907, Val loss: 9.396, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 165, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 165, Train loss: 4.906, Val loss: 9.397, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 166, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 166, Train loss: 4.906, Val loss: 9.399, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 167, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 167, Train loss: 4.906, Val loss: 9.401, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 168, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 168, Train loss: 4.907, Val loss: 9.404, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 169, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 169, Train loss: 4.906, Val loss: 9.410, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 170, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 170, Train loss: 4.905, Val loss: 9.415, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 171, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 171, Train loss: 4.906, Val loss: 9.417, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 172, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 172, Train loss: 4.907, Val loss: 9.422, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 173, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 173, Train loss: 4.907, Val loss: 9.427, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 174, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 174, Train loss: 4.907, Val loss: 9.431, Epoch time = 0.392s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 175, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 175, Train loss: 4.906, Val loss: 9.440, Epoch time = 0.391s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 176, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 176, Train loss: 4.905, Val loss: 9.444, Epoch time = 0.389s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 177, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 177, Train loss: 4.906, Val loss: 9.445, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 178, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 178, Train loss: 4.906, Val loss: 9.453, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 179, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 179, Train loss: 4.907, Val loss: 9.458, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 180, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 180, Train loss: 4.906, Val loss: 9.459, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 181, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 181, Train loss: 4.906, Val loss: 9.464, Epoch time = 0.391s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 182, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 182, Train loss: 4.906, Val loss: 9.468, Epoch time = 0.390s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 183, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 183, Train loss: 4.907, Val loss: 9.471, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 184, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 184, Train loss: 4.906, Val loss: 9.475, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 185, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 185, Train loss: 4.907, Val loss: 9.478, Epoch time = 0.389s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 186, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 186, Train loss: 4.906, Val loss: 9.482, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 187, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 187, Train loss: 4.906, Val loss: 9.489, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 188, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 188, Train loss: 4.907, Val loss: 9.494, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 189, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 189, Train loss: 4.906, Val loss: 9.497, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 190, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 190, Train loss: 4.906, Val loss: 9.502, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 191, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 191, Train loss: 4.906, Val loss: 9.506, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 192, iter 0, losses 5.06, avg. loss 5.06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 192, Train loss: 4.904, Val loss: 9.512, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 193, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 193, Train loss: 4.905, Val loss: 9.516, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 194, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 194, Train loss: 4.906, Val loss: 9.519, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 195, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 195, Train loss: 4.905, Val loss: 9.523, Epoch time = 0.385s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 196, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 196, Train loss: 4.905, Val loss: 9.527, Epoch time = 0.386s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 197, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 197, Train loss: 4.905, Val loss: 9.529, Epoch time = 0.387s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 198, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 198, Train loss: 4.906, Val loss: 9.531, Epoch time = 0.388s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 199, iter 0, losses 5.07, avg. loss 5.07\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 199, Train loss: 4.905, Val loss: 9.536, Epoch time = 0.390s\n"
          ]
        }
      ],
      "source": [
        "from timeit import default_timer as timer\n",
        "args.max_epoch = 200\n",
        "\n",
        "for epoch in range(args.max_epoch):\n",
        "    start_time = timer()\n",
        "    train_loss = train_epoch(transformer, optimizer, epoch)\n",
        "    end_time = timer()\n",
        "    val_loss = evaluate(transformer)\n",
        "    print((f\"Epoch: {epoch}, Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}, \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhtPlECcIrVz",
        "outputId": "2b352d9c-8cc1-47e5-c90d-2493052220e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Nov 13 22:17:24 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0              72W /  70W |   5289MiB / 15360MiB |     86%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiVrxI2tasTR"
      },
      "source": [
        "## Testing the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhLRKX6vccix"
      },
      "source": [
        "Greedy Decoding: At each step, pick the most probable token.\n",
        "\n",
        "The straightforward decoding strategy is greedy - at each step, generate a token with the highest probability. This can be a good baseline to test our model, but this method is inherently flawed: the best token at the current step does not necessarily lead to the best sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adbc1m3Ec9aj"
      },
      "source": [
        "## Q10: (5 points) Greedy Decoding\n",
        "Implement the `greedy_decode` function  to initialize the\n",
        "necessary module for our TransformerNMT model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1wBX52xaaeq"
      },
      "outputs": [],
      "source": [
        "# function to generate output sequence using greedy algorithm\n",
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    \"\"\" Generates a sequence using greedy decoding based on the given model.\n",
        "    @param model (torch.nn.Module): The transformer model used for decoding.\n",
        "    @param src (torch.Tensor): The source sequence tensor.\n",
        "    @param src_mask (torch.Tensor): The mask for the source sequence.\n",
        "    @param max_len (int): The maximum length of the generated sequence.\n",
        "    @param start_symbol (int): The starting symbol for the decoding process.\n",
        "\n",
        "    @returns: torch.Tensor: The generated sequence tensor.\n",
        "\n",
        "    Note:\n",
        "        The decoding process is performed using greedy decoding, where at each step,\n",
        "        the model predicts the next word in the sequence based on the highest probability.\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    src_seg_len = 13\n",
        "    bs = 1\n",
        "    \"\"\"\n",
        "    src = src.to(DEVICE) # (src_seg_len, bs)\n",
        "    #print(\"src\", src.shape) # torch.Size([13, 1])\n",
        "    src_mask = src_mask.to(DEVICE) # (src_seg_len, src_seg_len)\n",
        "    #print(\"src_mask\", src_mask.shape) # torch.Size([13, 13])\n",
        "\n",
        "    # ys = None\n",
        "    # YOUR CODE HERE (~12 lines)\n",
        "    ### TODO - Complete the sequence generation loop:\n",
        "    ###     1. Initialize `ys` with the start token index # (tgt_seg_len, 1). This will be the initial input to the decoder.\n",
        "    ###     2. Encode the source sentence using `encode` function and store it as `memory`.\n",
        "    ###     3. For each step in the loop (up to `max_len-1`):\n",
        "    ###         a. Move `memory` to the device.\n",
        "    ###         b. Generate the target mask using `generate_square_subsequent_mask`.\n",
        "    ###         c. Decode the current sequence `ys` using `decode` function.\n",
        "    ###         d. Transpose the output and project it to the target vocabulary using `target_vocab_projection`.\n",
        "    ###         e. Get the next word by taking the argmax of the probabilities.\n",
        "    ###         f. Append the next word to `ys`.\n",
        "    ###         g. Break the loop if the next word is the end-of-sequence token (`EOS_IDX`).\n",
        "    # $$     Hints:\n",
        "    ###           - ys is shape (tgt_seg_len, bs)\n",
        "    ###           - memory is shape (src_seg_len, bs, emb_size)\n",
        "    ###           - output after decoding is shape (tgt_seg_len, bs, emb_size)\n",
        "    ys = torch.tensor([[start_symbol]]).to(DEVICE) # shape(tgt_seg_len, bs)\n",
        "    #print(\"ys\", ys.shape) # torch.Size([1, 1])\n",
        "    memory = model.encode(src, src_mask) # shape(src_seg_len, bs, emb_size)\n",
        "    #print(\"memory\", memory.shape) # torch.Size([13, 1, 776])\n",
        "    for i in range(max_len):\n",
        "        memory = memory.to(DEVICE) # shape(src_seg_len, bs, emb_size)\n",
        "        # print(\"memory\", memory.shape)\n",
        "        tgt_mask = generate_square_subsequent_mask(ys.shape[0]).to(DEVICE) # shape(tgt_seg_len, tgt_seg_len)\n",
        "        # print(\"tgt_mask\", tgt_mask.shape)\n",
        "        dec_output = model.decode(ys, memory, tgt_mask) # shape(tgt_seg_len, bs, emb_size)\n",
        "        # print(\"dec_output\", dec_output.shape)\n",
        "        logits = model.target_vocab_projection(dec_output) # shape(tgt_seg_len, bs, tgt_vocab_size)\n",
        "        # print(\"logits\", logits.shape)\n",
        "        # target_vocab_projection shape (emb_size, tgt_vocab_size)\n",
        "        predicted_token_idx = torch.argmax(logits[-1, ...], dim=-1) # shape(bs)\n",
        "        print(\"predicted_token_idx\", predicted_token_idx)\n",
        "        print(\"tgt_tensor\", tgt_tensor.shape)\n",
        "        ys = torch.cat((ys, predicted_token_idx.view(1, -1)), dim=0) # shape(tgt_seg_len+1, bs)\n",
        "        print(\"ys\", ys.shape)\n",
        "        if predicted_token_idx == vocab.tgt.words2indices(EOS_IDX):\n",
        "            break\n",
        "    # END YOUR CODE\n",
        "    return ys\n",
        "\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_sentence: str):\n",
        "    model.eval()\n",
        "    src = text_transform[SRC_LANGUAGE](src_sentence).view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  src, src_mask, max_len=num_tokens + 20, start_symbol=BOS_IDX).flatten()\n",
        "    return \" \".join(vocab.tgt.indices2words(list(tgt_tokens.cpu().numpy()))).replace(\"<s>\", \"\").replace(\"</s>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggy9A0qBdfmL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86baafe5-a3ff-4e22-b53c-66c200234146"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([2, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([3, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([4, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([5, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([6, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([7, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([8, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([9, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([10, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([11, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([12, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([13, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([14, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([15, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([16, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([17, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([18, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([19, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([20, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([21, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([22, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([23, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([24, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([25, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([26, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([27, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([28, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([29, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([30, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([31, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([32, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([33, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([34, 1])\n",
            "'int' object is not subscriptable 2\n",
            "Sample 0:\n",
            "Source: ‚ñÅKhoa ‚ñÅh·ªçc ‚ñÅƒë ·∫±ng ‚ñÅsau ‚ñÅm·ªôt ‚ñÅti√™u ‚ñÅƒë·ªÅ ‚ñÅv·ªÅ ‚ñÅkh√≠ ‚ñÅh·∫≠u\n",
            "Reference:  ‚ñÅRachel ‚ñÅP ike ‚ñÅ : ‚ñÅThe ‚ñÅscience ‚ñÅ behind ‚ñÅa ‚ñÅclimate ‚ñÅheadline \n",
            "Translation:                                  \n",
            "\n",
            "\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([2, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([3, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([4, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([5, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([6, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([7, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([8, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([9, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([10, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([11, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([12, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([13, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([14, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([15, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([16, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([17, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([18, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([19, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([20, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([21, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([22, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([23, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([24, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([25, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([26, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([27, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([28, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([29, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([30, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([31, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([32, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([33, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([34, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([35, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([36, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([37, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([38, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([39, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([40, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([41, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([42, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([43, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([44, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([45, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([46, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([47, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([48, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([49, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([50, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([51, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([52, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([53, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([54, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([55, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([56, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([57, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([58, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([59, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([60, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([61, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([62, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([63, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([64, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([65, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([66, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([67, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([68, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([69, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([70, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([71, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([72, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([73, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([74, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([75, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([76, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([77, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([78, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([79, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([80, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([81, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([82, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([83, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([84, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([85, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([86, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([87, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([88, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([89, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([90, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([91, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([92, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([93, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([94, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([95, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([96, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([97, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([98, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([99, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([100, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([101, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([102, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([103, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([104, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([105, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([106, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([107, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([108, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([109, 1])\n",
            "'int' object is not subscriptable 2\n",
            "Sample 1:\n",
            "Source: ‚ñÅTro ng ‚ñÅ4 ‚ñÅph√∫t ‚ñÅ, ‚ñÅchuy√™n ‚ñÅgia ‚ñÅho√° ‚ñÅh·ªçc ‚ñÅkh√≠ ‚ñÅquy·ªÉn ‚ñÅRachel ‚ñÅP ike ‚ñÅgi·ªõi ‚ñÅt hi·ªáu ‚ñÅs∆° ‚ñÅl∆∞·ª£c ‚ñÅv·ªÅ ‚ñÅnh·ªØng ‚ñÅn·ªó ‚ñÅl·ª±c ‚ñÅkhoa ‚ñÅh·ªçc ‚ñÅm i·ªát ‚ñÅm√† i ‚ñÅƒë ·∫±ng ‚ñÅsau ‚ñÅnh·ªØng ‚ñÅti√™u ‚ñÅƒë·ªÅ ‚ñÅt√°o ‚ñÅb·∫°o ‚ñÅv·ªÅ ‚ñÅbi·∫øn ‚ñÅƒë·ªïi ‚ñÅkh√≠ ‚ñÅh·∫≠u ‚ñÅ, ‚ñÅc√πng ‚ñÅv·ªõi ‚ñÅƒëo√†n ‚ñÅnghi√™n ‚ñÅc·ª©u ‚ñÅc·ªßa ‚ñÅm√¨nh ‚ñÅ-- ‚ñÅh√†ng ‚ñÅng√†n ‚ñÅng∆∞·ªùi ‚ñÅƒë√£ ‚ñÅc·ªë ng ‚ñÅ hi·∫øn ‚ñÅcho ‚ñÅd·ª± ‚ñÅ√°n ‚ñÅn√†y ‚ñÅ-- ‚ñÅm·ªôt ‚ñÅchuy·∫øn ‚ñÅbay ‚ñÅm·∫°o ‚ñÅhi·ªÉm ‚ñÅqua ‚ñÅr·ª´ng ‚ñÅgi√† ‚ñÅƒë·ªÉ ‚ñÅt√¨m ‚ñÅki·∫øm ‚ñÅth√¥ng ‚ñÅtin ‚ñÅv·ªÅ ‚ñÅm·ªôt ‚ñÅph√¢n ‚ñÅt·ª≠ ‚ñÅthe n ‚ñÅch·ªë t ‚ñÅ.\n",
            "Reference:  ‚ñÅIn ‚ñÅ4 ‚ñÅminutes ‚ñÅ , ‚ñÅatmospher ic ‚ñÅchemist ‚ñÅRachel ‚ñÅP ike ‚ñÅprovide s ‚ñÅa ‚ñÅglimpse ‚ñÅof ‚ñÅthe ‚ñÅmassive ‚ñÅscientific ‚ñÅeffort ‚ñÅ behind ‚ñÅthe ‚ñÅbold ‚ñÅheadline s ‚ñÅon ‚ñÅclimate ‚ñÅchange ‚ñÅ , ‚ñÅwith ‚ñÅher ‚ñÅteam ‚ñÅ-- ‚ñÅone ‚ñÅof ‚ñÅ thousands ‚ñÅwho ‚ñÅcontribute d ‚ñÅ-- ‚ñÅ taking ‚ñÅa ‚ñÅrisk y ‚ñÅflight ‚ñÅover ‚ñÅthe ‚ñÅrainforest ‚ñÅin ‚ñÅpursuit ‚ñÅof ‚ñÅdata ‚ñÅon ‚ñÅa ‚ñÅkey ‚ñÅmolecule ‚ñÅ. \n",
            "Translation:                                                                                                             \n",
            "\n",
            "\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([2, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([3, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([4, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([5, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([6, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([7, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([8, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([9, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([10, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([11, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([12, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([13, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([14, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([15, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([16, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([17, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([18, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([19, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([20, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([21, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([22, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([23, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([24, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([25, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([26, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([27, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([28, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([29, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([30, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([31, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([32, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([33, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([34, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([35, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([36, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([37, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([38, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([39, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([40, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([41, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([42, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([43, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([44, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([45, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([46, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([47, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([48, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([49, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([50, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([51, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([52, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([53, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([54, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([55, 1])\n",
            "'int' object is not subscriptable 2\n",
            "Sample 2:\n",
            "Source: ‚ñÅT√¥i ‚ñÅmu·ªën ‚ñÅcho ‚ñÅc√°c ‚ñÅb·∫°n ‚ñÅbi·∫øt ‚ñÅv·ªÅ ‚ñÅs·ª± ‚ñÅto ‚ñÅl·ªõn ‚ñÅc·ªßa ‚ñÅnh·ªØng ‚ñÅn·ªó ‚ñÅl·ª±c ‚ñÅkhoa ‚ñÅh·ªçc ‚ñÅƒë√£ ‚ñÅg√≥p ‚ñÅph·∫ßn ‚ñÅl√†m ‚ñÅn√™n ‚ñÅc√°c ‚ñÅd√≤ ng ‚ñÅt√≠ t ‚ñÅb·∫°n ‚ñÅth∆∞·ªùng ‚ñÅth·∫•y ‚ñÅtr√™n ‚ñÅb√°o ‚ñÅ.\n",
            "Reference:  ‚ñÅI ‚ñÅ& apos ; d ‚ñÅlike ‚ñÅto ‚ñÅtalk ‚ñÅto ‚ñÅyou ‚ñÅto day ‚ñÅabout ‚ñÅthe ‚ñÅscale ‚ñÅof ‚ñÅthe ‚ñÅscientific ‚ñÅeffort ‚ñÅthat ‚ñÅgo es ‚ñÅinto ‚ñÅ making ‚ñÅthe ‚ñÅheadline s ‚ñÅyou ‚ñÅsee ‚ñÅin ‚ñÅthe ‚ñÅpaper ‚ñÅ. \n",
            "Translation:                                                       \n",
            "\n",
            "\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([2, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([3, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([4, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([5, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([6, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([7, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([8, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([9, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([10, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([11, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([12, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([13, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([14, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([15, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([16, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([17, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([18, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([19, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([20, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([21, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([22, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([23, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([24, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([25, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([26, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([27, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([28, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([29, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([30, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([31, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([32, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([33, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([34, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([35, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([36, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([37, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([38, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([39, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([40, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([41, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([42, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([43, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([44, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([45, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([46, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([47, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([48, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([49, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([50, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([51, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([52, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([53, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([54, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([55, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([56, 1])\n",
            "'int' object is not subscriptable 2\n",
            "Sample 3:\n",
            "Source: ‚ñÅC√≥ ‚ñÅnh·ªØng ‚ñÅd√≤ ng ‚ñÅtr √¥ng ‚ñÅnh∆∞ ‚ñÅth·∫ø ‚ñÅn√†y ‚ñÅkhi ‚ñÅb√†n ‚ñÅv·ªÅ ‚ñÅbi·∫øn ‚ñÅƒë·ªïi ‚ñÅkh√≠ ‚ñÅh·∫≠u ‚ñÅ, ‚ñÅv√† ‚ñÅnh∆∞ ‚ñÅth·∫ø ‚ñÅn√†y ‚ñÅkhi ‚ñÅn√≥i ‚ñÅv·ªÅ ‚ñÅch·∫•t ‚ñÅl∆∞·ª£ng ‚ñÅkh√¥ng ‚ñÅkh√≠ ‚ñÅhay ‚ñÅkh√≥i ‚ñÅb·ª• i ‚ñÅ.\n",
            "Reference:  ‚ñÅHead lines ‚ñÅthat ‚ñÅlook ‚ñÅlike ‚ñÅ this ‚ñÅwhen ‚ñÅthe y ‚ñÅhave ‚ñÅto ‚ñÅdo ‚ñÅwith ‚ñÅclimate ‚ñÅchange ‚ñÅ , ‚ñÅand ‚ñÅheadline s ‚ñÅthat ‚ñÅlook ‚ñÅlike ‚ñÅ this ‚ñÅwhen ‚ñÅthe y ‚ñÅhave ‚ñÅto ‚ñÅdo ‚ñÅwith ‚ñÅair ‚ñÅquality ‚ñÅor ‚ñÅsmog ‚ñÅ. \n",
            "Translation:                                                        \n",
            "\n",
            "\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([2, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([3, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([4, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([5, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([6, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([7, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([8, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([9, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([10, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([11, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([12, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([13, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([14, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([15, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([16, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([17, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([18, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([19, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([20, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([21, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([22, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([23, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([24, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([25, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([26, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([27, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([28, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([29, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([30, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([31, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([32, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([33, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([34, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([35, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([36, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([37, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([38, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([39, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([40, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([41, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([42, 1])\n",
            "'int' object is not subscriptable 2\n",
            "Sample 4:\n",
            "Source: ‚ñÅC·∫£ ‚ñÅhai ‚ñÅƒë·ªÅu ‚ñÅl√† ‚ñÅm·ªôt ‚ñÅnh √°nh ‚ñÅc·ªßa ‚ñÅc√πng ‚ñÅm·ªôt ‚ñÅlƒ©nh ‚ñÅv·ª±c ‚ñÅtrong ‚ñÅng√†nh ‚ñÅkhoa ‚ñÅh·ªçc ‚ñÅkh√≠ ‚ñÅquy·ªÉn ‚ñÅ.\n",
            "Reference:  ‚ñÅThe y ‚ñÅare ‚ñÅboth ‚ñÅtwo ‚ñÅbranche s ‚ñÅof ‚ñÅthe ‚ñÅsame ‚ñÅfield ‚ñÅof ‚ñÅatmospher ic ‚ñÅscience ‚ñÅ. \n",
            "Translation:                                          \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Testing on training set\n",
        "num_logs = 5\n",
        "if (num_logs > len(train_data_small)):\n",
        "  num_logs = len(train_data_small)\n",
        "\n",
        "for i in range(num_logs):\n",
        "  src_sentence, tgt_sentence = train_data_small[i]\n",
        "  translation = translate(transformer, src_sentence)\n",
        "  print(f\"Sample {i}:\")\n",
        "  print(\"Source: \" + \" \".join(src_sentence))\n",
        "  print(\"Reference: \" + \" \".join(tgt_sentence).replace(\"<s>\", \"\").replace(\"</s>\", \"\"))\n",
        "  print(\"Translation: \" + translation)\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTTotHLSdf82",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d0e5ef1-5ab5-4104-a853-64ede8b0fa78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([2, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([3, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([4, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([5, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([6, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([7, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([8, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([9, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([10, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([11, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([12, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([13, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([14, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([15, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([16, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([17, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([18, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([19, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([20, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([21, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([22, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([23, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([24, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([25, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([26, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([27, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([28, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([29, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([30, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([31, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([32, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([33, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([34, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([35, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([36, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([37, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([38, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([39, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([40, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([41, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([42, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([43, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([44, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([45, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([46, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([47, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([48, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([49, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([50, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([51, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([52, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([53, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([54, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([55, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([56, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([57, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([58, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([59, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([60, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([61, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([62, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([63, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([64, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([65, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([66, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([67, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([68, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([69, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([70, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([71, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([72, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([73, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([74, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([75, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([76, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([77, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([78, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([79, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([80, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([81, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([82, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([83, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([84, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([85, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([86, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([87, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([88, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([89, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([90, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([91, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([92, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([93, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([94, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([95, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([96, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([97, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([98, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([99, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([100, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([101, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([102, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([103, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([104, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([105, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([106, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([107, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([108, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([109, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([110, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([111, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([112, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([113, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([114, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([115, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([116, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([117, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([118, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([119, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([120, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([121, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([122, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([123, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([124, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([125, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([126, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([127, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([128, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([129, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([130, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([131, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([132, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([133, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([134, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([135, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([136, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([137, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([138, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([139, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([140, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([141, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([142, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([143, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([144, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([145, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([146, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([147, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([148, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([149, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([150, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([151, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([152, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([153, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([154, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([155, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([156, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([157, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([158, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([159, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([160, 1])\n",
            "'int' object is not subscriptable 2\n",
            "Sample 0:\n",
            "Source: ‚ñÅL√† m ‚ñÅsao ‚ñÅt√¥i ‚ñÅc√≥ ‚ñÅth·ªÉ ‚ñÅtr√¨nh ‚ñÅb√†y ‚ñÅtrong ‚ñÅ10 ‚ñÅph√∫t ‚ñÅv·ªÅ ‚ñÅs·ª£ i ‚ñÅd√¢y ‚ñÅli√™n ‚ñÅk·∫øt ‚ñÅnh·ªØng ‚ñÅng∆∞·ªùi ‚ñÅph·ª• ‚ñÅn·ªØ ‚ñÅqua ‚ñÅba ‚ñÅth·∫ø ‚ñÅh·ªá ‚ñÅ, ‚ñÅv·ªÅ ‚ñÅvi·ªác ‚ñÅl√†m ‚ñÅth·∫ø ‚ñÅn√†o ‚ñÅnh·ªØng ‚ñÅs·ª£ i ‚ñÅd√¢y ‚ñÅm·∫°nh ‚ñÅm·∫Ω ‚ñÅƒë√°ng ‚ñÅkinh ‚ñÅng·∫°c ‚ñÅ·∫•y ‚ñÅƒë√£ ‚ñÅn√≠ u ‚ñÅch ·∫∑t ‚ñÅl·∫•y ‚ñÅcu·ªôc ‚ñÅs·ªëng ‚ñÅc·ªßa ‚ñÅm·ªôt ‚ñÅc√¥ ‚ñÅb√© ‚ñÅb·ªën ‚ñÅtu·ªïi ‚ñÅco ‚ñÅqu ·∫Øp ‚ñÅv·ªõi ‚ñÅƒë ·ª©a ‚ñÅem ‚ñÅg√°i ‚ñÅnh·ªè ‚ñÅc·ªßa ‚ñÅc√¥ ‚ñÅb√© ‚ñÅ, ‚ñÅv·ªõi ‚ñÅm·∫π ‚ñÅv√† ‚ñÅb√† ‚ñÅtrong ‚ñÅsu·ªët ‚ñÅnƒÉm ‚ñÅng√†y ‚ñÅƒë√™m ‚ñÅtr√™n ‚ñÅcon ‚ñÅthuy·ªÅn ‚ñÅnh·ªè ‚ñÅl√™n h ‚ñÅƒë√™ nh ‚ñÅtr√™n ‚ñÅB i·ªÉn ‚ñÅƒê√¥ng ‚ñÅh∆°n ‚ñÅ30 ‚ñÅnƒÉm ‚ñÅtr∆∞·ªõc ‚ñÅ, ‚ñÅnh·ªØng ‚ñÅs·ª£ i ‚ñÅd√¢y ‚ñÅli√™n ‚ñÅk·∫øt ‚ñÅƒë√£ ‚ñÅn√≠ u ‚ñÅl·∫•y ‚ñÅcu·ªôc ‚ñÅƒë·ªùi ‚ñÅc√¥ ‚ñÅb√© ‚ñÅ·∫•y ‚ñÅv√† ‚ñÅkh√¥ng ‚ñÅbao ‚ñÅgi·ªù ‚ñÅ r·ªùi ‚ñÅƒëi ‚ñÅ-- ‚ñÅc√¥ ‚ñÅb√© ‚ñÅ·∫•y ‚ñÅgi·ªù ‚ñÅs·ªëng ‚ñÅ·ªü ‚ñÅSan ‚ñÅFrancis co ‚ñÅv√† ‚ñÅƒëang ‚ñÅn√≥i ‚ñÅchuy·ªán ‚ñÅv·ªõi ‚ñÅc√°c ‚ñÅb·∫°n ‚ñÅ h√¥m ‚ñÅnay ‚ñÅ?\n",
            "Reference:  ‚ñÅHow ‚ñÅcan ‚ñÅI ‚ñÅspeak ‚ñÅin ‚ñÅ10 ‚ñÅminutes ‚ñÅabout ‚ñÅthe ‚ñÅbond s ‚ñÅof ‚ñÅwomen ‚ñÅover ‚ñÅthree ‚ñÅgeneration s ‚ñÅ , ‚ñÅabout ‚ñÅhow ‚ñÅthe ‚ñÅastonishing ‚ñÅstrength ‚ñÅof ‚ñÅt hose ‚ñÅbond s ‚ñÅto ok ‚ñÅhold ‚ñÅin ‚ñÅthe ‚ñÅlife ‚ñÅof ‚ñÅa ‚ñÅfour - year - old ‚ñÅgirl ‚ñÅhuddle d ‚ñÅwith ‚ñÅher ‚ñÅyoung ‚ñÅsister ‚ñÅ , ‚ñÅher ‚ñÅmother ‚ñÅand ‚ñÅher ‚ñÅgrandmother ‚ñÅfor ‚ñÅfive ‚ñÅdays ‚ñÅand ‚ñÅnight s ‚ñÅin ‚ñÅa ‚ñÅsmall ‚ñÅboat ‚ñÅin ‚ñÅthe ‚ñÅChina ‚ñÅSea ‚ñÅmore ‚ñÅthan ‚ñÅ30 ‚ñÅyears ‚ñÅago ‚ñÅ , ‚ñÅbond s ‚ñÅthat ‚ñÅto ok ‚ñÅhold ‚ñÅin ‚ñÅthe ‚ñÅlife ‚ñÅof ‚ñÅthat ‚ñÅsmall ‚ñÅgirl ‚ñÅand ‚ñÅnever ‚ñÅlet ‚ñÅgo ‚ñÅ-- ‚ñÅthat ‚ñÅsmall ‚ñÅgirl ‚ñÅnow ‚ñÅliving ‚ñÅin ‚ñÅSan ‚ñÅFrancisco ‚ñÅand ‚ñÅspeaking ‚ñÅto ‚ñÅyou ‚ñÅto day ‚ñÅ? \n",
            "Translation:                                                                                                                                                                \n",
            "\n",
            "\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([2, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([3, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([4, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([5, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([6, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([7, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([8, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([9, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([10, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([11, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([12, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([13, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([14, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([15, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([16, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([17, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([18, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([19, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([20, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([21, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([22, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([23, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([24, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([25, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([26, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([27, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([28, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([29, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([30, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([31, 1])\n",
            "'int' object is not subscriptable 2\n",
            "Sample 1:\n",
            "Source: ‚ñÅC √¢u ‚ñÅchuy·ªán ‚ñÅn√†y ‚ñÅch∆∞a ‚ñÅk·∫øt ‚ñÅth√∫c ‚ñÅ.\n",
            "Reference:  ‚ñÅThi s ‚ñÅis ‚ñÅnot ‚ñÅa ‚ñÅfinish ed ‚ñÅstory ‚ñÅ. \n",
            "Translation:                               \n",
            "\n",
            "\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([2, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([3, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([4, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([5, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([6, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([7, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([8, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([9, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([10, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([11, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([12, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([13, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([14, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([15, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([16, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([17, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([18, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([19, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([20, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([21, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([22, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([23, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([24, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([25, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([26, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([27, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([28, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([29, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([30, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([31, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([32, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([33, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([34, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([35, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([36, 1])\n",
            "'int' object is not subscriptable 2\n",
            "Sample 2:\n",
            "Source: ‚ñÅN√≥ ‚ñÅl√† ‚ñÅm·ªôt ‚ñÅtr√≤ ‚ñÅch∆°i ‚ñÅgh√©p ‚ñÅh√¨nh ‚ñÅv·∫´ n ‚ñÅƒëang ‚ñÅƒë∆∞·ª£c ‚ñÅx·∫øp ‚ñÅ.\n",
            "Reference:  ‚ñÅIt ‚ñÅis ‚ñÅa ‚ñÅjig saw ‚ñÅpuzzle ‚ñÅstill ‚ñÅbeing ‚ñÅput ‚ñÅtogether ‚ñÅ. \n",
            "Translation:                                    \n",
            "\n",
            "\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([2, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([3, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([4, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([5, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([6, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([7, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([8, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([9, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([10, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([11, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([12, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([13, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([14, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([15, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([16, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([17, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([18, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([19, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([20, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([21, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([22, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([23, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([24, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([25, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([26, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([27, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([28, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([29, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([30, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([31, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([32, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([33, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([34, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([35, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([36, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([37, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([38, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([39, 1])\n",
            "'int' object is not subscriptable 2\n",
            "Sample 3:\n",
            "Source: ‚ñÅH√£ y ‚ñÅƒë·ªÉ ‚ñÅt√¥i ‚ñÅk·ªÉ ‚ñÅcho ‚ñÅc√°c ‚ñÅb·∫°n ‚ñÅv·ªÅ ‚ñÅv√† i ‚ñÅm ·∫£nh ‚ñÅgh√©p ‚ñÅnh√© ‚ñÅ.\n",
            "Reference:  ‚ñÅLet ‚ñÅme ‚ñÅtell ‚ñÅyou ‚ñÅabout ‚ñÅsome ‚ñÅof ‚ñÅthe ‚ñÅpiece s ‚ñÅ. \n",
            "Translation:                                       \n",
            "\n",
            "\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([2, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([3, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([4, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([5, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([6, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([7, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([8, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([9, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([10, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([11, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([12, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([13, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([14, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([15, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([16, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([17, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([18, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([19, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([20, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([21, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([22, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([23, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([24, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([25, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([26, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([27, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([28, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([29, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([30, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([31, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([32, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([33, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([34, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([35, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([36, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([37, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([38, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([39, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([40, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([41, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([42, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([43, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([44, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([45, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([46, 1])\n",
            "'int' object is not subscriptable 2\n",
            "Sample 4:\n",
            "Source: ‚ñÅH√£ y ‚ñÅt∆∞·ªüng ‚ñÅt∆∞·ª£ng ‚ñÅm ·∫£nh ‚ñÅƒë·∫ßu ‚ñÅti√™n ‚ñÅ: ‚ñÅm·ªôt ‚ñÅng∆∞·ªùi ‚ñÅƒë√†n ‚ñÅ√¥ng ‚ñÅƒë·ªë t ‚ñÅch√° y ‚ñÅs·ª± ‚ñÅnghi·ªáp ‚ñÅc·∫£ ‚ñÅƒë·ªùi ‚ñÅm√¨nh ‚ñÅ.\n",
            "Reference:  ‚ñÅI magine ‚ñÅthe ‚ñÅfirst ‚ñÅpiece ‚ñÅ : ‚ñÅa ‚ñÅman ‚ñÅburning ‚ñÅhis ‚ñÅlife ‚ñÅ& apos ; s ‚ñÅwork ‚ñÅ. \n",
            "Translation:                                              \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# @title Testing on evaluate set\n",
        "num_logs = 5\n",
        "if (num_logs > len(dev_data_small)):\n",
        "  num_logs = len(dev_data_small)\n",
        "\n",
        "for i in range(num_logs):\n",
        "  src_sentence, tgt_sentence = dev_data_small[i]\n",
        "  translation = translate(transformer, src_sentence)\n",
        "  print(f\"Sample {i}:\")\n",
        "  print(\"Source: \" + \" \".join(src_sentence))\n",
        "  print(\"Reference: \" + \" \".join(tgt_sentence).replace(\"<s>\", \"\").replace(\"</s>\", \"\"))\n",
        "  print(\"Translation: \" + translation)\n",
        "  print('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP1WiKDW-H-M"
      },
      "source": [
        "## Attention Map Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wohO_TWw5AJM"
      },
      "source": [
        "After applying greedy decoding, we can further visualize it to see what is happening at each layer of the attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIjzNITn-PKL"
      },
      "source": [
        "`torch.nn.Transformer` does not support outputting attention map by default, so we will need to make some monkey patch (we do not recommend doing this in practice, it will be better to rewrite a customized Transformer code based on its Pytorch's implementation). Please refer to this [gist](https://gist.github.com/airalcorn2/50ec06517ce96ecc143503e21fa6cb91) for more detail."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "382xsmtA-Hdr"
      },
      "outputs": [],
      "source": [
        "# @title Helper functions\n",
        "def patch_attention(m):\n",
        "    forward_orig = m.forward\n",
        "\n",
        "    def wrap(*args, **kwargs):\n",
        "        kwargs[\"need_weights\"] = True\n",
        "        kwargs[\"average_attn_weights\"] = False\n",
        "\n",
        "        return forward_orig(*args, **kwargs)\n",
        "\n",
        "    m.forward = wrap\n",
        "\n",
        "class SaveOutput:\n",
        "    def __init__(self):\n",
        "        self.outputs = []\n",
        "\n",
        "    def __call__(self, module, module_in, module_out):\n",
        "        self.outputs.append(module_out[1])\n",
        "\n",
        "    def clear(self):\n",
        "        self.outputs = []\n",
        "\n",
        "def monkey_patch_pytorch_transformer(model, n_encoder_layers, n_decoder_layers):\n",
        "    encoder_hook_handles = []\n",
        "    encoder_save_outputs = []\n",
        "    for layer in range(n_encoder_layers):\n",
        "        patch_attention(model.encoder.layers[layer].self_attn)\n",
        "        encoder_save_outputs.append(SaveOutput())\n",
        "        encoder_hook_handles.append(model.encoder.layers[layer].self_attn.register_forward_hook(encoder_save_outputs[-1]))\n",
        "\n",
        "    decoder_self_attn_hook_handles = []\n",
        "    decoder_self_attn_save_outputs = []\n",
        "    decoder_cross_attn_hook_handles = []\n",
        "    decoder_cross_attn_save_outputs = []\n",
        "    for layer in range(n_decoder_layers):\n",
        "        patch_attention(model.decoder.layers[layer].self_attn)\n",
        "        decoder_self_attn_save_outputs.append(SaveOutput())\n",
        "        decoder_self_attn_hook_handles.append(model.decoder.layers[layer].self_attn.register_forward_hook(decoder_self_attn_save_outputs[-1]))\n",
        "\n",
        "        patch_attention(model.decoder.layers[layer].multihead_attn)\n",
        "        decoder_cross_attn_save_outputs.append(SaveOutput())\n",
        "        decoder_cross_attn_hook_handles.append(model.decoder.layers[layer].multihead_attn.register_forward_hook(decoder_cross_attn_save_outputs[-1]))\n",
        "\n",
        "    return encoder_hook_handles, encoder_save_outputs, \\\n",
        "    decoder_self_attn_hook_handles, decoder_self_attn_save_outputs, \\\n",
        "    decoder_cross_attn_hook_handles, decoder_cross_attn_save_outputs\n",
        "\n",
        "def remove_hook_pytorch_transformer(encoder_hook_handles, decoder_self_attn_hook_handles, decoder_cross_attn_hook_handles):\n",
        "    for hook in encoder_hook_handles:\n",
        "        hook.remove()\n",
        "    for hook in decoder_self_attn_hook_handles:\n",
        "        hook.remove()\n",
        "    for hook in decoder_cross_attn_hook_handles:\n",
        "        hook.remove()\n",
        "\n",
        "encoder_hook_handles, encoder_save_outputs, \\\n",
        "decoder_self_attn_hook_handles, decoder_self_attn_save_outputs, \\\n",
        "decoder_cross_attn_hook_handles, decoder_cross_attn_save_outputs \\\n",
        "= monkey_patch_pytorch_transformer(transformer.transformer, args.n_encoder_layers, args.n_decoder_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjRH_7EV-z-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed23e7f3-4c9e-41a0-b611-5a32bb3b5364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([2, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([3, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([4, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([5, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([6, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([7, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([8, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([9, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([10, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([11, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([12, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([13, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([14, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([15, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([16, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([17, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([18, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([19, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([20, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([21, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([22, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([23, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([24, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([25, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([26, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([27, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([28, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([29, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([30, 1])\n",
            "'int' object is not subscriptable 2\n",
            "predicted_token_idx tensor([2], device='cuda:0')\n",
            "tgt_tensor torch.Size([112, 2])\n",
            "ys torch.Size([31, 1])\n",
            "'int' object is not subscriptable 2\n"
          ]
        }
      ],
      "source": [
        "# Let visualize the attention mechanism with this example, since it is relatively short\n",
        "src_sentence, tgt_sentence = dev_data_small[1]\n",
        "translation = translate(transformer, src_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crRdhDrD_DSL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1237d26e-3f00-484b-f2a5-9838b0d8e608"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Layer 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Layer 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder Layer 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Self Layer 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Src Layer 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Self Layer 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Src Layer 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 8 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABiIAAAJeCAYAAAA5jR0SAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA3tUlEQVR4nO3de7DcdX3/8fdZDnzNIbCclJiIlYRoIeAQDSJWsRnq4DWNBi9jowUGoc4oOl6otuu0WLE/19qqo+No7YwFgs44CDhYFZWCIKAiiBDbSCESECIhImG5HNnQnM/vj0xClt3knD3Zz17OPh4zO2O+5+SbDyFPzM5rzveMpJRSAAAAAAAAZFDq9QEAAAAAAIDZyxABAAAAAABkY4gAAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsDBEAAAAAAEA2hggAAAAAACAbQwQAAAAAAJCNIQIAAAAAAMhmdLqfeOT843OeA9p2x+9u7vURdEHf0QU00wU063UXmqDf9LqJCF3Qf3QBzXQBzabbha+IAAAAAAAAsjFEAAAAAAAA2RgiAAAAAACAbAwRAAAAAABANoYIAAAAAAAgG0MEAAAAAACQjSECAAAAAADIxhABAAAAAABkM9rqYr1ej3q93nBtMk1GacRuAQAAAAAATF/LZaFarUa5XG54bZ3Y3O2zAQAAAAAAA67lEFGpVKJWqzW8xscWdvtsAAAAAADAgGv5aKaiKKIoioZrHssEAAAAAAC0y7oAAAAAAABkY4gAAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsDBEAAAAAAEA2hggAAAAAACAbQwQAAAAAAJCNIQIAAAAAAMjGEAEAAAAAAGRjiAAAAAAAALIxRAAAAAAAANkYIgAAAAAAgGwMEQAAAAAAQDaGCAAAAAAAIBtDBAAAAAAAkI0hAgAAAAAAyMYQAQAAAAAAZDPa6mK9Xo96vd5wbTJNRmnEbgEAAAAAAExfy2WhWq1GuVxueG2d2NztswEAAAAAAAOu5RBRqVSiVqs1vMbHFnb7bAAAAAAAwIBr+WimoiiiKIqGax7LBAAAAAAAtMu6AAAAAAAAZGOIAAAAAAAAsjFEAAAAAAAA2RgiAAAAAACAbAwRAAAAAABANoYIAAAAAAAgG0MEAAAAAACQjSECAAAAAADIxhABAAAAAABkY4gAAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsDBEAAAAAAEA2hggAAAAAACAbQwQAAAAAAJCNIQIAAAAAAMhmtNXFer0e9Xq94dpkmozSiN0CAAAAAACYvpbLQrVajXK53PDaOrG522cDAAAAAAAGXMsholKpRK1Wa3iNjy3s9tkAAAAAAIAB1/LRTEVRRFEUDdc8lgkAAAAAAGiXdQEAAAAAAMjGEAEAAAAAAGRjiAAAAAAAALIxRAAAAAAAANkYIgAAAAAAgGwMEQAAAAAAQDaGCAAAAAAAIBtDBAAAAAAAkI0hAgAAAAAAyMYQAQAAAAAAZGOIAAAAAAAAsjFEAAAAAAAA2RgiAAAAAACAbAwRAAAAAABANoYIAAAAAAAgG0MEAAAAAACQjSECAAAAAADIZrTVxXq9HvV6veHaZJqM0ojdAgAAAAAAmL6Wy0K1Wo1yudzw2jqxudtnAwAAAAAABlzLIaJSqUStVmt4jY8t7PbZAAAAAACAAdfy0UxFUURRFA3XPJYJAAAAAABol3UBAAAAAADIxhABAAAAAABkY4gAAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsDBEAAAAAAEA2hggAAAAAACAbQwQAAAAAAJCNIQIAAAAAAMjGEAEAAAAAAGRjiAAAAAAAALIxRAAAAAAAANkYIgAAAAAAgGwMEQAAAAAAQDaGCAAAAAAAIBtDBAAAAAAAkM1oq4v1ej3q9XrDtck0GaURuwUAAAAAADB9LZeFarUa5XK54bV1YnO3zwYAAAAAAAy4lkNEpVKJWq3W8BofW9jtswEAAAAAAAOu5aOZiqKIoigarnksEwAAAAAA0C7rAgAAAAAAkI0hAgAAAAAAyMYQAQAAAAAAZGOIAAAAAAAAsjFEAAAAAAAA2RgiAAAAAACAbAwRAAAAAABANoYIAAAAAAAgG0MEAAAAAACQjSECAAAAAADIxhABAAAAAABkY4gAAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsDBEAAAAAAEA2hggAAAAAACAbQwQAAAAAAJDNaKuL9Xo96vV6w7XJNBmlEbsFAAAAAAAwfS2XhWq1GuVyueG1dWJzt88GAAAAAAAMuJZDRKVSiVqt1vAaH1vY7bMBAAAAAAADruWjmYqiiKIoGq55LBMAAAAAANAu6wIAAAAAAJCNIQIAAAAAAMjGEAEAAAAAAGRjiAAAAAAAALIxRAAAAAAAANkYIgAAAAAAgGwMEQAAAAAAQDaGCAAAAAAAIBtDBAAAAAAAkI0hAgAAAAAAyMYQAQAAAAAAZGOIAAAAAAAAsjFEAAAAAAAA2RgiAAAAAACAbAwRAAAAAABANoYIAAAAAAAgG0MEAAAAAACQzWiri/V6Per1esO1yTQZpRG7BQAAAAAAMH0tl4VqtRrlcrnhtXVic7fPBgAAAAAADLiWQ0SlUolardbwGh9b2O2zAQAAAAAAA67lo5mKooiiKBqueSwTAAAAAADQLusCAAAAAACQjSECAAAAAADIxhABAAAAAABkY4gAAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsDBEAAAAAAEA2hggAAAAAACAbQwQAAAAAAJCNIQIAAAAAAMjGEAEAAAAAAGRjiAAAAAAAALIxRAAAAAAAANkYIgAAAAAAgGwMEQAAAAAAQDaGCAAAAAAAIJvRVhfr9XrU6/WGa5NpMkojdgsAAAAAAGD6Wi4L1Wo1yuVyw2vrxOZunw0AAAAAABhwLYeISqUStVqt4TU+trDbZwMAAAAAAAZcy0czFUURRVE0XPNYJgAAAAAAoF3WBQAAAAAAIBtDBAAAAAAAkI0hAgAAAAAAyMYQAQAAAAAAZGOIAAAAAAAAsjFEAAAAAAAA2RgiAAAAAACAbAwRAAAAAABANoYIAAAAAAAgG0MEAAAAAACQjSECAAAAAADIxhABAAAAAABkY4gAAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsDBEAAAAAAEA2hggAAAAAACCb0VYX6/V61Ov1hmuTaTJKI3YLAAAAAABg+louC9VqNcrlcsNr68Tmbp8NAAAAAAAYcC2HiEqlErVareE1Praw22cDAAAAAAAGXMtHMxVFEUVRNFzzWCYAAAAAAKBd1gUAAAAAACAbQwQAAAAAAJCNIQIAAAAAAMjGEAEAAAAAAGRjiAAAAAAAALIxRAAAAAAAANkYIgAAAAAAgGwMEQAAAAAAQDaGCAAAAAAAIBtDBAAAAAAAkI0hAgAAAAAAyMYQAQAAAAAAZGOIAAAAAAAAsjFEAAAAAAAA2RgiAAAAAACAbAwRAAAAAABANoYIAAAAAAAgm9FWF+v1etTr9YZrk2kySiN2CwAAAAAAYPpaLgvVajXK5XLDa+vE5m6fDQAAAAAAGHAth4hKpRK1Wq3hNT62sNtnAwAAAAAABlzLRzMVRRFFUTRc81gmAAAAAACgXdYFAAAAAAAgG0MEAAAAAACQjSECAAAAAADIxhABAAAAAABkY4gAAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsDBEAAAAAAEA2hggAAAAAACAbQwQAAAAAAJCNIQIAAAAAAMjGEAEAAAAAAGRjiAAAAAAAALIxRAAAAAAAANkYIgAAAAAAgGwMEQAAAAAAQDajrS7W6/Wo1+sN1ybTZJRG7BYAAAAAAMD0tVwWqtVqlMvlhtfWic3dPhsAAAAAADDgWg4RlUolarVaw2t8bGG3zwYAAAAAAAy4lo9mKooiiqJouOaxTAAAAAAAQLusCwAAAAAAQDaGCAAAAAAAIBtDBAAAAAAAkI0hAgAAAAAAyMYQAQAAAAAAZGOIAAAAAAAAsjFEAAAAAAAA2RgiAAAAAACAbAwRAAAAAABANoYIAAAAAAAgG0MEAAAAAACQjSECAAAAAADIxhABAAAAAABkY4gAAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsDBEAAAAAAEA2o60u1uv1qNfrDdcm02SURuwWAAAAAADA9LVcFqrVapTL5YbX1onN3T4bAAAAAAAw4FoOEZVKJWq1WsNrfGxht88GAAAAAAAMuJaPZiqKIoqiaLjmsUwAAAAAAEC7rAsAAAAAAEA2hggAAAAAACAbQwQAAAAAAJCNIQIAAAAAAMjGEAEAAAAAAGRjiAAAAAAAALIxRAAAAAAAANkYIgAAAAAAgGwMEQAAAAAAQDaGCAAAAAAAIBtDBAAAAAAAkI0hAgAAAAAAyMYQAQAAAAAAZGOIAAAAAAAAsjFEAAAAAAAA2RgiAAAAAACAbEZbXazX61Gv1xuuTabJKI3YLQAAAAAAgOlruSxUq9Uol8sNr60Tm7t9NgAAAAAAYMC1HCIqlUrUarWG1/jYwm6fDQAAAAAAGHAtH81UFEUURdFwzWOZAAAAAACAdlkXAAAAAACAbAwRAAAAAABANoYIAAAAAAAgG0MEAAAAAACQjSECAAAAAADIxhABAAAAAABkY4gAAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsDBEAAAAAAEA2hggAAAAAACAbQwQAAAAAAJCNIQIAAAAAAMjGEAEAAAAAAGRjiAAAAAAAALIxRAAAAAAAANkYIgAAAAAAgGxGW12s1+tRr9cbrk2mySiN2C0AAAAAAIDpa7ksVKvVKJfLDa+tE5u7fTYAAAAAAGDAtRwiKpVK1Gq1htf42MJunw0AAAAAABhwLR/NVBRFFEXRcM1jmQAAAAAAgHZZFwAAAAAAgGwMEQAAAAAAQDaGCAAAAAAAIBtDBAAAAAAAkI0hAgAAAAAAyMYQAQAAAAAAZGOIAAAAAAAAsjFEAAAAAAAA2RgiAAAAAACAbAwRAAAAAABANoYIAAAAAAAgG0MEAAAAAACQjSECAAAAAADIxhABAAAAAABkY4gAAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsRltdrNfrUa/XG65NpskojdgtAAAAAACA6Wu5LFSr1SiXyw2vrRObu302AAAAAABgwLUcIiqVStRqtYbX+NjCbp8NAAAAAAAYcC0fzVQURRRF0XDNY5kAAAAAAIB2WRcAAAAAAIBsDBEAAAAAAEA2hggAAAAAACAbQwQAAAAAAJCNIQIAAAAAAMjGEAEAAAAAAGRjiAAAAAAAALIxRAAAAAAAANkYIgAAAAAAgGwMEQAAAAAAQDaGCAAAAAAAIBtDBAAAAAAAkI0hAgAAAAAAyMYQAQAAAAAAZGOIAAAAAAAAsjFEAAAAAAAA2Yy2uliv16Nerzdcm0yTURqxWwAAAAAAANPXclmoVqtRLpcbXlsnNnf7bAAAAAAAwIBrOURUKpWo1WoNr/Gxhd0+GwAAAAAAMOBaPpqpKIooiqLhmscyAQAAAAAA7bIuAAAAAAAA2RgiAAAAAACAbAwRAAAAAABANoYIAAAAAAAgG0MEAAAAAACQjSECAAAAAADIxhABAAAAAABkY4gAAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsDBEAAAAAAEA2hggAAAAAACAbQwQAAAAAAJCNIQIAAAAAAMjGEAEAAAAAAGRjiAAAAAAAALIxRAAAAAAAANmMtrpYr9ejXq83XJtMk1EasVsAAAAAAADT13JZqFarUS6XG15bJzZ3+2wAAAAAAMCAazlEVCqVqNVqDa/xsYXdPhsAAAAAADDgWj6aqSiKKIqi4ZrHMgEAAAAAAO2yLgAAAAAAANkYIgAAAAAAgGwMEQAAAAAAQDaGCAAAAAAAIBtDBAAAAAAAkI0hAgAAAAAAyMYQAQAAAAAAZGOIAAAAAAAAsjFEAAAAAAAA2RgiAAAAAACAbAwRAAAAAABANoYIAAAAAAAgG0MEAAAAAACQjSECAAAAAADIxhABAAAAAABkY4gAAAAAAACyGW11sV6vR71eb7g2mSajNGK3AAAAAAAApq/lslCtVqNcLje8tk5s7vbZAAAAAACAAddyiKhUKlGr1Rpe42MLu302AAAAAABgwLV8NFNRFFEURcM1j2UCAAAAAADaZV0AAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsDBEAAAAAAEA2hggAAAAAACAbQwQAAAAAAJCNIQIAAAAAAMjGEAEAAAAAAGRjiAAAAAAAALIxRAAAAAAAANkYIgAAAAAAgGwMEQAAAAAAQDaGCAAAAAAAIBtDBAAAAAAAkI0hAgAAAAAAyMYQAQAAAAAAZGOIAAAAAAAAshltdbFer0e9Xm+4NpkmozRitwAAAAAAAKav5bJQrVajXC43vLZObO722QAAAAAAgAHXcoioVCpRq9UaXuNjC7t9NgAAAAAAYMC1fDRTURRRFEXDNY9lAgAAAAAA2mVdAAAAAAAAsjFEAAAAAAAA2RgiAAAAAACAbAwRAAAAAABANoYIAAAAAAAgG0MEAAAAAACQjSECAAAAAADIxhABAAAAAABkY4gAAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsDBEAAAAAAEA2hggAAAAAACAbQwQAAAAAAJCNIQIAAAAAAMjGEAEAAAAAAGRjiAAAAAAAALIZbXWxXq9HvV5vuDaZJqM0YrcAAAAAAACmr+WyUK1Wo1wuN7y2Tmzu9tkAAAAAAIAB13KIqFQqUavVGl7jYwu7fTYAAAAAAGDAtXw0U1EUURRFwzWPZQIAAAAAANplXQAAAAAAALIxRAAAAAAAANkYIgAAAAAAgGwMEQAAAAAAQDaGCAAAAAAAIBtDBAAAAAAAkI0hAgAAAAAAyMYQAQAAAAAAZGOIAAAAAAAAsjFEAAAAAAAA2RgiAAAAAACAbAwRAAAAAABANoYIAAAAAAAgG0MEAAAAAACQjSECAAAAAADIxhABAAAAAABkM9rqYr1ej3q93nBtMk1GacRuAQAAAAAATF/LZaFarUa5XG54bZ3Y3O2zAQAAAAAAA67lEFGpVKJWqzW8xscWdvtsAAAAAADAgGv5aKaiKKIoioZrHssEAAAAAAC0y7oAAAAAAABkY4gAAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsDBEAAAAAAEA2hggAAAAAACAbQwQAAAAAAJCNIQIAAAAAAMjGEAEAAAAAAGRjiAAAAAAAALIxRAAAAAAAANkYIgAAAAAAgGwMEQAAAAAAQDaGCAAAAAAAIBtDBAAAAAAAkI0hAgAAAAAAyMYQAQAAAAAAZDPa6mK9Xo96vd5wbTJNRmnEbgEAAAAAAExfy2WhWq1GuVxueG2d2NztswEAAAAAAAOu5RBRqVSiVqs1vMbHFnb7bAAAAAAAwIBr+WimoiiiKIqGax7LBAAAAAAAtMu6AAAAAAAAZGOIAAAAAAAAsjFEAAAAAAAA2RgiAAAAAACAbAwRAAAAAABANoYIAAAAAAAgG0MEAAAAAACQjSECAAAAAADIxhABAAAAAABkY4gAAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsDBEAAAAAAEA2hggAAAAAACAbQwQAAAAAAJCNIQIAAAAAAMhmtNXFer0e9Xq94dpkmozSiN0CAAAAAACYvpbLQrVajXK53PDaOrG522cDAAAAAAAGXMsholKpRK1Wa3iNjy3s9tkAAAAAAIAB1/LRTEVRRFEUDdc8lgkAAAAAAGiXdQEAAAAAAMjGEAEAAAAAAGRjiAAAAAAAALIxRAAAAAAAANkYIgAAAAAAgGwMEQAAAAAAQDaGCAAAAAAAIBtDBAAAAAAAkI0hAgAAAAAAyMYQAQAAAAAAZGOIAAAAAAAAsjFEAAAAAAAA2RgiAAAAAACAbAwRAAAAAABANoYIAAAAAAAgG0MEAAAAAACQjSECAAAAAADIZrTVxXq9HvV6veHaZJqM0ojdAgAAAAAAmL6Wy0K1Wo1yudzw2jqxudtnAwAAAAAABlzLIaJSqUStVmt4jY8t7PbZAAAAAACAAdfy0UxFUURRFA3XPJYJAAAAAABol3UBAAAAAADIxhABAAAAAABkY4gAAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsDBEAAAAAAEA2hggAAAAAACAbQwQAAAAAAJCNIQIAAAAAAMjGEAEAAAAAAGRjiAAAAAAAALIxRAAAAAAAANkYIgAAAAAAgGwMEQAAAAAAQDaGCAAAAAAAIBtDBAAAAAAAkI0hAgAAAAAAyGa01cV6vR71er3h2mSajNKI3QIAAAAAAJi+lstCtVqNcrnc8No6sbnbZwMAAAAAAAZcyyGiUqlErVZreI2PLez22QAAAAAAgAHX8tFMRVFEURQN1zyWCQAAAAAAaJd1AQAAAAAAyMYQAQAAAAAAZGOIAAAAAAAAsjFEAAAAAAAA2RgiAAAAAACAbAwRAAAAAABANoYIAAAAAAAgn9RBTzzxRProRz+annjiiaG6pzP29z17bRB+nwbhjDnuOaxn7AfD+ns/jGfMcU9d9O6ezti/99RFb+43KPd0Rl10836Dck9n1EU375fjnoNwxhz3HIQz9oNB+X0axj8fg3DGXPccSSmlTo0ajzzySJTL5ajVanHwwQcPzT2dsb/v2WuD8Ps0CGfMcc9hPWM/GNbf+2E8Y4576qJ393TG/r2nLnpzv0G5pzPqopv3G5R7OqMuunm/HPcchDPmuOcgnLEfDMrv0zD++RiEM+a6p0czAQAAAAAA2RgiAAAAAACAbAwRAAAAAABANh0dIoqiiI9+9KNRFMVQ3dMZ+/uevTYIv0+DcMYc9xzWM/aDYf29H8Yz5rinLnp3T2fs33vqojf3G5R7OqMuunm/QbmnM+qim/fLcc9BOGOOew7CGfvBoPw+DeOfj0E4Y657dvSbVQMAAAAAAOzOo5kAAAAAAIBsDBEAAAAAAEA2hggAAAAAACAbQwQAAAAAAJDNjIaI3/72t/E3f/M38cgjjzR9rFarxYc+9KF44IEH9vlwnbRt27a477774je/+U3DayZOP/30+NGPftThE3bW2rVro16vN13ftm1brF27dkb3vOmmm+LGG29sun7jjTfGzTffPKN7zia6GL4uNDG1Ye5iEJqI0EUv6EIXO+niKbrQxU66eMqgdTFs7y0idNFtg9ZEhC52p4s8dKGL3fV7FzMaIj7zmc/EI488EgcffHDTx8rlcjz66KPxmc98ZsaHevofxH35g3nnnXfGn/3Zn8WcOXNi0aJFccQRR8QRRxwRixcvjiOOOGJG56vVanHyySfHn/zJn8QnPvGJ2LRp04zu08rExETcfvvtsW7duoZXu84444yo1WpN1x999NE444wzZnS2s88+O+69996m65s2bYqzzz57Rvd8ui9+8Ytx3nnndeRe3aaL4euiG01E6GJv+rmLnE1E6EIXrXWyiQhdPJ0u8tGFLnbSxVMGpYthfW8RoYtuG+b3FhG6eDpd7KALXeyu77tIM/D85z8/XXfddXv8+A033JCOOeaYmdw6pZTSyMhIKpVKe3y142Uve1lasWJF+u53v5t+8YtfpFtvvbXhNVNbtmxJn/70p9OyZcvS6Ohoes1rXpO+8Y1vpG3bts34fitXruzIP3NKO34Pt2zZ0nT91ltvTePj4zM644EHHph+/etfN12/66670ty5c2d0z6d7xStekY444oiO3KvbdDF8XXSjiZR0sTf93kWnm9h5T13oYk862URKung6XeSjC13spIunDEoXw/reIiVddNuwv7dISRe708UOutDF7vq9ixkNEWNjY+mee+7Z48fvueeeNDY2NuNDPf0P4k033ZT+/d//PS1dujRdeumlbZ/1V7/61YzPMh0///nP03ve8570jGc8Ix166KHp/e9/f7rjjjvausfb3va2dOKJJ6abbropHXjggekHP/hBuuiii9JRRx2Vvv3tb0/7Pi984QvT8uXLU6lUSscee2xavnz5rteyZcvSQQcdlN7ylre0+4+YUkpp3rx56cc//nHT9RtuuCEdcsghM7rnVJYvX57e9KY3Zbl3p+mi0TB00YsmUtLF7gapi040kZIu9kQXO3SyiZ1n1YUuukEXT9GFLnYalC6G7b1FSrroFe8tGulCFynp4ul00d9djM7kqyjmzJkTd999dxx++OEtP3733XfHnDlzZvxVGi94wQuarh1//PFx2GGHxb/8y7/EG9/4xmnf65hjjokHH3xwxmeZyv333x9XXnllXHnllbHffvvF6173uvjlL38ZxxxzTHzqU5+KD3zgA9O6z9VXXx2XX355HH/88VEqlWLRokXxyle+Mg4++OCoVquxcuXKad1n9erVERFx6623xqtf/eqYO3furo8dcMABsXjx4njTm97U9j9nRMSrXvWqqFQqcfnll0e5XI6IiIcffjg+8pGPxCtf+coZ3XMqt956azzxxBNZ7t1punjKsHTRiyYidLG7QemiU01E6GJPdLFDJ5uI0IUuukcXO+hCF7sblC6G7b1FhC56xXuLp+hCFzvp4im6GIAuZrJevO51r0tnnXXWHj9+5plnpte+9rUzXkf25M4772x7xbvqqqvSS1/60vTDH/4wPfjgg6lWqzW8ZmLbtm3pkksuSStXrkz7779/etGLXpS+9KUvNdzvsssua2uBOuigg9LGjRtTSikdfvjh6frrr08p7fiSmjlz5uz15z7++ONN1y644IL0hz/8Ydq//nTcd999acmSJalcLqeTTjopnXTSSemQQw5JRx11VPrNb37T0V9rp5GRkXT00UdnuXen6WL4uuhFEynpYjr6oYscTaSkiz3Rxd7NpImUdNEpupiaLnShi2aD0sUwvLdISRf9YJjfW6SkC120pgtdDFIXMxoirr766rTffvulc845J23evHnX9c2bN6cPfvCDab/99ktXXXXVjA/19D+IDz/8cPrVr36V3vrWt6YXvOAFbd1rZGSk5fPMdl6biT/6oz9K4+Pj6d3vfnf6xS9+0fJztm7dmhYvXjztex5//PHpe9/7XkoppVWrVqVTTz013XfffenDH/5wWrJkyV5/7sc//vH0z//8z03Xf/e736UvfelL6cMf/vCuaz/72c/SfffdN+1zPd1jjz2WvvzlL6d3v/vd6ZxzzkkXXnjhPj27diqD8h/+lHQxrF10u4mUdLG7fu4iRxMp6WJPdLFDJ5tISRe66B5d6EIXzQali2F4b5GSLvrBML+3SEkXumhNF7oYpC5mNESklNK//du/paIoUqlUSoccckgaHx9PpVIpFUWRvvjFL874QCm1/kYoIyMj6fDDD2/5/Ku9ueaaa/b6mom1a9d2/KsNLrroonT++eenlFK6+eab06GHHppKpVJ6xjOekb7+9a/v9edu2bIlveENb0hf/vKXU0opbd++Pf385z9P8+bNS0cddVRDzB/60IfSqaee2tGz5zQo/+HfSRe66AZdPKWfu8jRREq62BNd7NDJJlLShS66SxedpYvWdLFDP/8dKqX+e2+Rki76xbC+t0hJF92mix10oYvd7WsXIymlNNPHOm3atCkuvvji2LBhQ6SU4sgjj4w3v/nN8cd//McNn3fcccfFkiVL4pJLLpnWfa+99tqGH5dKpZg/f34873nPi9HRGX1bi772uc99Lp588slYuXJlHH300RERMTExEbfffnscfvjhceihh07rPjfccEP8wz/8Q1x22WVxyimnxJ/+6Z9GtVqNUqkUk5OTERHx4x//ON72trfF3Xff3fY5165du9ePn3baaW3fcyqlUimWLl0a69ev7/i9c9FFZwxCF71oIkIXu9OFLnbSxQ7D1kSELvZGFzvoQhe708UOw9ZFp5qI0EU/8N6iM3Sxd7rYQRe62N2+drFPQ8R0zfSQ69evj9/85jexbdu2huuvf/3r27rPddddF1/+8pfjrrvuim984xvx7Gc/Oy666KI44ogj4uUvf3lb99rp5ptvjosvvrjl+S677LK27vXpT386arVafOUrX4lNmzbN6DwREc95znPia1/7WqxYsSLK5XLccsst8dznPjf222+/2L59e0Ts+CY1S5cundY3FnnwwQcbAhsfH2/4+JNPPhkTExNxwAEHxNjYWDz00EMzPvueDNp/+Nuhi73rxy76oYkIXbTSr110sokIXeyNLhp1qokIXeiiP+li73SxZ7po1K9/h4roz/cWEboYJLPtvUWELiJ0sa90sXe62Lt97aKvpqtt27bFAQccEHfddVe88Y1vjHXr1sXIyEjs3EpGRkYiInb9y9qbn/3sZ3HCCSfEpZdeGqeeemq8/e1vj1tuuSXq9XpERNRqtfjEJz4R3/3ud9s+59e//vU47bTT4tWvfnX84Ac/iFe96lVxxx13xAMPPBCnnHJK2/c755xz4ogjjojNmzfHO97xjpaf8x//8R9T3mfevHnxk5/8JFasWBFFUcSjjz4aERG7b0233357zJ8/f1rn+uIXvxhPPvlkfPzjH4+IiK1btzZ9zp133hnvete74kMf+tC07kn7dNE/XWiifwxCF51uIkIX7Fknm4jQhS5mB13ogmaD8HeoiP59bxGhi9lIF7qgmS50kcWMH+rUzvOfpvH8qBtuuCG9+MUvTiml9Bd/8RfpDW94Q/rd736X5s6dm/7nf/4nXXfddemEE05IP/rRj/Z6n0cffTSdffbZu74j/Ate8IJ04YUXppRSmjt3bvr1r3+dUkrplltuSQsWLEgppXTvvfem7du3T/nP8bvf/S6llNKxxx6bvvCFLzTcc3JyMv31X/91Ovfcc6e8TyuHHXZYetnLXpZWr16dVq9enVauXJkWLVqUDj744HTKKadM6x71ej397d/+bXrwwQfTmWeemd70pjelJ598ctezx+6888507LHHpve9733Tut/WrVvT6tWr09vf/va9ft5NN92UjjrqqGnds12D9Ey+duliav3WRT80kZIu+r2LnE2kpIs9GeYuOtVESrrQxeDQxdR00Zou+vfvUCn1/3uLlHQxSGbDe4uUdLEnupgZXUxNF3vWs29W3Y6pDrl27dr0ohe9KN15550ppR3f8fy2225LKaV08MEHp9tvvz2llNJVV12VXvjCF+711/rYxz6WVq1atevHc+bMSRs3bkwpNf4h//Wvf52Koth1vu9///t7ve8111yTli1bllJKaWxsbNc9582bl9atW5dSSmn9+vVp4cKFe71PO7Zv357e+c53tvxO61N5+OGH08knn5zmzZuXRkZG0uLFi9Po6GhasWJFeuyxx9q61+c///m9fvwXv/hFOuigg9o+43TM1v/wp6SLmeqHLnrZREq66OcuetFESrpIaXi76GQTKeliJ130P13MjC500a9/h0pp8N5bpKSLfjfo7y1S0sXe6GJmdDEzuthhX7voi0czzZ8/Px577LG477774nnPe15s3749DjrooIiIOPTQQ+O3v/1tHHXUUbFo0aL43//9373e66/+6q/iBz/4Qfzd3/1dfPKTn4yFCxfGhg0bYvHixQ2fd/3118eSJUsiIuL//b//FyeccMIe73nxxRfHueeeG9/+9rcjYsdzuHZ+Oc2zn/3s+O///u849thj4+GHH46JiYmZ/jY0KZVK8cEPfjBOOumk+PCHP9zWzy2Xy3HllVfGDTfcELfddls89thjcdxxx8XJJ5/c9jne+973RkTEt771rYbrKaW4//774wtf+EKceOKJbd+XvdNFa/3QhSZ6p5+76FUTEboYZp1sIkIXO+lisOliz3QxvPr571ARg/neIkIXg04XreliuOmiNV10SAfGkClNZy2599570xve8IaUUkovf/nL0ze/+c2UUkpr1qxJr3nNa9L111+fTjvttPT85z9/yl9vcnIyfepTn0oppfSJT3wiHXPMMemnP/1pOuigg9J1112XvvrVr6b58+dPuSLt9K1vfStt2bJl14/XrFmTPv3pT6eUUjrvvPPS/Pnz01lnnZUWLVrU1pfoTMd3vvOddOihh7b98+699949fuwnP/nJjM4yMjLS8CqVSmnBggVpzZo16be//e2M7jmVu+++O23atCnLvXtNFzPXL130oomUdNGvXfSyiZR0McxddLKJlHSRki4GgS5mThe6SKm//g6V0mC+t0hJF/1ukN9bpKSLnXTRWbqYOV3sexd9M0Ts7nvf+1669NJLU0o7npd11FFHpZGRkXTooYemq666qq1fe3JyMv3TP/1TOvDAA3f9y3nGM56R/v7v/76t++zu97///a7f9O3bt6dqtZpWrVqVPvjBD6aHHnpoRvf8wAc+0PB6//vfn9761remuXPnprPPPrvt+x199NHp97//fdP166+/PpXL5Rmdkc7SxdR0MXxmUxc5mkhJF8OonS462URKung6XfQPXUxNF8OnV10M63uLlHTR72bTe4uUdEFn6GJqushnJKXdvlV3JqVSKZYuXRrr16+f8T0eeuihGB8f3/Vd2du1bdu22LBhQzz22GNxzDHHxNy5c2d8lhz+/M//vOHHpVIp5s+fH694xSviHe94R4yOtvcUrXe84x2xbt26+OEPf7jrS6h+9KMfxapVq+If//Ef4wMf+EDHzs7M6GJquhg+upiaLobPvnaxr01E6CJCF/1GF1PTxfDpdRfD1kSELvqd9xZT08Xw0cXUdJHPwAwRM3X++efHX/7lX8acOXM6et/JycnYsGFDbNmyJSYnJxs+tmLFio7+WjMxOTkZb37zm+Ohhx6K73//+/HjH/84Xv/618c//dM/xfve974Z3XP79u1xwQUXxFVXXdXyn/vqq6/uxNGHhi66r9NdaKLzZlsX/d5EhC4GgS66Txf9Txfdp4v+16suhvW9RYQu+t1s+/+KCF3oYt/pojd0scOsHyIWLFgQf/jDH+Itb3lLnHnmmfGyl71sn+/505/+NN72trfFPffcE0//7RsZGYnt27fv86/RCdu2bYuVK1fGxMRErFu3LqrVarznPe+Z8f3e8573xAUXXBArV66MZz3rWU3L52c/+9l9PfJQ0UVvdLILTXTebOpiUJqI0EW/00Vv6KK/6aI3dNHfetXFML+3iNBFP5tN/18RoQtddIYuekcXXRoi7rnnnth///3jsMMOy/1LNfm///u/+M///M+44IIL4oorroglS5bEGWecEaeffnosXLhwRvd84QtfGEceeWR87GMfa/kvu1wut33Pxx9/PD75yU/uccm66667przHunXrmq49+uijsWbNmli5cmW8613v2nV92bJlbZ/x0EMPjbVr18brXve6tn8uzXQxtX7vQhOdN5u6yNFEhC6GkS6mpovho4up6WL49KqLYXpvEaGLQTKb/r8iQhe66AxdTE0X+XRliOgXDzzwQHz1q1+NCy+8MG6//fZ4zWteE2eeeWasWrUqSqXStO9z4IEHxm233RbPe97zOna2NWvWxLXXXhunnnpqy3Cm82U6pVIpRkZGGhbA3X+883/PdBE87LDD4pprrokjjzyy7Z9L/9LFzLvQxOzViS5yNBGhC3pHF7qgmS50QaPZ/t4iQhe0Txe6oJkuhrOL9r+7xgBbsGBBvPzlL4877rgj7rjjjvjlL38Zp59+eoyPj8f5558fJ5100rTu85KXvCQ2bNjQ0T/kV1xxRXznO9+JE088ccb32LhxY9s/57jjjoslS5bEJZdcMuXnnnPOOfG5z30uvvCFL+zTN/ajv+ii2XS70MTs1YkucjQRoQt6RxfNdIEumuliuM329xYRuqB9umhNF8NNF63N9i6GYoh44IEH4qKLLorzzz8/7rrrrli9enV8+9vfjpNPPjkef/zxOO+88+L000+Pe+65Z1r3e+973xvnnHNObN68OY499tjYf//9Gz4+k8cejY+Px7x589r+ebtbtGhR2z/n1ltvjSeeeGKPH3/jG9/Y8OOrr746rrjiinj+85/f9M992WWXtf3r0zu62LO9daGJ2a2TXeRoIkIXdJ8u9kwXw0sXe6aL4TQs7y0idMH06WLvdDGcdLF3s72LWf9oplWrVsX3v//9OPLII+Oss86K0047rekP05YtW2LhwoVNz/zak1ZfIrSvjz366le/GpdffnlceOGFMTY21vbPn6mpvknNGWecMe17nX/++Z06FpnpYu/21oUmZq9Od5GjiQhd0F262DtdDCdd7J0uho/3FlPTxfDRxdR0MXx0MbXZ3sWs/4qIZz7zmXHttdfGS1/60j1+zvz589v6cpnpfu5UX06zfPnyhi+f2bBhQyxYsCAWL17ctGTdcsst0z5fJ/XrH1z2jS5mThOzV6e76FQTEbqgd3Qxc7qYvXQxc7qYnby32De6mJ10sW90MTvpYt/Mhi5m/RDxla98ZcrPGRkZaevLZab7uVM99mj16tXT/jX7wSte8Yq47LLL4pBDDmm4/sgjj8Tq1avj6quv7s3BaJsuOkMTs0unu+hUExG6oHd00Rm6mF100Rm6mD28t+gcXcweuugcXcweuuicQe1i1g8R55133l4/fu6553bpJM0++tGP9uzXnolrrrkmtm3b1nT9iSeeiOuuu64HJ2KmdNEZmphddNEZuphddNEZuphddNEZupg9NNE5upg9dNE5upg9dNE5g9rFrB8ivvnNbzb8+Mknn4yNGzfG6OhoPPe5z+3pH/Ld3XTTTTE5ORkveclLGq7feOONsd9++8Xxxx/fo5NFrFu3btf/Xr9+fWzevHnXj7dv3x7f+9734tnPfnYvjsYM6WLfaGJ20sW+0cXspIt9o4vZSRf7Rhezjyb2nS5mH13sO13MPrrYdwPfRRpCtVotnXLKKWnt2rVZf52RkZF09NFHT+tzX/ziF6dvfOMbTdcvvfTSdMIJJ3T6aLtM54wjIyOpVCqlUqmURkZGml5jY2PpK1/5SrYz0h26eMpUZ9TE8OhGF+00kZIu6D1dPEUX7KSLp+iClLy3eDpdkJIunk4XpKSLp5vtXcz6r4ho5eCDD46PfexjsWrVqjj11FN7fZyI2LFiHXfccU3Xly9f3vI7pXfTxo0bI6UUS5YsiZ/97Gcxf/78XR874IAD4pnPfGbst99+u65N5xvp0X90MX2aGB66mD5dDA9dTJ8uhocupk8Xw0ET7dHFcNBFe3QxHHTRnkHvYiiHiIiIWq0WtVqt18fYpSiKeOCBB2LJkiUN1++///4YHe3tv6ad3/hlcnJyWp8/nW+kR3/SxfRoYrjoYnp0MVx0MT26GC66mB5dDA9NTJ8uhocupk8Xw0MX0zfoXcz6IeLzn/98w49TSnH//ffHRRddFK997Wt7dKpmr3rVq6JSqcTll18e5XI5IiIefvjh+MhHPhKvfOUre3w6ZhtdQDNdQDNdQDNdQCNNQDNdQDNdMOuHiM9+9rMNPy6VSjF//vw4/fTTo1Kp9OhUzf71X/81VqxYEYsWLYrly5dHxI7VasGCBXHRRRdl+3U3btwY+++/f7b70590sXe6GE662DtdDCdd7J0uhpMu9k4Xw0cTU9PF8NHF1HQxfHQxtdnexUhKKfX6ELNVqVSKpUuXTvv5YY8//nh87Wtfi9tuuy3mzJkTy5YtizVr1jT8Aey3Z3u10u4/N8NlGLvQBHszkz8fumC204UuaKYLXdBsGN9bROiCvdOFLmimi/7oYtZ/RcQgOfDAA+Od73znXj+n357tBbnpAprpAprpAprpAhppAprpAprpIo9ZP0Q8/vjj8clPfjKuuuqq2LJlS9M387jrrruy/dqz/ctpGFy6gGa96kIT9DNdQDNdQCPvLaCZLqCZLpj1Q8RZZ50V1157bZx66qnxrGc9K0ZGRrr2a+/8TubQb3QBzXrVhSboZ7qAZrqARt5bQDNdQDNdMOuHiCuuuCK+853vxIknntjro0Df0AU00wU00wU00wU00gQ00wU00wWlXh8gt/Hx8Zg3b16vjwF9RRfQTBfQTBfQTBfQSBPQTBfQTBfM+iHi4x//eJx77rkxMTHR66NA39AFNNMFNNMFNNMFNNIENNMFNNMFs/LRTMuXL294ztiGDRtiwYIFsXjx4qZvTHLLLbd0+3iznm8A05900Tua6F+66B1d9C9d9I4u+pcuekcX/UkTvaWL/qSL3tJFf9JFb/VbF7NyiFi9enWvjzDUfAOY/qSL3tFE/9JF7+iif+mid3TRv3TRO7roT5roLV30J130li76ky56q9+6GEkppV4fgukrlUqxdOnSWL9+fa+PAn1DF9BMF9BMF9BMF9BIE9BMF9BMF+2b9d8j4qabboobb7yx6fqNN94YN998cw9OBL2nC2imC2imC2imC2ikCWimC2imC2b9EHH22WfHvffe23R906ZNcfbZZ/fgRPtm48aN8V//9V+9PgYDThfQTBfQTBfQTBfQSBPQTBfQTBfM+kczzZ07N9atWxdLlixpuL5x48ZYtmxZPProoz06GfSOLqCZLqCZLqCZLqCRJqCZLqCZLpj1XxFRFEU88MADTdfvv//+GB2dld+rG6akC2imC2imC2imC2ikCWimC2imC2b9V0SsWbMm7r///rj88sujXC5HRMTDDz8cq1evjmc+85lx8cUX9/iE0H26gGa6gGa6gGa6gEaagGa6gGa6YNYPEZs2bYoVK1bE73//+1i+fHlERNx6662xYMGCuPLKK+M5z3lOj08I3acLaKYLaKYLaKYLaKQJaKYLaKYLZv0QERHx+OOPx9e+9rW47bbbYs6cObFs2bJYs2ZN7L///rs+57jjjoslS5bEJZdc0sOTQvfoAprpAprpAprpAhppAprpAprpYrgNxRAxHaVSKZYuXRrr16/v9VGgb+gCmukCmukCmukCGmkCmukCmuli9pr136waAAAAAADoHUMEAAAAAACQjSECAAAAAADIxhABAAAAAABkY4gAAAAAAACyMUQAAAAAAADZGCIAAAAAAIBsDBEAAAAAAEA2hggAAAAAACCb0V4foF9s3Lgx9t9//14fA/qKLqCZLqCZLqCZLqCRJqCZLqCZLmavkZRS6vUhAAAAAACA2cmjmQAAAAAAgGwMEQAAAAAAQDaGCAAAAAAAIBtDBAAAAAAAkI0hAgAAAAAAyMYQAQAAAAAAZGOIAAAAAAAAsjFEAAAAAAAA2fx/maEvTwGpAHkAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Self Layer 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Src Layer 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x1000 with 8 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "\n",
        "def draw_driver(n_encoder_layers, n_decoder_layers, n_heads, src, tgt):\n",
        "\n",
        "    def draw(data, x, y, ax):\n",
        "        seaborn.heatmap(data,\n",
        "                        xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0,\n",
        "                        cbar=False, ax=ax)\n",
        "\n",
        "    for layer in range(0, n_encoder_layers):\n",
        "        fig, axs = plt.subplots(1, n_heads, figsize=(20, 10))\n",
        "        print(\"Encoder Layer\", layer+1)\n",
        "        for h in range(n_heads):\n",
        "            draw(encoder_save_outputs[layer].outputs[-1][0][h].cpu().data[:len(src), :len(src)],\n",
        "                src, src if h == 0 else [], ax=axs[h])\n",
        "        plt.show()\n",
        "\n",
        "    for layer in range(0, n_decoder_layers):\n",
        "        fig, axs = plt.subplots(1, n_heads, figsize=(20, 10))\n",
        "        print(\"Decoder Self Layer\", layer+1)\n",
        "        for h in range(n_heads):\n",
        "            draw(decoder_self_attn_save_outputs[layer].outputs[-1][0][h].cpu().data[:len(tgt), :len(tgt)],\n",
        "                tgt, tgt if h ==0 else [], ax=axs[h])\n",
        "        plt.show()\n",
        "        print(\"Decoder Src Layer\", layer+1)\n",
        "        fig, axs = plt.subplots(1,args.n_heads, figsize=(20, 10))\n",
        "        for h in range(n_heads):\n",
        "            draw(decoder_cross_attn_save_outputs[layer].outputs[-1][0][h].cpu().data[:len(tgt), :len(src)],\n",
        "                src, tgt if h ==0 else [], ax=axs[h])\n",
        "        plt.show()\n",
        "\n",
        "MAX_SENTENCE_LENGTH_SHOWN = 25\n",
        "draw_driver(args.n_encoder_layers, args.n_decoder_layers, args.n_heads, src_sentence[:MAX_SENTENCE_LENGTH_SHOWN], translation.split(' ')[:MAX_SENTENCE_LENGTH_SHOWN])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI6QmPSEqQSu"
      },
      "source": [
        "# Great work! You have completed all the tasks in this assignment üëè"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}